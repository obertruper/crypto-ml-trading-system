#!/usr/bin/env python3
"""
Enhanced Temporal Fusion Transformer –¥–ª—è –∫—Ä–∏–ø—Ç–æ—Ç—Ä–µ–π–¥–∏–Ω–≥–∞ v2.1
–£–ª—É—á—à–µ–Ω–∏—è v2.0:
- –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å Bitcoin –∏ market features
- Focal Loss –¥–ª—è –ª—É—á—à–µ–π —Ä–∞–±–æ—Ç—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º
- Multi-scale convolutions
- Attention pooling
- Gradient accumulation
- Time-based features
- Ensemble support

–£–ª—É—á—à–µ–Ω–∏—è v2.1:
- OHLC features (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è, —Å–≤–µ—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã)
- Symbol embeddings (one-hot –¥–ª—è —Ç–æ–ø –º–æ–Ω–µ—Ç, –∫–∞—Ç–µ–≥–æ—Ä–∏–∏)
- –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫ EMA/VWAP
- Layer Normalization –¥–ª—è OHLC —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
- Post-processing —Ñ–∏–ª—å—Ç—Ä—ã
"""

import os
import sys
import time
import json
import pickle
import yaml
import warnings
import argparse
import gc
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Union

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import (
    mean_absolute_error, mean_squared_error, r2_score,
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report
)
from scipy import stats
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
log_dir = f"logs/training_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
os.makedirs(log_dir, exist_ok=True)
os.makedirs(f"{log_dir}/plots", exist_ok=True)
os.makedirs(f"{log_dir}/tensorboard", exist_ok=True)
os.makedirs("trained_model", exist_ok=True)  # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –º–æ–¥–µ–ª–µ–π

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'{log_dir}/training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from database_utils import PostgreSQLManager

# –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
warnings.filterwarnings('ignore')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logger.info(f"üñ•Ô∏è GPU –¥–æ—Å—Ç—É–ø–µ–Ω: {gpus[0].name}")
        
        # –í—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º mixed precision –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        # TODO: –≤–∫–ª—é—á–∏—Ç—å –æ–±—Ä–∞—Ç–Ω–æ –ø–æ—Å–ª–µ –æ—Ç–ª–∞–¥–∫–∏ nan –ø—Ä–æ–±–ª–µ–º—ã
        # policy = tf.keras.mixed_precision.Policy('mixed_float16')
        # tf.keras.mixed_precision.set_global_policy(policy)
        logger.info("‚ö†Ô∏è Mixed precision –≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏")
    except RuntimeError as e:
        logger.error(f"–û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ GPU: {e}")


class MemoryCleanupCallback(keras.callbacks.Callback):
    """Callback –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –ø–∞–º—è—Ç–∏ –º–µ–∂–¥—É —ç–ø–æ—Ö–∞–º–∏"""
    
    def on_epoch_end(self, epoch, logs=None):
        # –û—á–∏—â–∞–µ–º –∫—ç—à –∏ —Å–æ–±–∏—Ä–∞–µ–º –º—É—Å–æ—Ä
        tf.keras.backend.clear_session()
        gc.collect()
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ GPU (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
        if tf.config.list_physical_devices('GPU'):
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞–º—è—Ç–∏
                gpu_memory = tf.config.experimental.get_memory_info('GPU:0')
                if 'current' in gpu_memory:
                    used_mb = gpu_memory['current'] / 1024 / 1024
                    logger.info(f"üíæ GPU –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ —ç–ø–æ—Ö–∏ {epoch+1}: {used_mb:.0f} MB")
            except:
                pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –µ—Å–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞


class VisualizationCallback(keras.callbacks.Callback):
    """Callback –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, log_dir, model_name, update_freq=5, task='regression'):
        super().__init__()
        self.log_dir = log_dir
        self.model_name = model_name
        self.update_freq = update_freq
        self.epoch_count = 0
        self.task = task
        
        self.history = {
            'loss': [],
            'val_loss': [],
            'lr': []
        }
        
        if task == 'regression':
            self.history.update({
                'mae': [],
                'val_mae': [],
                'rmse': [],
                'val_rmse': []
            })
        else:  # classification
            self.history.update({
                'accuracy': [],
                'val_accuracy': [],
                'precision': [],
                'val_precision': [],
                'recall': [],
                'val_recall': []
            })
        
        self.fig, self.axes = plt.subplots(2, 2, figsize=(15, 10))
        self.fig.suptitle(f'Training Progress: {model_name}', fontsize=16)
        
    def on_epoch_end(self, epoch, logs=None):
        self.epoch_count += 1
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        self.history['loss'].append(logs.get('loss', 0))
        self.history['val_loss'].append(logs.get('val_loss', 0))
        
        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ learning rate
        try:
            lr = self.model.optimizer.learning_rate
            if hasattr(lr, '__call__'):  # –ï—Å–ª–∏ —ç—Ç–æ schedule
                lr_value = lr(self.model.optimizer.iterations).numpy()
            else:
                lr_value = lr.numpy()
        except:
            lr_value = logs.get('lr', 0.0001)  # Fallback –∑–Ω–∞—á–µ–Ω–∏–µ
        self.history['lr'].append(lr_value)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        if self.task == 'regression':
            self.history['mae'].append(logs.get('mae', 0))
            self.history['val_mae'].append(logs.get('val_mae', 0))
            self.history['rmse'].append(logs.get('rmse', 0))
            self.history['val_rmse'].append(logs.get('val_rmse', 0))
        else:  # classification
            self.history['accuracy'].append(logs.get('accuracy', 0))
            self.history['val_accuracy'].append(logs.get('val_accuracy', 0))
            self.history['precision'].append(logs.get('precision', 0))
            self.history['val_precision'].append(logs.get('val_precision', 0))
            self.history['recall'].append(logs.get('recall', 0))
            self.history['val_recall'].append(logs.get('val_recall', 0))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ CSV
        metrics_df = pd.DataFrame(self.history)
        metrics_df.to_csv(f'{self.log_dir}/{self.model_name}_metrics.csv', index=False)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –≥—Ä–∞—Ñ–∏–∫–∏
        if self.epoch_count % self.update_freq == 0:
            self.update_plots()
            
    def update_plots(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤"""
        epochs = range(1, len(self.history['loss']) + 1)
        
        # –ì—Ä–∞—Ñ–∏–∫ 1: Loss
        self.axes[0, 0].clear()
        self.axes[0, 0].plot(epochs, self.history['loss'], 'b-', label='Train Loss', linewidth=2)
        self.axes[0, 0].plot(epochs, self.history['val_loss'], 'r-', label='Val Loss', linewidth=2)
        self.axes[0, 0].set_title('Model Loss')
        self.axes[0, 0].set_xlabel('Epoch')
        self.axes[0, 0].set_ylabel('Loss')
        self.axes[0, 0].legend()
        self.axes[0, 0].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 2: –û—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞
        self.axes[0, 1].clear()
        if self.task == 'regression':
            self.axes[0, 1].plot(epochs, self.history['mae'], 'b-', label='Train MAE', linewidth=2)
            self.axes[0, 1].plot(epochs, self.history['val_mae'], 'r-', label='Val MAE', linewidth=2)
            self.axes[0, 1].set_title('Mean Absolute Error (%)')
            self.axes[0, 1].set_ylabel('MAE (%)')
        else:  # classification
            self.axes[0, 1].plot(epochs, self.history['accuracy'], 'b-', label='Train Accuracy', linewidth=2)
            self.axes[0, 1].plot(epochs, self.history['val_accuracy'], 'r-', label='Val Accuracy', linewidth=2)
            self.axes[0, 1].set_title('Model Accuracy')
            self.axes[0, 1].set_ylabel('Accuracy')
            self.axes[0, 1].set_ylim(0, 1)
        
        self.axes[0, 1].set_xlabel('Epoch')
        self.axes[0, 1].legend()
        self.axes[0, 1].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 3: Learning Rate
        self.axes[1, 0].clear()
        self.axes[1, 0].plot(epochs, self.history['lr'], 'g-', linewidth=2)
        self.axes[1, 0].set_title('Learning Rate Schedule')
        self.axes[1, 0].set_xlabel('Epoch')
        self.axes[1, 0].set_ylabel('Learning Rate')
        self.axes[1, 0].set_yscale('log')
        self.axes[1, 0].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 4: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.axes[1, 1].clear()
        self.axes[1, 1].axis('off')
        
        if self.task == 'regression':
            current_stats = f"""
Current Epoch: {len(self.history['loss'])}

Loss:
  Train:  {self.history['loss'][-1]:.4f}
  Val:    {self.history['val_loss'][-1]:.4f}
  
MAE (%):
  Train:  {self.history['mae'][-1]:.3f}%
  Val:    {self.history['val_mae'][-1]:.3f}%

RMSE (%):
  Train:  {self.history['rmse'][-1]:.3f}%
  Val:    {self.history['val_rmse'][-1]:.3f}%

Best Results:
  Val Loss: {min(self.history['val_loss']):.4f}
  Val MAE:  {min(self.history['val_mae']):.3f}%
  Epoch:    {self.history['val_loss'].index(min(self.history['val_loss'])) + 1}

Learning Rate: {self.history['lr'][-1]:.2e}
"""
        else:  # classification
            current_stats = f"""
Current Epoch: {len(self.history['loss'])}

Loss:
  Train:  {self.history['loss'][-1]:.4f}
  Val:    {self.history['val_loss'][-1]:.4f}
  
Accuracy:
  Train:  {self.history['accuracy'][-1]:.3f}
  Val:    {self.history['val_accuracy'][-1]:.3f}

Precision:
  Train:  {self.history['precision'][-1]:.3f}
  Val:    {self.history['val_precision'][-1]:.3f}
  
Recall:
  Train:  {self.history['recall'][-1]:.3f}
  Val:    {self.history['val_recall'][-1]:.3f}

Best Results:
  Val Loss: {min(self.history['val_loss']):.4f}
  Val Acc:  {max(self.history['val_accuracy']) if self.history['val_accuracy'] else 0:.3f}
  Epoch:    {self.history['val_accuracy'].index(max(self.history['val_accuracy'])) + 1 if self.history['val_accuracy'] else 0}

Learning Rate: {self.history['lr'][-1]:.2e}
"""
        
        self.axes[1, 1].text(0.1, 0.5, current_stats, fontsize=12, 
                            verticalalignment='center', family='monospace',
                            bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.5))
        
        plt.tight_layout()
        plt.savefig(f'{self.log_dir}/plots/training_progress.png', dpi=150, bbox_inches='tight')
        plt.savefig(f'{self.log_dir}/plots/epoch_{self.epoch_count:03d}.png', dpi=100)
        
        logger.info(f"üìä –ì—Ä–∞—Ñ–∏–∫ –æ–±–Ω–æ–≤–ª–µ–Ω: —ç–ø–æ—Ö–∞ {self.epoch_count}")


class FocalLoss(keras.losses.Loss):
    """Focal Loss –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤"""
    def __init__(self, alpha=0.25, gamma=2.0, label_smoothing=0.05, **kwargs):
        super().__init__(**kwargs)
        self.alpha = alpha
        self.gamma = gamma
        self.label_smoothing = label_smoothing
    
    def call(self, y_true, y_pred):
        # Ensure float32 computation for numerical stability
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)
        
        # Label smoothing
        y_true = y_true * (1 - self.label_smoothing) + 0.5 * self.label_smoothing
        
        # Clip predictions to prevent log(0) - –±–æ–ª–µ–µ –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã
        epsilon = 1e-7
        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)
        
        # Calculate focal loss with numerical stability
        alpha_t = y_true * self.alpha + (1 - y_true) * (1 - self.alpha)
        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)
        
        # –î–æ–±–∞–≤–ª—è–µ–º epsilon —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å log(0)
        p_t = tf.clip_by_value(p_t, epsilon, 1.0)
        
        # Focal loss computation
        focal_weight = tf.pow((1 - p_t), self.gamma)
        focal_loss = -alpha_t * focal_weight * tf.math.log(p_t + epsilon)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ nan/inf
        focal_loss = tf.where(tf.math.is_finite(focal_loss), focal_loss, 0.0)
        
        return tf.reduce_mean(focal_loss)


class MultiScaleConv1D(layers.Layer):
    """Multi-scale 1D Convolutions –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Ä–∞–∑–Ω—ã—Ö –º–∞—Å—à—Ç–∞–±–æ–≤"""
    def __init__(self, filters, kernel_sizes=[3, 5, 7], **kwargs):
        super().__init__(**kwargs)
        self.convs = [
            layers.Conv1D(filters // len(kernel_sizes), kernel_size, padding='same', activation='relu')
            for kernel_size in kernel_sizes
        ]
        self.batch_norm = layers.BatchNormalization()
        
    def call(self, inputs):
        outputs = [conv(inputs) for conv in self.convs]
        concatenated = layers.concatenate(outputs, axis=-1)
        return self.batch_norm(concatenated)


class AttentionPooling(layers.Layer):
    """Attention-based pooling –≤–º–µ—Å—Ç–æ GlobalAveragePooling"""
    def __init__(self, hidden_dim=128, **kwargs):
        super().__init__(**kwargs)
        self.hidden_dim = hidden_dim
        
    def build(self, input_shape):
        self.W = self.add_weight(
            shape=(input_shape[-1], self.hidden_dim),
            initializer='glorot_uniform',
            trainable=True
        )
        self.b = self.add_weight(
            shape=(self.hidden_dim,),
            initializer='zeros',
            trainable=True
        )
        self.u = self.add_weight(
            shape=(self.hidden_dim,),
            initializer='glorot_uniform',
            trainable=True
        )
        
    def call(self, inputs):
        # Attention scores
        hidden = tf.nn.tanh(tf.matmul(inputs, self.W) + self.b)
        scores = tf.matmul(hidden, tf.expand_dims(self.u, -1))
        scores = tf.squeeze(scores, -1)
        
        # Softmax weights
        weights = tf.nn.softmax(scores, axis=1)
        weights = tf.expand_dims(weights, -1)
        
        # Weighted sum
        weighted_input = inputs * weights
        return tf.reduce_sum(weighted_input, axis=1)


class GatedResidualNetwork(layers.Layer):
    """Enhanced GRN —Å dropout –∏ layer norm"""
    def __init__(self, hidden_dim, output_dim=None, dropout=0.1, **kwargs):
        super().__init__(**kwargs)
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim or hidden_dim
        self.dropout_rate = dropout
        
    def build(self, input_shape):
        self.dense1 = layers.Dense(self.hidden_dim, activation='elu')
        self.dense2 = layers.Dense(self.output_dim)
        self.dropout = layers.Dropout(self.dropout_rate)
        self.layer_norm = layers.LayerNormalization()
        
        if self.output_dim != input_shape[-1]:
            self.skip_proj = layers.Dense(self.output_dim)
        else:
            self.skip_proj = None
            
        self.gate = layers.Dense(self.output_dim, activation='sigmoid')
        
    def call(self, inputs, training=False):
        hidden = self.dense1(inputs)
        hidden = self.dropout(hidden, training=training)
        hidden = self.dense2(hidden)
        hidden = self.dropout(hidden, training=training)
        
        if self.skip_proj is not None:
            residual = self.skip_proj(inputs)
        else:
            residual = inputs
            
        gate_values = self.gate(hidden)
        output = gate_values * hidden + (1 - gate_values) * residual
        return self.layer_norm(output)


class PositionalEncoding(layers.Layer):
    """–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å learnable parameters"""
    def __init__(self, sequence_length, d_model, **kwargs):
        super().__init__(**kwargs)
        self.sequence_length = sequence_length
        self.d_model = d_model
        
    def build(self, input_shape):
        # Standard positional encoding
        position = tf.expand_dims(tf.range(0, self.sequence_length, dtype=tf.float32), 1)
        div_term = tf.exp(tf.range(0, self.d_model, 2, dtype=tf.float32) * 
                          -(tf.math.log(10000.0) / self.d_model))
        
        pe = tf.zeros((self.sequence_length, self.d_model))
        pe_sin = tf.sin(position * div_term)
        pe_cos = tf.cos(position * div_term)
        
        # –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ sin –∏ cos
        # pe_sin –∏ pe_cos –∏–º–µ—é—Ç —Ñ–æ—Ä–º—É (sequence_length, d_model//2)
        # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π pe —Ç–µ–Ω–∑–æ—Ä
        pe_list = []
        for i in range(self.d_model):
            if i % 2 == 0:
                # –ß–µ—Ç–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ - sin
                pe_list.append(pe_sin[:, i // 2:i // 2 + 1])
            else:
                # –ù–µ—á–µ—Ç–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ - cos
                pe_list.append(pe_cos[:, i // 2:i // 2 + 1])
        
        pe = tf.concat(pe_list, axis=1)
        
        self.pe = self.add_weight(
            name='positional_encoding',
            shape=(1, self.sequence_length, self.d_model),
            initializer=tf.constant_initializer(pe.numpy()),
            trainable=True  # –î–µ–ª–∞–µ–º –æ–±—É—á–∞–µ–º—ã–º
        )
        
    def call(self, inputs):
        return inputs + self.pe


class EnhancedTransformerBlock(layers.Layer):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π Transformer –±–ª–æ–∫ —Å pre-norm –∏ gate mechanism"""
    def __init__(self, d_model, num_heads, ff_dim, dropout=0.1, **kwargs):
        super().__init__(**kwargs)
        self.attention = layers.MultiHeadAttention(
            num_heads=num_heads, 
            key_dim=d_model,
            dropout=dropout
        )
        self.ffn = keras.Sequential([
            layers.Dense(ff_dim, activation='gelu'),
            layers.Dropout(dropout),
            layers.Dense(d_model),
            layers.Dropout(dropout)
        ])
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.gate = layers.Dense(d_model, activation='sigmoid')
        
    def call(self, inputs, training=False):
        # Pre-norm
        normed = self.layernorm1(inputs)
        attn_output = self.attention(normed, normed, training=training)
        gate1 = self.gate(attn_output)
        out1 = inputs + gate1 * attn_output
        
        # Feed-forward with pre-norm
        normed2 = self.layernorm2(out1)
        ffn_output = self.ffn(normed2, training=training)
        gate2 = self.gate(ffn_output)
        return out1 + gate2 * ffn_output


class EnhancedTemporalFusionTransformer(keras.Model):
    """Enhanced TFT —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏ v2.1"""
    def __init__(self, num_features, sequence_length, d_model=256, num_heads=8, 
                 num_transformer_blocks=6, mlp_units=[512, 256], dropout=0.3, 
                 task='regression', use_multi_scale=True):
        super().__init__()
        
        self.num_features = num_features
        self.sequence_length = sequence_length
        self.d_model = d_model
        self.task = task
        self.use_multi_scale = use_multi_scale
        
        # Input normalization –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ OHLC features
        self.input_norm = layers.LayerNormalization(epsilon=1e-6)
        
        # Variable selection network
        self.vsn_dense = layers.Dense(d_model, activation='relu')
        self.vsn_grn = GatedResidualNetwork(d_model, num_features, dropout)
        
        # Multi-scale convolutions (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if use_multi_scale:
            self.multi_scale_conv = MultiScaleConv1D(d_model)
            self.projection_layer = layers.Dense(d_model)  # –ü—Ä–æ–µ–∫—Ü–∏—è –ø–æ—Å–ª–µ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏–∏
        
        # LSTM encoder
        self.lstm_encoder = layers.Bidirectional(
            layers.LSTM(d_model // 2, return_sequences=True, dropout=dropout)
        )
        
        # Positional encoding
        self.pos_encoding = PositionalEncoding(sequence_length, d_model)
        
        # Enhanced Transformer blocks
        self.transformer_blocks = [
            EnhancedTransformerBlock(d_model, num_heads, mlp_units[0], dropout)
            for _ in range(num_transformer_blocks)
        ]
        
        # Attention pooling –≤–º–µ—Å—Ç–æ GlobalAveragePooling
        self.attention_pooling = AttentionPooling(d_model)
        
        # Output layers
        if task == 'regression':
            self.output_dense = keras.Sequential([
                layers.Dense(mlp_units[0], activation='gelu'),
                layers.Dropout(dropout),
                layers.BatchNormalization(),
                layers.Dense(mlp_units[1], activation='gelu'),
                layers.Dropout(dropout),
                layers.Dense(1)  # dtype='float32' —É–±—Ä–∞–Ω–æ, —Ç–∞–∫ –∫–∞–∫ mixed precision –æ—Ç–∫–ª—é—á–µ–Ω
            ])
        else:  # classification
            self.output_dense = keras.Sequential([
                layers.Dense(mlp_units[0], activation='gelu'),
                layers.Dropout(dropout),
                layers.BatchNormalization(),
                layers.Dense(mlp_units[1], activation='gelu'),
                layers.Dropout(dropout),
                layers.Dense(1, activation='sigmoid')  # dtype='float32' —É–±—Ä–∞–Ω–æ, —Ç–∞–∫ –∫–∞–∫ mixed precision –æ—Ç–∫–ª—é—á–µ–Ω
            ])
    
    def call(self, inputs, training=False):
        # Input normalization
        normalized_inputs = self.input_norm(inputs)
        
        # Variable selection
        selected = self.vsn_dense(normalized_inputs)
        selected = self.vsn_grn(selected, training=training)
        
        # Multi-scale convolutions
        if self.use_multi_scale:
            conv_features = self.multi_scale_conv(selected)
            selected = layers.concatenate([selected, conv_features], axis=-1)
            selected = self.projection_layer(selected)  # Project back to d_model
        
        # LSTM encoding
        lstm_out = self.lstm_encoder(selected, training=training)
        
        # Positional encoding
        x = self.pos_encoding(lstm_out)
        
        # Transformer blocks
        for transformer in self.transformer_blocks:
            x = transformer(x, training=training)
        
        # Attention pooling
        pooled = self.attention_pooling(x)
        
        # Output
        return self.output_dense(pooled, training=training)


class MarketFeatureExtractor:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä—ã–Ω–æ—á–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤–∫–ª—é—á–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é —Å BTC"""
    
    def __init__(self, db_manager):
        self.db = db_manager
        self.btc_data = None
        self.market_data = {}
        
    def load_market_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö BTC –∏ —Ç–æ–ø –º–æ–Ω–µ—Ç –¥–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π"""
        logger.info("üìä –ó–∞–≥—Ä—É–∑–∫–∞ —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º BTC
        query_btc = """
        SELECT timestamp, close, volume,
               (high - low) / close as volatility
        FROM raw_market_data
        WHERE symbol = 'BTCUSDT' 
          AND market_type = 'futures'
        ORDER BY timestamp
        """
        self.btc_data = self.db.fetch_dataframe(query_btc).set_index('timestamp')
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ø –º–æ–Ω–µ—Ç—ã –¥–ª—è cross-correlations
        top_symbols = ['ETHUSDT', 'BNBUSDT', 'SOLUSDT', 'XRPUSDT']
        for symbol in top_symbols:
            query = f"""
            SELECT timestamp, close
            FROM raw_market_data
            WHERE symbol = '{symbol}'
              AND market_type = 'futures'
            ORDER BY timestamp
            """
            df = self.db.fetch_dataframe(query)
            if not df.empty:
                self.market_data[symbol] = df.set_index('timestamp')['close']
        
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ BTC: {len(self.btc_data)} –∑–∞–ø–∏—Å–µ–π")
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å: {list(self.market_data.keys())}")
    
    def calculate_features(self, df):
        """–†–∞—Å—á–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä—ã–Ω–æ—á–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤–∫–ª—é—á–∞—è OHLC features"""
        logger.info("üîß –†–∞—Å—á–µ—Ç —Ä—ã–Ω–æ—á–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ OHLC features...")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['hour'] = pd.to_datetime(df['datetime']).dt.hour
        df['day_of_week'] = pd.to_datetime(df['datetime']).dt.dayofweek
        df['day_of_month'] = pd.to_datetime(df['datetime']).dt.day
        df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
        
        # –¶–∏–∫–ª–∏—á–µ—Å–∫–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
        df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
        df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Å–∏–º–≤–æ–ª–∞–º –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –∏ OHLC features
        enhanced_dfs = []
        for symbol, symbol_df in df.groupby('symbol'):
            symbol_df = symbol_df.copy()
            
            # BTC correlation features
            if symbol != 'BTCUSDT' and self.btc_data is not None:
                # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–æ timestamp
                merged = symbol_df.merge(
                    self.btc_data[['close', 'volatility']], 
                    left_on='timestamp', 
                    right_index=True,
                    how='left',
                    suffixes=('', '_btc')
                )
                
                # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å BTC (rolling)
                if len(merged) > 20:
                    symbol_df['btc_correlation_20'] = merged['close'].rolling(20).corr(merged['close_btc'])
                    symbol_df['btc_correlation_60'] = merged['close'].rolling(60).corr(merged['close_btc'])
                else:
                    symbol_df['btc_correlation_20'] = 0
                    symbol_df['btc_correlation_60'] = 0
                
                # BTC price changes
                symbol_df['btc_return_1h'] = merged['close_btc'].pct_change(4)
                symbol_df['btc_return_4h'] = merged['close_btc'].pct_change(16)
                # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å BTC
                btc_returns = merged['close_btc'].pct_change()
                symbol_df['btc_volatility'] = btc_returns.rolling(20).std()
                
                # Relative strength to BTC
                symbol_df['relative_strength_btc'] = (
                    symbol_df['close'].pct_change(20) / 
                    merged['close_btc'].pct_change(20).replace(0, 1)
                )
            else:
                # –î–ª—è BTC –∏–ª–∏ –µ—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö
                symbol_df['btc_correlation_20'] = 1 if symbol == 'BTCUSDT' else 0
                symbol_df['btc_correlation_60'] = 1 if symbol == 'BTCUSDT' else 0
                symbol_df['btc_return_1h'] = 0
                symbol_df['btc_return_4h'] = 0
                symbol_df['btc_volatility'] = 0
                symbol_df['relative_strength_btc'] = 1
            
            # OHLC Features - –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–µ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã
            # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ close)
            symbol_df['open_ratio'] = (symbol_df['open'] / symbol_df['close'] - 1)
            symbol_df['high_ratio'] = (symbol_df['high'] / symbol_df['close'] - 1)
            symbol_df['low_ratio'] = (symbol_df['low'] / symbol_df['close'] - 1)
            symbol_df['hl_spread'] = (symbol_df['high'] - symbol_df['low']) / symbol_df['close']
            
            # –°–≤–µ—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            symbol_df['body_size'] = np.abs(symbol_df['close'] - symbol_df['open']) / symbol_df['close']
            symbol_df['upper_shadow'] = (symbol_df['high'] - symbol_df[['open', 'close']].max(axis=1)) / symbol_df['close']
            symbol_df['lower_shadow'] = (symbol_df[['open', 'close']].min(axis=1) - symbol_df['low']) / symbol_df['close']
            symbol_df['is_bullish'] = (symbol_df['close'] > symbol_df['open']).astype(int)
            
            # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–µ returns –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            symbol_df['log_return'] = np.log(symbol_df['close'] / symbol_df['close'].shift(1)).fillna(0)
            symbol_df['log_volume'] = np.log(symbol_df['volume'] + 1)
            
            # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫ —Å–∫–æ–ª—å–∑—è—â–∏–º —Å—Ä–µ–¥–Ω–∏–º
            if 'technical_indicators' in symbol_df.columns:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º EMA –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ technical_indicators
                ema_15_values = []
                ema_50_values = []
                vwap_values = []
                
                for idx, row in symbol_df.iterrows():
                    indicators = row['technical_indicators']
                    ema_15_values.append(indicators.get('ema_15', row['close']))
                    ema_50_values.append(indicators.get('ema_50', row['close']))
                    vwap_values.append(indicators.get('vwap_val', row['close']))
                
                symbol_df['price_to_ema15'] = symbol_df['close'] / pd.Series(ema_15_values, index=symbol_df.index) - 1
                symbol_df['price_to_ema50'] = symbol_df['close'] / pd.Series(ema_50_values, index=symbol_df.index) - 1
                symbol_df['price_to_vwap'] = symbol_df['close'] / pd.Series(vwap_values, index=symbol_df.index) - 1
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –µ—Å–ª–∏ technical_indicators –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
                symbol_df['price_to_ema15'] = 0.0
                symbol_df['price_to_ema50'] = 0.0
                symbol_df['price_to_vwap'] = 0.0
            
            # Symbol embeddings
            # –¢–æ–ø 10 –º–æ–Ω–µ—Ç –ø–æ –∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏–∏
            top_symbols = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'SOLUSDT', 'XRPUSDT', 
                          'ADAUSDT', 'DOGEUSDT', 'AVAXUSDT', 'DOTUSDT', 'MATICUSDT']
            
            for top_symbol in top_symbols:
                symbol_df[f'is_{top_symbol.lower().replace("usdt", "")}'] = int(symbol == top_symbol)
            
            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –º–æ–Ω–µ—Ç
            major_coins = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT']
            meme_coins = ['DOGEUSDT', 'SHIBUSDT', 'PEPEUSDT', '1000PEPEUSDT', 'FLOKIUSDT', 'WIFUSDT']
            defi_coins = ['AAVEUSDT', 'UNIUSDT', 'CAKEUSDT', 'DYDXUSDT']
            
            symbol_df['is_major'] = int(symbol in major_coins)
            symbol_df['is_meme'] = int(symbol in meme_coins)
            symbol_df['is_defi'] = int(symbol in defi_coins)
            symbol_df['is_alt'] = int(not (symbol in major_coins or symbol in meme_coins or symbol in defi_coins))
            
            # Market regime (volatility-based)
            if 'volatility_16' in symbol_df.columns:
                vol_percentile = symbol_df['volatility_16'].rolling(100).rank(pct=True)
                symbol_df['market_regime_low_vol'] = (vol_percentile < 0.33).astype(int)
                symbol_df['market_regime_med_vol'] = ((vol_percentile >= 0.33) & (vol_percentile < 0.67)).astype(int)
                symbol_df['market_regime_high_vol'] = (vol_percentile >= 0.67).astype(int)
            else:
                symbol_df['market_regime_low_vol'] = 0
                symbol_df['market_regime_med_vol'] = 1
                symbol_df['market_regime_high_vol'] = 0
            
            # –£–º–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ NaN –∑–Ω–∞—á–µ–Ω–∏–π
            numeric_columns = symbol_df.select_dtypes(include=[np.number]).columns
            
            # –î–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º forward fill, –∑–∞—Ç–µ–º backward fill
            for col in numeric_columns:
                if 'correlation' in col or 'return' in col or 'ratio' in col:
                    # –î–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –∏ returns –∏—Å–ø–æ–ª—å–∑—É–µ–º forward fill
                    symbol_df[col] = symbol_df[col].ffill().bfill()
                else:
                    # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö - –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
                    symbol_df[col] = symbol_df[col].fillna(0)
            
            enhanced_dfs.append(symbol_df)
        
        result_df = pd.concat(enhanced_dfs, ignore_index=True)
        logger.info("‚úÖ –†—ã–Ω–æ—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã")
        return result_df


class EnhancedTransformerTrainer:
    """Enhanced Trainer —Å gradient accumulation –∏ ensemble support"""
    
    def __init__(self, db_manager: PostgreSQLManager, config_path='config.yaml', 
                 task='regression', ensemble_size=1, test_mode=False):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
            
        self.db = db_manager
        self.task = task
        self.ensemble_size = ensemble_size
        self.test_mode = test_mode
        self.sequence_length = self.config['model']['sequence_length']
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞
        if self.test_mode:
            self.batch_size = 8  # –ï—â–µ –º–µ–Ω—å—à–µ –¥–ª—è —Ç–µ—Å—Ç–∞
            self.gradient_accumulation_steps = 2  # –ë—ã—Å—Ç—Ä–µ–µ
            self.epochs = 3  # –¢–æ–ª—å–∫–æ 3 —ç–ø–æ—Ö–∏
            self.test_symbols = ['BTCUSDT', 'ETHUSDT']  # –¢–æ–ª—å–∫–æ 2 —Å–∏–º–≤–æ–ª–∞
            logger.info("‚ö° –¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º: batch_size=8, epochs=3, symbols=2")
        else:
            self.batch_size = 8  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è RTX 4090 (24GB)
            self.gradient_accumulation_steps = 4  # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch_size = 32
            self.epochs = 100
        
        # Enhanced –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è RTX 4090)
        self.d_model = 128  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 256
        self.num_heads = 8
        self.num_transformer_blocks = 4  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 6
        self.ff_dim = 256  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 512
        self.dropout_rate = 0.3  # –£–≤–µ–ª–∏—á–µ–Ω–æ
        
        self.scaler = RobustScaler()
        self.models = {}
        self.feature_columns = None
        self.log_dir = log_dir
        
        # Market feature extractor
        self.market_extractor = MarketFeatureExtractor(db_manager)
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        self.TECHNICAL_INDICATORS = [
            # –¢—Ä–µ–Ω–¥–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            'ema_15', 'adx_val', 'adx_plus_di', 'adx_minus_di',
            'macd_val', 'macd_signal', 'macd_hist', 'sar',
            'ichimoku_conv', 'ichimoku_base', 'aroon_up', 'aroon_down',
            
            # –û—Å—Ü–∏–ª–ª—è—Ç–æ—Ä—ã
            'rsi_val', 'stoch_k', 'stoch_d', 'cci_val', 'roc_val',
            'williams_r', 'awesome_osc', 'ultimate_osc',
            
            # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
            'atr_val', 'bb_position', 'bb_width', 'donchian_position',
            'keltner_position', 'ulcer_index', 'mass_index',
            
            # –û–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            'obv_val', 'obv_signal', 'cmf_val', 'force_index',
            'eom_val', 'vpt_val', 'nvi_val', 'vwap_val',
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ
            'ema_50', 'ema_200', 'trix_val', 'trix_signal',
            'vortex_pos', 'vortex_neg', 'vortex_ratio',
            'price_change_1', 'price_change_4', 'price_change_16',
            'volatility_4', 'volatility_16', 'volume_ratio'
        ]
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ market features
        self.MARKET_FEATURES = [
            'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'is_weekend',
            'btc_correlation_20', 'btc_correlation_60',
            'btc_return_1h', 'btc_return_4h', 'btc_volatility',
            'relative_strength_btc',
            'market_regime_low_vol', 'market_regime_med_vol', 'market_regime_high_vol'
        ]
        
        # OHLC features
        self.OHLC_FEATURES = [
            'open_ratio', 'high_ratio', 'low_ratio', 'hl_spread',
            'body_size', 'upper_shadow', 'lower_shadow', 'is_bullish',
            'log_return', 'log_volume',
            'price_to_ema15', 'price_to_ema50', 'price_to_vwap'
        ]
        
        # Symbol embeddings
        self.SYMBOL_FEATURES = [
            'is_btc', 'is_eth', 'is_bnb', 'is_sol', 'is_xrp',
            'is_ada', 'is_doge', 'is_avax', 'is_dot', 'is_matic',
            'is_major', 'is_meme', 'is_defi', 'is_alt',
            'market_regime_low_vol', 'market_regime_med_vol', 'market_regime_high_vol'
        ]
        
        self.feature_importance = {}
        
    def convert_to_binary_labels(self, returns, threshold=0.3):
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ expected returns –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏"""
        return (returns > threshold).astype(np.float32)
    
    def create_model_with_warmup(self, input_shape, name):
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å warmup learning rate"""
        model = EnhancedTemporalFusionTransformer(
            num_features=input_shape[1],
            sequence_length=input_shape[0],
            d_model=self.d_model,
            num_heads=self.num_heads,
            num_transformer_blocks=self.num_transformer_blocks,
            mlp_units=[self.ff_dim, self.ff_dim//2],
            dropout=self.dropout_rate,
            task=self.task,
            use_multi_scale=True
        )
        
        # Warmup schedule - —É–º–µ–Ω—å—à–∞–µ–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Å mixed precision
        initial_learning_rate = 0.00001  # –£–º–µ–Ω—å—à–µ–Ω–æ –≤ 10 —Ä–∞–∑
        target_learning_rate = 0.0001   # –£–º–µ–Ω—å—à–µ–Ω–æ –≤ 3 —Ä–∞–∑–∞
        warmup_steps = 2000  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º warmup –ø–µ—Ä–∏–æ–¥
        
        lr_schedule = keras.optimizers.schedules.PolynomialDecay(
            initial_learning_rate,
            decay_steps=warmup_steps,
            end_learning_rate=target_learning_rate,
            power=1.0
        )
        
        optimizer = keras.optimizers.Adam(
            learning_rate=lr_schedule,
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-7
        )
        
        # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è mixed precision
        # TODO: —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–æ–≥–¥–∞ –≤–∫–ª—é—á–∏–º mixed precision –æ–±—Ä–∞—Ç–Ω–æ
        # if tf.keras.mixed_precision.global_policy().name == 'mixed_float16':
        #     optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)
        
        # –ö–æ–º–ø–∏–ª—è—Ü–∏—è
        if self.task == 'regression':
            model.compile(
                optimizer=optimizer,
                loss=keras.losses.Huber(delta=1.0),
                metrics=[
                    keras.metrics.MeanAbsoluteError(name='mae'),
                    keras.metrics.RootMeanSquaredError(name='rmse')
                ]
            )
        else:  # classification
            model.compile(
                optimizer=optimizer,
                loss=FocalLoss(alpha=0.25, gamma=2.0, label_smoothing=0.05),
                metrics=[
                    keras.metrics.BinaryAccuracy(name='accuracy'),
                    keras.metrics.Precision(name='precision'),
                    keras.metrics.Recall(name='recall')
                ]
            )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        dummy_input = tf.zeros((1, input_shape[0], input_shape[1]))
        _ = model(dummy_input)
        
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ enhanced –º–æ–¥–µ–ª—å {name} —Å {model.count_params():,} –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏")
        logger.info(f"   –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch size: {self.batch_size * self.gradient_accumulation_steps}")
        
        return model
    
    def augment_data(self, X, y, augmentation_factor=0.1):
        """–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å noise injection –∏ mixup (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ 50% –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        sample_size = len(X) // 2
        sample_indices = np.random.choice(len(X), sample_size, replace=False)
        X_sample = X[sample_indices]
        y_sample = y[sample_indices]
        
        augmented_X = []
        augmented_y = []
        
        # –¢–æ–ª—å–∫–æ Mixup (–±–µ–∑ noise injection –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏)
        indices = np.random.permutation(sample_size)
        lambda_mix = np.random.beta(0.2, 0.2, size=(sample_size, 1, 1))
        X_mixed = lambda_mix * X_sample + (1 - lambda_mix) * X_sample[indices]
        
        if self.task == 'regression':
            lambda_y = lambda_mix.squeeze()
            y_mixed = lambda_y * y_sample + (1 - lambda_y) * y_sample[indices]
        else:
            # –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ lambda
            lambda_y = lambda_mix.squeeze()
            y_mixed = (lambda_y * y_sample + (1 - lambda_y) * y_sample[indices] > 0.5).astype(np.float32)
        
        augmented_X.append(X_mixed)
        augmented_y.append(y_mixed)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º (–æ—Ä–∏–≥–∏–Ω–∞–ª + mixup)
        X_augmented = np.concatenate([X] + augmented_X, axis=0)
        y_augmented = np.concatenate([y] + augmented_y, axis=0)
        
        # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º
        shuffle_indices = np.random.permutation(len(X_augmented))
        
        return X_augmented[shuffle_indices], y_augmented[shuffle_indices]
    
    def load_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å market features"""
        logger.info("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL...")
        start_time = time.time()
        
        # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤
        symbols_query = """
        SELECT DISTINCT p.symbol 
        FROM processed_market_data p
        JOIN raw_market_data r ON p.raw_data_id = r.id
        WHERE p.technical_indicators IS NOT NULL
          AND r.market_type = 'futures'
        ORDER BY p.symbol
        """
        
        symbols_df = self.db.fetch_dataframe(symbols_query)
        symbols = symbols_df['symbol'].tolist()
        
        # –í —Ç–µ—Å—Ç–æ–≤–æ–º —Ä–µ–∂–∏–º–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Å–∏–º–≤–æ–ª—ã
        if self.test_mode:
            symbols = [s for s in symbols if s in self.test_symbols]
            logger.info(f"‚ö° –¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º: –∑–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ {symbols}")
        
        logger.info(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(symbols)} —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Å–∏–º–≤–æ–ª–∞–º –±–∞—Ç—á–∞–º–∏
        all_data = []
        batch_size = 5  # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ 5 —Å–∏–º–≤–æ–ª–æ–≤ –∑–∞ —Ä–∞–∑
        
        from tqdm import tqdm
        for i in tqdm(range(0, len(symbols), batch_size), desc="–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞—Ç—á–µ–π"):
            batch_symbols = symbols[i:i+batch_size]
            symbols_str = "', '".join(batch_symbols)
            
            batch_query = f"""
            SELECT 
                p.symbol, p.timestamp, p.datetime,
                p.technical_indicators,
                p.buy_expected_return,
                p.sell_expected_return,
                p.is_long_entry,
                p.is_short_entry,
                p.open, p.high, p.low, p.close, p.volume
            FROM processed_market_data p
            JOIN raw_market_data r ON p.raw_data_id = r.id
            WHERE p.technical_indicators IS NOT NULL
              AND r.market_type = 'futures'
              AND p.symbol IN ('{symbols_str}')
              {"AND p.timestamp > EXTRACT(EPOCH FROM (CURRENT_TIMESTAMP - INTERVAL '10 days'))::bigint" if self.test_mode else ""}
            ORDER BY p.symbol, p.timestamp
            """
            
            try:
                batch_df = self.db.fetch_dataframe(batch_query)
                all_data.append(batch_df)
                logger.info(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(batch_df):,} –∑–∞–ø–∏—Å–µ–π –¥–ª—è {batch_symbols}")
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {batch_symbols}: {e}")
                # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ –æ–¥–Ω–æ–º—É —Å–∏–º–≤–æ–ª—É
                for symbol in batch_symbols:
                    try:
                        single_query = f"""
                        SELECT 
                            p.symbol, p.timestamp, p.datetime,
                            p.technical_indicators,
                            p.buy_expected_return,
                            p.sell_expected_return,
                            p.is_long_entry,
                            p.is_short_entry,
                            p.open, p.high, p.low, p.close, p.volume
                        FROM processed_market_data p
                        JOIN raw_market_data r ON p.raw_data_id = r.id
                        WHERE p.technical_indicators IS NOT NULL
                          AND r.market_type = 'futures'
                          AND p.symbol = '{symbol}'
                        ORDER BY p.timestamp
                        """
                        single_df = self.db.fetch_dataframe(single_query)
                        all_data.append(single_df)
                        logger.info(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(single_df):,} –∑–∞–ø–∏—Å–µ–π –¥–ª—è {symbol}")
                    except Exception as e2:
                        logger.error(f"   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {symbol}: {e2}")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        if not all_data:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ!")
        
        df = pd.concat(all_data, ignore_index=True)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º market data
        self.market_extractor.load_market_data()
        
        # –î–æ–±–∞–≤–ª—è–µ–º market features
        df = self.market_extractor.calculate_features(df)
        
        load_time = time.time() - start_time
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –∑–∞ {load_time:.2f} —Å–µ–∫—É–Ω–¥ ({load_time/60:.1f} –º–∏–Ω—É—Ç)")
        
        if len(df) == 0:
            raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        symbol_counts = df['symbol'].value_counts()
        logger.info("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–∏–º–≤–æ–ª–∞–º:")
        for symbol, count in symbol_counts.items():
            logger.info(f"   {symbol}: {count:,} –∑–∞–ø–∏—Å–µ–π")
        
        return df
    
    def prepare_features_and_targets(self, df):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å market features"""
        logger.info("üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...")
        start_time = time.time()
        
        all_features = self.TECHNICAL_INDICATORS + self.MARKET_FEATURES + self.OHLC_FEATURES + self.SYMBOL_FEATURES
        logger.info(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {len(all_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        logger.info(f"   - –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã: {len(self.TECHNICAL_INDICATORS)}")
        logger.info(f"   - Market features: {len(self.MARKET_FEATURES)}")
        logger.info(f"   - OHLC features: {len(self.OHLC_FEATURES)}")
        logger.info(f"   - Symbol features: {len(self.SYMBOL_FEATURES)}")
        
        # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        train_end = int(len(df) * 0.7)
        val_end = int(len(df) * 0.85)
        
        df['split'] = 'test'
        df.loc[df.index < train_end, 'split'] = 'train'
        df.loc[(df.index >= train_end) & (df.index < val_end), 'split'] = 'val'
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ split
        grouped_data = {
            'train': {'X': [], 'y_buy': [], 'y_sell': [], 'symbols': []},
            'val': {'X': [], 'y_buy': [], 'y_sell': [], 'symbols': []},
            'test': {'X': [], 'y_buy': [], 'y_sell': [], 'symbols': []}
        }
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        from tqdm import tqdm
        for symbol, symbol_df in tqdm(df.groupby('symbol'), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∏–º–≤–æ–ª–æ–≤"):
            for split_name, split_df in symbol_df.groupby('split'):
                X_split = []
                
                for _, row in split_df.iterrows():
                    feature_values = []
                    
                    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
                    indicators = row['technical_indicators']
                    for indicator in self.TECHNICAL_INDICATORS:
                        if indicator in ['volatility_4', 'volatility_16']:
                            # –≠—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏—è –º–æ–≥—É—Ç –±—ã—Ç—å –≤ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö
                            value = row.get(indicator, indicators.get(indicator, 0.0))
                        else:
                            value = indicators.get(indicator, 0.0)
                        
                        if value is None or pd.isna(value):
                            value = 0.0
                        feature_values.append(float(value))
                    
                    # Market features
                    for feature in self.MARKET_FEATURES:
                        value = row.get(feature, 0.0)
                        if value is None or pd.isna(value):
                            value = 0.0
                        feature_values.append(float(value))
                    
                    # OHLC features
                    for feature in self.OHLC_FEATURES:
                        value = row.get(feature, 0.0)
                        if value is None or pd.isna(value):
                            value = 0.0
                        feature_values.append(float(value))
                    
                    # Symbol features
                    for feature in self.SYMBOL_FEATURES:
                        value = row.get(feature, 0.0)
                        if value is None or pd.isna(value):
                            value = 0.0
                        feature_values.append(float(value))
                    
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                    # RSI extremes
                    rsi = indicators.get("rsi_val", 50.0)
                    feature_values.append(1.0 if rsi is not None and rsi < 30 else 0.0)
                    feature_values.append(1.0 if rsi is not None and rsi > 70 else 0.0)
                    
                    # MACD signal
                    macd = indicators.get("macd_val", 0.0)
                    macd_signal = indicators.get("macd_signal", 0.0)
                    feature_values.append(1.0 if macd is not None and macd_signal is not None and macd > macd_signal else 0.0)
                    
                    # Bollinger bands position
                    bb_position = indicators.get("bb_position", 0.5)
                    feature_values.append(1.0 if bb_position is not None and bb_position < 0.2 else 0.0)
                    feature_values.append(1.0 if bb_position is not None and bb_position > 0.8 else 0.0)
                    
                    # Trend strength
                    adx = indicators.get("adx_val", 0.0)
                    feature_values.append(1.0 if adx is not None and adx > 25 else 0.0)
                    
                    # Volume spike
                    volume_ratio = indicators.get("volume_ratio", 1.0)
                    feature_values.append(1.0 if volume_ratio is not None and volume_ratio > 2.0 else 0.0)
                    
                    X_split.append(feature_values)
                
                # –¶–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                y_buy = split_df["buy_expected_return"].values.astype(float)
                y_sell = split_df["sell_expected_return"].values.astype(float)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π split
                grouped_data[split_name]["X"].extend(X_split)
                grouped_data[split_name]["y_buy"].extend(y_buy)
                grouped_data[split_name]["y_sell"].extend(y_sell)
                grouped_data[split_name]["symbols"].extend([symbol] * len(X_split))
        
        prep_time = time.time() - start_time
        logger.info(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {prep_time:.2f} —Å–µ–∫—É–Ω–¥ ({prep_time/60:.1f} –º–∏–Ω—É—Ç)")
        
        return grouped_data
    
    def create_sequences(self, X, y, symbols, stride=3):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Å –º–µ–Ω—å—à–∏–º stride –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        logger.info(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª–∏–Ω–æ–π {self.sequence_length}...")
        start_time = time.time()
        
        sequences = []
        targets = []
        seq_symbols = []
        
        unique_symbols = np.unique(symbols)
        
        for symbol in unique_symbols:
            symbol_mask = symbols == symbol
            symbol_indices = np.where(symbol_mask)[0]
            
            if len(symbol_indices) < self.sequence_length + 1:
                continue
            
            X_symbol = X[symbol_indices]
            y_symbol = y[symbol_indices]
            
            # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å –º–µ–Ω—å—à–∏–º stride
            for i in range(0, len(X_symbol) - self.sequence_length, stride):
                sequences.append(X_symbol[i:i + self.sequence_length])
                targets.append(y_symbol[i + self.sequence_length])
                seq_symbols.append(symbol)
        
        sequences = np.array(sequences)
        targets = np.array(targets)
        
        seq_time = time.time() - start_time
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(sequences)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∑–∞ {seq_time:.2f} —Å–µ–∫—É–Ω–¥")
        logger.info(f"   –§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {sequences.shape}")
        
        return sequences, targets, np.array(seq_symbols)
    
    def train_model_with_gradient_accumulation(self, model, X_train, y_train, X_val, y_val, model_name):
        """–û–±—É—á–µ–Ω–∏–µ —Å gradient accumulation –¥–ª—è –±–æ–ª—å—à–µ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ batch size"""
        logger.info(f"\n{'='*70}")
        logger.info(f"üöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø: {model_name}")
        logger.info(f"{'='*70}")
        
        # Custom training loop –¥–ª—è gradient accumulation
        @tf.function
        def train_step(inputs, targets, accumulation_steps):
            accumulated_gradients = [tf.zeros_like(var) for var in model.trainable_variables]
            
            for i in range(accumulation_steps):
                start_idx = i * self.batch_size
                end_idx = (i + 1) * self.batch_size
                
                batch_inputs = inputs[start_idx:end_idx]
                batch_targets = targets[start_idx:end_idx]
                
                with tf.GradientTape() as tape:
                    predictions = model(batch_inputs, training=True)
                    loss = model.compiled_loss(batch_targets, predictions)
                    loss = loss / accumulation_steps
                
                gradients = tape.gradient(loss, model.trainable_variables)
                accumulated_gradients = [
                    acc_grad + grad for acc_grad, grad in zip(accumulated_gradients, gradients)
                ]
            
            model.optimizer.apply_gradients(zip(accumulated_gradients, model.trainable_variables))
            return loss * accumulation_steps
        
        # Callbacks
        callbacks = [
            VisualizationCallback(self.log_dir, model_name, update_freq=5, task=self.task),
            
            MemoryCleanupCallback(),  # –î–æ–±–∞–≤–ª—è–µ–º –æ—á–∏—Å—Ç–∫—É –ø–∞–º—è—Ç–∏
            
            keras.callbacks.EarlyStopping(
                monitor='val_loss',
                patience=25,
                restore_best_weights=True,
                verbose=1
            ),
            
            # –£–±—Ä–∞–Ω–æ LearningRateScheduler - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CosineDecay –≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–µ
            
            keras.callbacks.ModelCheckpoint(
                filepath=f'trained_model/{model_name}_best.h5',
                monitor='val_loss',
                save_best_only=True,
                verbose=1,
                save_weights_only=False  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å—é –º–æ–¥–µ–ª—å
            ),
            
            keras.callbacks.TensorBoard(
                log_dir=f'{self.log_dir}/tensorboard/{model_name}',
                histogram_freq=1,
                write_graph=True,
                update_freq='epoch'
            )
        ]
        
        # –û–±—É—á–µ–Ω–∏–µ —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π
        if self.task == 'classification_binary':
            # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
            X_train_aug, y_train_aug = self.augment_data(X_train, y_train)
            
            # –ü–æ–¥—Å—á–µ—Ç –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤
            unique, counts = np.unique(y_train_aug, return_counts=True)
            class_weight = {0: counts[1] / counts[0], 1: 1.0} if len(unique) == 2 else None
            
            if class_weight:
                logger.info(f"üìä –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –ø–æ—Å–ª–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ - 0: {counts[0]:,}, 1: {counts[1]:,}")
                logger.info(f"‚öñÔ∏è –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ - 0: {class_weight[0]:.2f}, 1: {class_weight[1]:.2f}")
            
            history = model.fit(
                X_train_aug, y_train_aug,
                validation_data=(X_val, y_val),
                epochs=self.epochs,
                batch_size=self.batch_size * self.gradient_accumulation_steps,
                callbacks=callbacks,
                class_weight=class_weight,
                verbose=1
            )
        else:
            X_train_aug, y_train_aug = self.augment_data(X_train, y_train)
            
            history = model.fit(
                X_train_aug, y_train_aug,
                validation_data=(X_val, y_val),
                epochs=self.epochs,
                batch_size=self.batch_size * self.gradient_accumulation_steps,
                callbacks=callbacks,
                verbose=1
            )
        
        return history
    
    def evaluate_model_with_uncertainty(self, model, X_test, y_test, model_name, n_samples=7):
        """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —Å uncertainty estimation —á–µ—Ä–µ–∑ MC Dropout"""
        logger.info(f"\nüìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ {model_name} —Å uncertainty estimation...")
        
        # –ë–∞—Ç—á–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        batch_size = 1000  # –ú–µ–Ω—å—à–∏–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        n_samples_test = len(X_test)
        predictions = []
        
        # Multiple forward passes —Å dropout
        for sample_idx in range(n_samples):
            batch_predictions = []
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ –±–∞—Ç—á–∞–º
            for i in range(0, n_samples_test, batch_size):
                batch_X = X_test[i:min(i + batch_size, n_samples_test)]
                batch_pred = model(batch_X, training=True)  # training=True –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ dropout
                batch_predictions.append(batch_pred.numpy())
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–∞—Ç—á–µ–π
            predictions.append(np.concatenate(batch_predictions, axis=0))
        
        predictions = np.array(predictions)
        
        # –°—Ä–µ–¥–Ω–µ–µ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        y_pred_mean = predictions.mean(axis=0).flatten()
        y_pred_std = predictions.std(axis=0).flatten()
        
        if self.task == 'regression':
            # –ú–µ—Ç—Ä–∏–∫–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
            mae = mean_absolute_error(y_test, y_pred_mean)
            mse = mean_squared_error(y_test, y_pred_mean)
            rmse = np.sqrt(mse)
            r2 = r2_score(y_test, y_pred_mean)
            direction_accuracy = np.mean((y_pred_mean > 0) == (y_test > 0))
            
            # –ê–Ω–∞–ª–∏–∑ uncertainty
            high_confidence_mask = y_pred_std < np.percentile(y_pred_std, 30)
            high_conf_mae = mean_absolute_error(y_test[high_confidence_mask], y_pred_mean[high_confidence_mask])
            
            logger.info(f"\nüìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã {model_name}:")
            logger.info(f"   MAE: {mae:.4f}")
            logger.info(f"   RMSE: {rmse:.4f}")
            logger.info(f"   R¬≤: {r2:.4f}")
            logger.info(f"   Direction Accuracy: {direction_accuracy:.2%}")
            logger.info(f"   High Confidence MAE: {high_conf_mae:.4f}")
            logger.info(f"   Mean Uncertainty: {y_pred_std.mean():.4f}")
            
            metrics = {
                'mae': mae,
                'rmse': rmse,
                'r2': r2,
                'direction_accuracy': direction_accuracy,
                'high_conf_mae': high_conf_mae,
                'mean_uncertainty': y_pred_std.mean()
            }
        else:  # classification
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –≤–º–µ—Å—Ç–æ 0.5
            optimal_threshold = getattr(self, f'optimal_threshold_{model_name}', 0.5)
            y_pred_binary = (y_pred_mean > optimal_threshold).astype(int)
            
            # –ú–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            accuracy = accuracy_score(y_test, y_pred_binary)
            precision = precision_score(y_test, y_pred_binary, zero_division=0)
            recall = recall_score(y_test, y_pred_binary, zero_division=0)
            f1 = f1_score(y_test, y_pred_binary, zero_division=0)
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ confidence
            confidence = 1 - y_pred_std
            high_conf_mask = confidence > 0.7
            
            if high_conf_mask.sum() > 0:
                high_conf_accuracy = accuracy_score(
                    y_test[high_conf_mask], 
                    y_pred_binary[high_conf_mask]
                )
                high_conf_precision = precision_score(
                    y_test[high_conf_mask], 
                    y_pred_binary[high_conf_mask],
                    zero_division=0
                )
            else:
                high_conf_accuracy = 0
                high_conf_precision = 0
            
            logger.info(f"\nüìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã {model_name}:")
            logger.info(f"   Accuracy: {accuracy:.2%}")
            logger.info(f"   Precision: {precision:.2%}")
            logger.info(f"   Recall: {recall:.2%}")
            logger.info(f"   F1-Score: {f1:.2%}")
            logger.info(f"   High Confidence Samples: {high_conf_mask.sum()} ({high_conf_mask.mean():.1%})")
            logger.info(f"   High Confidence Accuracy: {high_conf_accuracy:.2%}")
            logger.info(f"   High Confidence Precision: {high_conf_precision:.2%}")
            
            metrics = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'high_conf_accuracy': high_conf_accuracy,
                'high_conf_precision': high_conf_precision,
                'high_conf_samples': high_conf_mask.sum()
            }
        
        return metrics
    
    def optimize_threshold(self, model, X_val, y_val, model_name):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        logger.info(f"üéØ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ –¥–ª—è {model_name}...")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred_proba = model.predict(X_val, verbose=0).flatten()
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ—Ä–æ–≥–∏
        thresholds = np.arange(0.3, 0.7, 0.05)
        best_f1 = 0
        best_threshold = 0.5
        
        for threshold in thresholds:
            y_pred = (y_pred_proba > threshold).astype(int)
            f1 = f1_score(y_val, y_pred, zero_division=0)
            
            if f1 > best_f1:
                best_f1 = f1
                best_threshold = threshold
        
        logger.info(f"   –õ—É—á—à–∏–π –ø–æ—Ä–æ–≥: {best_threshold:.2f} (F1: {best_f1:.3f})")
        return best_threshold
    
    def post_process_predictions(self, predictions, timestamps, symbols, min_interval=4):
        """Post-processing –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏"""
        processed_predictions = predictions.copy()
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        for symbol in np.unique(symbols):
            symbol_mask = symbols == symbol
            symbol_preds = processed_predictions[symbol_mask]
            symbol_times = timestamps[symbol_mask]
            
            # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –≤ —Ç–µ—á–µ–Ω–∏–µ min_interval —Å–≤–µ—á–µ–π
            last_signal_time = -np.inf
            for i in range(len(symbol_preds)):
                if symbol_preds[i] == 1:
                    if i - last_signal_time < min_interval:
                        processed_predictions[symbol_mask][i] = 0
                    else:
                        last_signal_time = i
        
        return processed_predictions
    
    def train_ensemble(self):
        """–û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π"""
        logger.info("="*80)
        logger.info(f"üöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø ENHANCED TRANSFORMER ENSEMBLE (v2.1)")
        logger.info(f"üìä –†–µ–∂–∏–º: {self.task}")
        logger.info(f"üéØ –†–∞–∑–º–µ—Ä –∞–Ω—Å–∞–º–±–ª—è: {self.ensemble_size}")
        logger.info("="*80)
        
        try:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            os.makedirs('trained_model', exist_ok=True)
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            df = self.load_data()
            grouped_data = self.prepare_features_and_targets(df)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy arrays
            X_train = np.array(grouped_data['train']['X'])
            X_val = np.array(grouped_data['val']['X'])
            X_test = np.array(grouped_data['test']['X'])
            
            all_buy = grouped_data['train']['y_buy'] + grouped_data['val']['y_buy'] + grouped_data['test']['y_buy']
            all_sell = grouped_data['train']['y_sell'] + grouped_data['val']['y_sell'] + grouped_data['test']['y_sell']
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            logger.info("üîÑ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
            X_train = self.scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
            X_val = self.scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)
            X_test = self.scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            if self.task == 'regression':
                logger.info("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ expected returns:")
                logger.info(f"   Buy - –°—Ä–µ–¥–Ω–µ–µ: {np.mean(all_buy):.3f}%")
                logger.info(f"   Buy - Std: {np.std(all_buy):.3f}%")
                logger.info(f"   Sell - –°—Ä–µ–¥–Ω–µ–µ: {np.mean(all_sell):.3f}%")
                logger.info(f"   Sell - Std: {np.std(all_sell):.3f}%")
            else:
                buy_binary = self.convert_to_binary_labels(np.array(all_buy), threshold=0.3)
                sell_binary = self.convert_to_binary_labels(np.array(all_sell), threshold=0.3)
                
                logger.info("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∏–Ω–∞—Ä–Ω—ã—Ö –º–µ—Ç–æ–∫ (–ø–æ—Ä–æ–≥ > 0.3%):")
                logger.info(f"   Buy - –ö–ª–∞—Å—Å 1: {np.mean(buy_binary):.1%}")
                logger.info(f"   Sell - –ö–ª–∞—Å—Å 1: {np.mean(sell_binary):.1%}")
            
            # –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è
            results = {}
            ensemble_models = {'buy': [], 'sell': []}
            
            for model_type in ['buy', 'sell']:
                logger.info(f"\n{'='*60}")
                logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π: {model_type}")
                logger.info(f"{'='*60}")
                
                for ensemble_idx in range(self.ensemble_size):
                    model_name = f"{model_type}_enhanced_v2.1_{ensemble_idx}"
                    
                    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Å —Ä–∞–∑–Ω—ã–º stride –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
                    stride = 3 + ensemble_idx  # 3, 4, 5...
                    
                    if model_type == 'buy':
                        y_data = {'train': grouped_data['train']['y_buy'],
                                 'val': grouped_data['val']['y_buy'],
                                 'test': grouped_data['test']['y_buy']}
                    else:
                        y_data = {'train': grouped_data['train']['y_sell'],
                                 'val': grouped_data['val']['y_sell'],
                                 'test': grouped_data['test']['y_sell']}
                    
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                    if self.task == 'classification_binary':
                        for split in y_data:
                            y_data[split] = self.convert_to_binary_labels(np.array(y_data[split]))
                    
                    # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                    X_train_seq, y_train_seq, _ = self.create_sequences(
                        X_train, np.array(y_data['train']), 
                        np.array(grouped_data['train']['symbols']), stride=stride
                    )
                    X_val_seq, y_val_seq, _ = self.create_sequences(
                        X_val, np.array(y_data['val']),
                        np.array(grouped_data['val']['symbols']), stride=stride
                    )
                    X_test_seq, y_test_seq, _ = self.create_sequences(
                        X_test, np.array(y_data['test']),
                        np.array(grouped_data['test']['symbols']), stride=stride
                    )
                    
                    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
                    model = self.create_model_with_warmup((self.sequence_length, X_train.shape[1]), model_name)
                    
                    # –û–±—É—á–µ–Ω–∏–µ —Å gradient accumulation
                    history = self.train_model_with_gradient_accumulation(
                        model, X_train_seq, y_train_seq, X_val_seq, y_val_seq, model_name
                    )
                    
                    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                    if self.task == 'classification_binary':
                        optimal_threshold = self.optimize_threshold(model, X_val_seq, y_val_seq, model_name)
                        setattr(self, f'optimal_threshold_{model_name}', optimal_threshold)
                    
                    # –û—Ü–µ–Ω–∫–∞ —Å uncertainty
                    metrics = self.evaluate_model_with_uncertainty(
                        model, X_test_seq, y_test_seq, model_name
                    )
                    
                    results[model_name] = metrics
                    ensemble_models[model_type].append(model)
                    self.models[model_name] = model
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            self.save_enhanced_results(results, ensemble_models)
            
            logger.info("\n‚úÖ –û–ë–£–ß–ï–ù–ò–ï ENHANCED ENSEMBLE –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!")
            logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {self.log_dir}")
            
        except Exception as e:
            logger.error(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
            logger.error("–¢—Ä–µ–π—Å–±–µ–∫:", exc_info=True)
            raise
    
    def convert_numpy_types(self, obj):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è numpy —Ç–∏–ø–æ–≤ –¥–ª—è JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏"""
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {key: self.convert_numpy_types(value) for key, value in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [self.convert_numpy_types(item) for item in obj]
        return obj
    
    def save_enhanced_results(self, results, ensemble_models):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ enhanced –º–æ–¥–µ–ª–∏"""
        logger.info("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ enhanced –º–æ–¥–µ–ª–µ–π –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
        for name, model in self.models.items():
            model_path = f'trained_model/{name}.h5'
            model.save(model_path)
            logger.info(f"   ‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler
        with open('trained_model/scaler_v2.1.pkl', 'wb') as f:
            pickle.dump(self.scaler, f)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é enhanced features
        feature_config = {
            'technical_indicators': self.TECHNICAL_INDICATORS,
            'market_features': self.MARKET_FEATURES,
            'ohlc_features': self.OHLC_FEATURES,
            'symbol_features': self.SYMBOL_FEATURES,
            'engineered_features': [
                'rsi_oversold', 'rsi_overbought', 'macd_bullish',
                'bb_near_lower', 'bb_near_upper', 'strong_trend', 'volume_spike'
            ],
            'total_features': len(self.TECHNICAL_INDICATORS) + len(self.MARKET_FEATURES) + 
                            len(self.OHLC_FEATURES) + len(self.SYMBOL_FEATURES) + 7
        }
        
        with open('trained_model/feature_config_v2.1.json', 'w') as f:
            json.dump(feature_config, f, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            'model_version': '2.1',
            'type': 'enhanced_temporal_fusion_transformer',
            'task_type': self.task,
            'ensemble_size': self.ensemble_size,
            'total_features': len(self.TECHNICAL_INDICATORS) + len(self.MARKET_FEATURES) + len(self.OHLC_FEATURES) + len(self.SYMBOL_FEATURES) + 7,  # +7 for engineered
            'architecture': {
                'sequence_length': self.sequence_length,
                'd_model': self.d_model,
                'num_heads': self.num_heads,
                'num_transformer_blocks': self.num_transformer_blocks,
                'ff_dim': self.ff_dim,
                'dropout_rate': self.dropout_rate,
                'use_multi_scale': True,
                'use_attention_pooling': True
            },
            'training': {
                'batch_size': self.batch_size,
                'gradient_accumulation_steps': self.gradient_accumulation_steps,
                'epochs': self.epochs,
                'loss_type': 'focal_loss' if self.task == 'classification_binary' else 'huber_loss',
                'augmentation': True
            },
            'results': results,
            'timestamp': datetime.now().isoformat()
        }
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º numpy —Ç–∏–ø—ã –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
        metadata = self.convert_numpy_types(metadata)
        
        with open('trained_model/metadata_v2.1.json', 'w') as f:
            json.dump(metadata, f, indent=2)
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        with open(f'{self.log_dir}/final_report_v2.1.txt', 'w') as f:
            f.write("="*80 + "\n")
            f.write("ENHANCED TEMPORAL FUSION TRANSFORMER v2.1 - –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢\n")
            f.write("="*80 + "\n\n")
            
            f.write(f"–î–∞—Ç–∞ –æ–±—É—á–µ–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"–ó–∞–¥–∞—á–∞: {self.task}\n")
            f.write(f"–†–∞–∑–º–µ—Ä –∞–Ω—Å–∞–º–±–ª—è: {self.ensemble_size}\n\n")
            
            f.write("–ê–†–•–ò–¢–ï–ö–¢–£–†–ê:\n")
            f.write(f"- Transformer blocks: {self.num_transformer_blocks}\n")
            f.write(f"- Model dimension: {self.d_model}\n")
            f.write(f"- Attention heads: {self.num_heads}\n")
            f.write(f"- Feed-forward dim: {self.ff_dim}\n")
            f.write(f"- Dropout rate: {self.dropout_rate}\n")
            f.write(f"- Multi-scale conv: –î–∞\n")
            f.write(f"- Attention pooling: –î–∞\n\n")
            
            f.write("–£–õ–£–ß–®–ï–ù–ò–Ø v2.0:\n")
            f.write("- Market features (BTC correlation, time features)\n")
            f.write("- Focal Loss –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n")
            f.write("- Gradient accumulation\n")
            f.write("- Data augmentation (noise, mixup)\n")
            f.write("- Uncertainty estimation\n")
            f.write("- Enhanced architecture\n\n")
            
            f.write("–£–õ–£–ß–®–ï–ù–ò–Ø v2.1:\n")
            f.write("- OHLC features (13 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤): –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è, —Å–≤–µ—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã\n")
            f.write("- Symbol embeddings (14 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤): one-hot –¥–ª—è —Ç–æ–ø-10 –º–æ–Ω–µ—Ç, –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n")
            f.write("- –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫ EMA15/50 –∏ VWAP\n")
            f.write("- Layer Normalization –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ OHLC\n")
            f.write("- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n")
            f.write("- Post-processing —Ñ–∏–ª—å—Ç—Ä—ã –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤\n")
            f.write(f"- –í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(self.TECHNICAL_INDICATORS) + len(self.MARKET_FEATURES) + len(self.OHLC_FEATURES) + len(self.SYMBOL_FEATURES) + 7}\n\n")
            
            f.write("–†–ï–ó–£–õ–¨–¢–ê–¢–´:\n")
            for model_name, metrics in results.items():
                f.write(f"\n{model_name}:\n")
                for metric_name, value in metrics.items():
                    if isinstance(value, float):
                        f.write(f"  - {metric_name}: {value:.4f}\n")
                    else:
                        f.write(f"  - {metric_name}: {value}\n")
        
        logger.info("‚úÖ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ enhanced –æ–±—É—á–µ–Ω–∏—è"""
    parser = argparse.ArgumentParser(description='Enhanced Temporal Fusion Transformer v2.1')
    parser.add_argument('--task', type=str, default='classification_binary',
                       choices=['regression', 'classification_binary'],
                       help='–¢–∏–ø –∑–∞–¥–∞—á–∏: regression –∏–ª–∏ classification_binary')
    parser.add_argument('--ensemble_size', type=int, default=3,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –≤ –∞–Ω—Å–∞–º–±–ª–µ (default: 3)')
    parser.add_argument('--test_mode', action='store_true',
                       help='–¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º: 2 —Å–∏–º–≤–æ–ª–∞, 3 —ç–ø–æ—Ö–∏, –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 –¥–Ω–µ–π')
    args = parser.parse_args()
    
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ Enhanced Temporal Fusion Transformer v2.1")
    logger.info(f"üìä –†–µ–∂–∏–º: {args.task}")
    logger.info(f"üéØ –†–∞–∑–º–µ—Ä –∞–Ω—Å–∞–º–±–ª—è: {args.ensemble_size}")
    if args.test_mode:
        logger.info("‚ö° –¢–ï–°–¢–û–í–´–ô –†–ï–ñ–ò–ú: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —ç–ø–æ—Ö–∏")
    
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
    db_config = {
        'dbname': 'crypto_trading',
        'user': 'ruslan',
        'password': 'ruslan',
        'host': 'localhost',
        'port': 5555
    }
    
    db_manager = PostgreSQLManager(**db_config)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ trainer
    trainer = EnhancedTransformerTrainer(
        db_manager, 
        task=args.task,
        ensemble_size=args.ensemble_size,
        test_mode=args.test_mode
    )
    trainer.train_ensemble()


if __name__ == "__main__":
    main()