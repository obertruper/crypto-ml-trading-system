#!/usr/bin/env python3
"""
–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–∞–±–ª–∏—Ü–µ raw_market_data
"""

import psycopg2
import yaml
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
with open('config.yaml', 'r') as f:
    config = yaml.safe_load(f)

db_config = config['database'].copy()
if not db_config.get('password'):
    db_config.pop('password', None)

print("="*80)
print("üîç –ü–†–û–í–ï–†–ö–ê –í–ê–õ–ò–î–ù–û–°–¢–ò RAW_MARKET_DATA")
print("="*80)

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
conn = psycopg2.connect(**db_config)

# 1. –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
print("\nüìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
query = """
SELECT 
    COUNT(*) as total_records,
    COUNT(DISTINCT symbol) as unique_symbols,
    COUNT(DISTINCT market_type) as market_types,
    COUNT(DISTINCT interval_minutes) as intervals,
    MIN(datetime) as start_date,
    MAX(datetime) as end_date
FROM raw_market_data
"""
df_stats = pd.read_sql(query, conn)
print(f"   –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {df_stats['total_records'][0]:,}")
print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤: {df_stats['unique_symbols'][0]}")
print(f"   –¢–∏–ø—ã —Ä—ã–Ω–∫–∞: {df_stats['market_types'][0]}")
print(f"   –ò–Ω—Ç–µ—Ä–≤–∞–ª—ã: {df_stats['intervals'][0]}")
print(f"   –ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö: {df_stats['start_date'][0]} - {df_stats['end_date'][0]}")

# 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤ —Ä—ã–Ω–∫–∞
print("\nüìä –¢–ò–ü–´ –†–´–ù–ö–ê:")
query = """
SELECT market_type, COUNT(*) as count, COUNT(DISTINCT symbol) as symbols
FROM raw_market_data
GROUP BY market_type
ORDER BY count DESC
"""
df_markets = pd.read_sql(query, conn)
for _, row in df_markets.iterrows():
    print(f"   {row['market_type']}: {row['count']:,} –∑–∞–ø–∏—Å–µ–π, {row['symbols']} —Å–∏–º–≤–æ–ª–æ–≤")

# 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
print("\nüìä –ò–ù–¢–ï–†–í–ê–õ–´:")
query = """
SELECT interval_minutes, COUNT(*) as count, COUNT(DISTINCT symbol) as symbols
FROM raw_market_data
GROUP BY interval_minutes
ORDER BY interval_minutes
"""
df_intervals = pd.read_sql(query, conn)
for _, row in df_intervals.iterrows():
    print(f"   {row['interval_minutes']} –º–∏–Ω—É—Ç: {row['count']:,} –∑–∞–ø–∏—Å–µ–π, {row['symbols']} —Å–∏–º–≤–æ–ª–æ–≤")

# 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NULL –∑–Ω–∞—á–µ–Ω–∏—è
print("\n‚ùì –ü–†–û–í–ï–†–ö–ê NULL –ó–ù–ê–ß–ï–ù–ò–ô:")
query = """
SELECT 
    SUM(CASE WHEN open IS NULL THEN 1 ELSE 0 END) as null_open,
    SUM(CASE WHEN high IS NULL THEN 1 ELSE 0 END) as null_high,
    SUM(CASE WHEN low IS NULL THEN 1 ELSE 0 END) as null_low,
    SUM(CASE WHEN close IS NULL THEN 1 ELSE 0 END) as null_close,
    SUM(CASE WHEN volume IS NULL THEN 1 ELSE 0 END) as null_volume,
    SUM(CASE WHEN timestamp IS NULL THEN 1 ELSE 0 END) as null_timestamp,
    SUM(CASE WHEN datetime IS NULL THEN 1 ELSE 0 END) as null_datetime
FROM raw_market_data
"""
df_nulls = pd.read_sql(query, conn)
null_found = False
for col in df_nulls.columns:
    if df_nulls[col][0] > 0:
        print(f"   ‚ö†Ô∏è {col}: {df_nulls[col][0]:,} NULL –∑–Ω–∞—á–µ–Ω–∏–π")
        null_found = True
if not null_found:
    print("   ‚úÖ NULL –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

# 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
print("\nüìâ –ü–†–û–í–ï–†–ö–ê –û–¢–†–ò–¶–ê–¢–ï–õ–¨–ù–´–• –ó–ù–ê–ß–ï–ù–ò–ô:")
query = """
SELECT 
    SUM(CASE WHEN open < 0 THEN 1 ELSE 0 END) as negative_open,
    SUM(CASE WHEN high < 0 THEN 1 ELSE 0 END) as negative_high,
    SUM(CASE WHEN low < 0 THEN 1 ELSE 0 END) as negative_low,
    SUM(CASE WHEN close < 0 THEN 1 ELSE 0 END) as negative_close,
    SUM(CASE WHEN volume < 0 THEN 1 ELSE 0 END) as negative_volume
FROM raw_market_data
"""
df_negative = pd.read_sql(query, conn)
negative_found = False
for col in df_negative.columns:
    if df_negative[col][0] > 0:
        print(f"   ‚ö†Ô∏è {col}: {df_negative[col][0]:,} –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")
        negative_found = True
if not negative_found:
    print("   ‚úÖ –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

# 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ OHLC –ª–æ–≥–∏–∫–∏
print("\nüìä –ü–†–û–í–ï–†–ö–ê OHLC –õ–û–ì–ò–ö–ò:")
query = """
SELECT 
    COUNT(*) as total,
    SUM(CASE WHEN high < low THEN 1 ELSE 0 END) as high_less_than_low,
    SUM(CASE WHEN high < open THEN 1 ELSE 0 END) as high_less_than_open,
    SUM(CASE WHEN high < close THEN 1 ELSE 0 END) as high_less_than_close,
    SUM(CASE WHEN low > open THEN 1 ELSE 0 END) as low_greater_than_open,
    SUM(CASE WHEN low > close THEN 1 ELSE 0 END) as low_greater_than_close
FROM raw_market_data
WHERE market_type = 'futures' AND interval_minutes = 15
"""
df_ohlc = pd.read_sql(query, conn)
ohlc_errors = False
if df_ohlc['high_less_than_low'][0] > 0:
    print(f"   ‚ö†Ô∏è High < Low: {df_ohlc['high_less_than_low'][0]:,} –∑–∞–ø–∏—Å–µ–π")
    ohlc_errors = True
if df_ohlc['high_less_than_open'][0] > 0:
    print(f"   ‚ö†Ô∏è High < Open: {df_ohlc['high_less_than_open'][0]:,} –∑–∞–ø–∏—Å–µ–π")
    ohlc_errors = True
if df_ohlc['high_less_than_close'][0] > 0:
    print(f"   ‚ö†Ô∏è High < Close: {df_ohlc['high_less_than_close'][0]:,} –∑–∞–ø–∏—Å–µ–π")
    ohlc_errors = True
if df_ohlc['low_greater_than_open'][0] > 0:
    print(f"   ‚ö†Ô∏è Low > Open: {df_ohlc['low_greater_than_open'][0]:,} –∑–∞–ø–∏—Å–µ–π")
    ohlc_errors = True
if df_ohlc['low_greater_than_close'][0] > 0:
    print(f"   ‚ö†Ô∏è Low > Close: {df_ohlc['low_greater_than_close'][0]:,} –∑–∞–ø–∏—Å–µ–π")
    ohlc_errors = True
if not ohlc_errors:
    print("   ‚úÖ OHLC –ª–æ–≥–∏–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞")

# 7. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
print("\n‚è∞ –ü–†–û–í–ï–†–ö–ê –í–†–ï–ú–ï–ù–ù–´–• –ò–ù–¢–ï–†–í–ê–õ–û–í (15-–º–∏–Ω—É—Ç–Ω—ã–µ –±–∞—Ä—ã):")
query = """
WITH time_gaps AS (
    SELECT 
        symbol,
        timestamp,
        datetime,
        LAG(timestamp) OVER (PARTITION BY symbol ORDER BY timestamp) as prev_timestamp,
        timestamp - LAG(timestamp) OVER (PARTITION BY symbol ORDER BY timestamp) as gap_seconds
    FROM raw_market_data
    WHERE market_type = 'futures' AND interval_minutes = 15
)
SELECT 
    COUNT(*) as total_gaps,
    SUM(CASE WHEN gap_seconds = 900 THEN 1 ELSE 0 END) as correct_gaps,
    SUM(CASE WHEN gap_seconds > 900 AND gap_seconds <= 3600 THEN 1 ELSE 0 END) as small_gaps,
    SUM(CASE WHEN gap_seconds > 3600 AND gap_seconds <= 86400 THEN 1 ELSE 0 END) as medium_gaps,
    SUM(CASE WHEN gap_seconds > 86400 THEN 1 ELSE 0 END) as large_gaps
FROM time_gaps
WHERE gap_seconds IS NOT NULL
"""
df_gaps = pd.read_sql(query, conn)
if df_gaps['total_gaps'][0] > 0:
    correct_pct = df_gaps['correct_gaps'][0] / df_gaps['total_gaps'][0] * 100
    print(f"   –ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã (15 –º–∏–Ω): {df_gaps['correct_gaps'][0]:,} ({correct_pct:.1f}%)")
    if df_gaps['small_gaps'][0] > 0:
        print(f"   –ú–∞–ª—ã–µ –ø—Ä–æ–ø—É—Å–∫–∏ (15–º-1—á): {df_gaps['small_gaps'][0]:,}")
    if df_gaps['medium_gaps'][0] > 0:
        print(f"   –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–∏ (1—á-24—á): {df_gaps['medium_gaps'][0]:,}")
    if df_gaps['large_gaps'][0] > 0:
        print(f"   –ë–æ–ª—å—à–∏–µ –ø—Ä–æ–ø—É—Å–∫–∏ (>24—á): {df_gaps['large_gaps'][0]:,}")

# 8. –¢–æ–ø —Å–∏–º–≤–æ–ª–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –¥–∞–Ω–Ω—ã—Ö
print("\nüìà –¢–û–ü-10 –°–ò–ú–í–û–õ–û–í –ü–û –ö–û–õ–ò–ß–ï–°–¢–í–£ –î–ê–ù–ù–´–•:")
query = """
SELECT 
    symbol, 
    COUNT(*) as records,
    MIN(datetime) as start_date,
    MAX(datetime) as end_date,
    ROUND(AVG(volume)::numeric, 2) as avg_volume
FROM raw_market_data
WHERE market_type = 'futures' AND interval_minutes = 15
GROUP BY symbol
ORDER BY records DESC
LIMIT 10
"""
df_symbols = pd.read_sql(query, conn)
for _, row in df_symbols.iterrows():
    days = (row['end_date'] - row['start_date']).days
    print(f"   {row['symbol']}: {row['records']:,} –∑–∞–ø–∏—Å–µ–π, {days} –¥–Ω–µ–π, avg vol: {row['avg_volume']:,.2f}")

# 9. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
print("\nüîç –ü–†–û–í–ï–†–ö–ê –î–£–ë–õ–ò–ö–ê–¢–û–í:")
query = """
SELECT symbol, timestamp, COUNT(*) as count
FROM raw_market_data
WHERE market_type = 'futures' AND interval_minutes = 15
GROUP BY symbol, timestamp
HAVING COUNT(*) > 1
LIMIT 10
"""
df_duplicates = pd.read_sql(query, conn)
if len(df_duplicates) > 0:
    print(f"   ‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ {len(df_duplicates)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤!")
    for _, row in df_duplicates.iterrows():
        print(f"      {row['symbol']} at {pd.to_datetime(row['timestamp'], unit='s')}: {row['count']} –∑–∞–ø–∏—Å–µ–π")
else:
    print("   ‚úÖ –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

# 10. –ü—Ä–æ–≤–µ—Ä–∫–∞ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
print("\nüìä –ü–†–û–í–ï–†–ö–ê –≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–´–• –ó–ù–ê–ß–ï–ù–ò–ô:")
query = """
WITH price_changes AS (
    SELECT 
        symbol,
        datetime,
        close,
        LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp) as prev_close,
        CASE 
            WHEN LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp) > 0 
            THEN ABS((close - LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp)) / 
                 LAG(close) OVER (PARTITION BY symbol ORDER BY timestamp) * 100)
            ELSE 0 
        END as change_pct
    FROM raw_market_data
    WHERE market_type = 'futures' AND interval_minutes = 15
)
SELECT 
    COUNT(*) as total,
    SUM(CASE WHEN change_pct > 10 THEN 1 ELSE 0 END) as extreme_changes,
    MAX(change_pct) as max_change
FROM price_changes
WHERE change_pct IS NOT NULL
"""
df_extreme = pd.read_sql(query, conn)
if df_extreme['extreme_changes'][0] > 0:
    print(f"   ‚ö†Ô∏è –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è (>10% –∑–∞ 15 –º–∏–Ω): {df_extreme['extreme_changes'][0]:,}")
    print(f"   ‚ö†Ô∏è –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ: {df_extreme['max_change'][0]:.2f}%")
else:
    print("   ‚úÖ –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

# 11. –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
print("\n" + "="*80)
print("üìã –ò–¢–û–ì–û–í–ê–Ø –û–¶–ï–ù–ö–ê:")
print("="*80)

issues = []
if null_found:
    issues.append("NULL –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∞–Ω–Ω—ã—Ö")
if negative_found:
    issues.append("–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ü–µ–Ω/–æ–±—ä–µ–º–æ–≤")
if ohlc_errors:
    issues.append("–ù–∞—Ä—É—à–µ–Ω–∏–µ OHLC –ª–æ–≥–∏–∫–∏")
if df_extreme['extreme_changes'][0] > 100:
    issues.append("–ú–Ω–æ–≥–æ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π —Ü–µ–Ω")
if len(df_duplicates) > 0:
    issues.append("–î—É–±–ª–∏–∫–∞—Ç—ã –≤ –¥–∞–Ω–Ω—ã—Ö")

if len(issues) == 0:
    print("‚úÖ –î–∞–Ω–Ω—ã–µ –≤–∞–ª–∏–¥–Ω—ã –∏ –≥–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
    print("   - –ù–µ—Ç NULL –∑–Ω–∞—á–µ–Ω–∏–π")
    print("   - –ù–µ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Ü–µ–Ω")
    print("   - OHLC –ª–æ–≥–∏–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞")
    print("   - –ù–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
else:
    print("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã:")
    for issue in issues:
        print(f"   - {issue}")
    print("\nüí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ—á–∏—Å—Ç–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è —Ñ—å—é—á–µ—Ä—Å–æ–≤
print("\nüîç –ü–†–û–í–ï–†–ö–ê –§–¨–Æ–ß–ï–†–°–ù–´–• –î–ê–ù–ù–´–•:")
query = """
SELECT COUNT(*) as futures_count
FROM raw_market_data
WHERE market_type = 'futures' AND interval_minutes = 15
"""
futures_count = pd.read_sql(query, conn)['futures_count'][0]
print(f"   –§—å—é—á–µ—Ä—Å–Ω—ã—Ö 15-–º–∏–Ω—É—Ç–Ω—ã—Ö –±–∞—Ä–æ–≤: {futures_count:,}")

if futures_count > 1000000:
    print("   ‚úÖ –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")
else:
    print("   ‚ö†Ô∏è –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–æ–ª—å—à–µ")

conn.close()
print("\n" + "="*80)