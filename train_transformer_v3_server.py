#!/usr/bin/env python3
"""
–°–µ—Ä–≤–µ—Ä–Ω–∞—è –≤–µ—Ä—Å–∏—è Temporal Fusion Transformer v3.0
–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã –Ω–∞ GPU —Å–µ—Ä–≤–µ—Ä–µ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import logging
from datetime import datetime
import json
import matplotlib
matplotlib.use('Agg')  # –î–ª—è —Ä–∞–±–æ—Ç—ã –±–µ–∑ GUI –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
import psycopg2
import gc
import time
warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è —Å–µ—Ä–≤–µ—Ä–∞
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # –î–ª—è –ª—É—á—à–µ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ GPU –æ—à–∏–±–æ–∫

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ XGBoost –º–æ–¥—É–ª—è–º
sys.path.append('xgboost_v3')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('transformer_v3_server.log')
    ]
)
logger = logging.getLogger(__name__)


def check_gpu():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU"""
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        logger.info(f"üéÆ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ GPU: {gpu_count}")
        for i in range(gpu_count):
            logger.info(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
            logger.info(f"   –ü–∞–º—è—Ç—å: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB")
        return True
    else:
        logger.warning("‚ö†Ô∏è GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
        return False


class TargetCalculator:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è TargetCalculator –¥–ª—è —Å–µ—Ä–≤–µ—Ä–∞"""
    def __init__(self, lookahead_bars=4, price_threshold=0.5):
        self.lookahead_bars = lookahead_bars
        self.price_threshold = price_threshold
        
    def calculate_threshold_binary(self, df):
        """–ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –ø–æ—Ä–æ–≥–æ–º"""
        df = df.copy()
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –±—É–¥—É—â–µ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã
        future_return = (df['close'].shift(-self.lookahead_bars) / df['close'] - 1) * 100
        
        # –ë–∏–Ω–∞—Ä–Ω—ã–π —Ç–∞—Ä–≥–µ—Ç
        df['target_threshold_binary'] = (future_return > self.price_threshold).astype(int)
        
        return df
        
    def calculate_simple_regression(self, df):
        """–ü—Ä–æ—Å—Ç–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –Ω–∞ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ"""
        df = df.copy()
        
        # –ë—É–¥—É—â–µ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
        df['target_simple_regression'] = (df['close'].shift(-self.lookahead_bars) / df['close'] - 1) * 100
        
        return df


class HierarchicalFeatureSelector:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è —Å–µ—Ä–≤–µ—Ä–∞"""
    def __init__(self, top_k=100):
        self.top_k = top_k
        
    def select_features(self, X, y, feature_names):
        """–ü—Ä–æ—Å—Ç–æ–π –æ—Ç–±–æ—Ä —Ç–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏"""
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
        correlations = []
        for col in X.columns:
            if X[col].std() > 0:
                corr = abs(X[col].corr(y))
                correlations.append((col, corr))
                
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
        correlations.sort(key=lambda x: x[1], reverse=True)
        
        # –û—Ç–±–∏—Ä–∞–µ–º —Ç–æ–ø-K
        selected = [col for col, _ in correlations[:self.top_k]]
        
        logger.info(f"‚úÖ –û—Ç–æ–±—Ä–∞–Ω–æ {len(selected)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        return X[selected], selected


class ImprovedGatedLinearUnit(nn.Module):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π GLU —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π –∏ dropout"""
    def __init__(self, input_dim, output_dim, dropout_rate=0.1):
        super().__init__()
        self.fc = nn.Linear(input_dim, output_dim * 2)
        self.layer_norm = nn.LayerNorm(output_dim)
        self.dropout = nn.Dropout(dropout_rate)
        
    def forward(self, x):
        import torch.nn.functional as F
        x = self.fc(x)
        x = F.glu(x, dim=-1)
        x = self.layer_norm(x)
        x = self.dropout(x)
        return x


class ImprovedGatedResidualNetwork(nn.Module):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π GRN —Å skip connections"""
    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.1):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.elu = nn.ELU()
        self.glu = ImprovedGatedLinearUnit(hidden_dim, output_dim, dropout_rate)
        self.skip_connection = nn.Linear(input_dim, output_dim) if input_dim != output_dim else None
        self.layer_norm = nn.LayerNorm(output_dim)
        
    def forward(self, x):
        residual = x
        
        x = self.fc1(x)
        x = self.elu(x)
        x = self.glu(x)
        
        if self.skip_connection is not None:
            residual = self.skip_connection(residual)
            
        x = x + residual
        x = self.layer_norm(x)
        
        return x


class ImprovedVariableSelectionNetwork(nn.Module):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–µ—Ç—å –≤—ã–±–æ—Ä–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
    def __init__(self, input_dim, num_inputs, hidden_dim, dropout_rate=0.1):
        super().__init__()
        self.num_inputs = num_inputs
        self.hidden_dim = hidden_dim
        
        self.flattened_grn = ImprovedGatedResidualNetwork(
            input_dim * num_inputs,
            hidden_dim,
            num_inputs,
            dropout_rate
        )
        
        self.variable_grns = nn.ModuleList([
            ImprovedGatedResidualNetwork(input_dim, hidden_dim, hidden_dim, dropout_rate)
            for _ in range(num_inputs)
        ])
        
        self.softmax = nn.Softmax(dim=-1)
        
    def forward(self, inputs):
        batch_size = inputs.size(0)
        
        flattened = inputs.view(batch_size, -1)
        variable_weights = self.softmax(self.flattened_grn(flattened))
        
        processed_inputs = []
        for i in range(self.num_inputs):
            processed = self.variable_grns[i](inputs[:, i, :])
            processed_inputs.append(processed)
            
        processed_inputs = torch.stack(processed_inputs, dim=1)
        weighted_inputs = processed_inputs * variable_weights.unsqueeze(-1)
        combined = weighted_inputs.sum(dim=1)
        
        return combined, variable_weights


class TemporalFusionTransformerV3(nn.Module):
    """TFT –º–æ–¥–µ–ª—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è —Å–µ—Ä–≤–µ—Ä–∞"""
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        self.input_dim = config['input_dim']
        self.hidden_dim = config['hidden_dim']
        self.num_heads = config['num_heads']
        self.dropout_rate = config['dropout_rate']
        self.output_dim = config['output_dim']
        
        # Variable Selection
        self.static_vsn = ImprovedVariableSelectionNetwork(
            self.input_dim,
            1,
            self.hidden_dim,
            self.dropout_rate
        )
        
        # LSTM Encoder (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è GPU)
        self.lstm_encoder = nn.LSTM(
            self.hidden_dim,
            self.hidden_dim,
            num_layers=2,
            dropout=self.dropout_rate,
            batch_first=True,
            bidirectional=True
        )
        
        # Multi-Head Attention
        self.self_attention = nn.MultiheadAttention(
            self.hidden_dim * 2,
            self.num_heads,
            dropout=self.dropout_rate,
            batch_first=True
        )
        
        # Decoder GRN
        self.decoder_grn = ImprovedGatedResidualNetwork(
            self.hidden_dim * 2,
            self.hidden_dim,
            self.hidden_dim,
            self.dropout_rate
        )
        
        # Output layers
        if config['task'] == 'regression':
            self.output_layer = nn.Sequential(
                nn.Linear(self.hidden_dim, self.hidden_dim // 2),
                nn.ReLU(),
                nn.Dropout(self.dropout_rate),
                nn.Linear(self.hidden_dim // 2, self.output_dim)
            )
        else:
            self.output_layer = nn.Sequential(
                nn.Linear(self.hidden_dim, self.hidden_dim // 2),
                nn.ReLU(),
                nn.Dropout(self.dropout_rate),
                nn.Linear(self.hidden_dim // 2, self.output_dim),
                nn.Sigmoid() if self.output_dim == 1 else nn.Softmax(dim=-1)
            )
            
    def forward(self, x_static):
        batch_size = x_static.size(0)
        
        # Variable Selection
        selected_features, feature_weights = self.static_vsn(x_static.unsqueeze(1))
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        sequence_length = 10
        lstm_input = selected_features.unsqueeze(1).repeat(1, sequence_length, 1)
        
        # LSTM Encoder
        lstm_output, (hidden, cell) = self.lstm_encoder(lstm_input)
        
        # Self-Attention
        attn_output, attn_weights = self.self_attention(
            lstm_output, lstm_output, lstm_output
        )
        
        # Decoder GRN
        decoded = self.decoder_grn(attn_output)
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è
        aggregated = decoded.mean(dim=1)
        
        # Output
        output = self.output_layer(aggregated)
        
        return output, {
            'feature_weights': feature_weights,
            'attention_weights': attn_weights
        }


class TransformerTrainer:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è —Å–µ—Ä–≤–µ—Ä–∞"""
    def __init__(self, model, config, log_dir):
        self.model = model
        self.config = config
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å gradient accumulation –¥–ª—è –±–æ–ª—å—à–∏—Ö –±–∞—Ç—á–µ–π
        self.optimizer = optim.AdamW(
            model.parameters(),
            lr=config['learning_rate'],
            weight_decay=config['weight_decay']
        )
        
        # Scheduler
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=5,
            verbose=True
        )
        
        # Loss functions
        if config['task'] == 'regression':
            self.criterion = nn.MSELoss()
        elif config['output_dim'] == 1:
            self.criterion = nn.BCELoss()
        else:
            self.criterion = nn.CrossEntropyLoss()
            
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
        # –í–∫–ª—é—á–∞–µ–º DataParallel –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö GPU
        if torch.cuda.device_count() > 1:
            logger.info(f"üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {torch.cuda.device_count()} GPU")
            self.model = nn.DataParallel(self.model)
            
        # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'train_metrics': [],
            'val_metrics': [],
            'learning_rates': [],
            'gpu_memory': []
        }
        
    def train_epoch(self, train_loader):
        """–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏ —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º GPU"""
        self.model.train()
        total_loss = 0
        predictions = []
        targets = []
        
        # Gradient accumulation steps
        accumulation_steps = 4
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            # Forward pass
            output, _ = self.model(data)
            
            # Loss
            if self.config['task'] == 'classification' and self.config['output_dim'] > 1:
                loss = self.criterion(output, target.long())
            else:
                loss = self.criterion(output.squeeze(), target)
                
            # Normalize loss for gradient accumulation
            loss = loss / accumulation_steps
            loss.backward()
            
            # Gradient accumulation
            if (batch_idx + 1) % accumulation_steps == 0:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                self.optimizer.step()
                self.optimizer.zero_grad()
                
            total_loss += loss.item() * accumulation_steps
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            predictions.extend(output.detach().cpu().numpy())
            targets.extend(target.detach().cpu().numpy())
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ GPU
            if batch_idx % 50 == 0 and torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1024**3
                memory_cached = torch.cuda.memory_reserved() / 1024**3
                logger.info(f"   Batch {batch_idx}/{len(train_loader)} | "
                          f"GPU Memory: {memory_used:.1f}GB / {memory_cached:.1f}GB")
                
        avg_loss = total_loss / len(train_loader)
        metrics = self._calculate_metrics(predictions, targets)
        
        return avg_loss, metrics
        
    def validate(self, val_loader):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
        self.model.eval()
        total_loss = 0
        predictions = []
        targets = []
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device), target.to(self.device)
                
                output, _ = self.model(data)
                
                if self.config['task'] == 'classification' and self.config['output_dim'] > 1:
                    loss = self.criterion(output, target.long())
                else:
                    loss = self.criterion(output.squeeze(), target)
                    
                total_loss += loss.item()
                
                predictions.extend(output.detach().cpu().numpy())
                targets.extend(target.detach().cpu().numpy())
                
        avg_loss = total_loss / len(val_loader)
        metrics = self._calculate_metrics(predictions, targets)
        
        return avg_loss, metrics
        
    def _calculate_metrics(self, predictions, targets):
        """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫"""
        predictions = np.array(predictions)
        targets = np.array(targets)
        
        if self.config['task'] == 'regression':
            return {
                'mae': mean_absolute_error(targets, predictions),
                'rmse': np.sqrt(mean_squared_error(targets, predictions)),
                'r2': r2_score(targets, predictions)
            }
        else:
            if self.config['output_dim'] == 1:
                preds_binary = (predictions > 0.5).astype(int).flatten()
                targets_flat = targets.flatten().astype(int)
                return {
                    'accuracy': accuracy_score(targets_flat, preds_binary),
                    'precision': precision_score(targets_flat, preds_binary, zero_division=0),
                    'recall': recall_score(targets_flat, preds_binary, zero_division=0),
                    'f1': f1_score(targets_flat, preds_binary, zero_division=0),
                    'roc_auc': roc_auc_score(targets_flat, predictions.flatten()) if len(np.unique(targets_flat)) > 1 else 0
                }
            else:
                preds_class = np.argmax(predictions, axis=1)
                return {
                    'accuracy': accuracy_score(targets, preds_class),
                    'macro_f1': f1_score(targets, preds_class, average='macro', zero_division=0)
                }
                
    def train(self, train_loader, val_loader, epochs):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è"""
        logger.info(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ {epochs} —ç–ø–æ—Ö")
        logger.info(f"üìä –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        best_val_loss = float('inf')
        patience_counter = 0
        
        start_time = time.time()
        
        for epoch in range(epochs):
            epoch_start = time.time()
            
            # –û–±—É—á–µ–Ω–∏–µ
            train_loss, train_metrics = self.train_epoch(train_loader)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            val_loss, val_metrics = self.validate(val_loader)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ scheduler
            self.scheduler.step(val_loss)
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # GPU memory stats
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1024**3
                self.history['gpu_memory'].append(memory_used)
                
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
            self.history['train_loss'].append(train_loss)
            self.history['val_loss'].append(val_loss)
            self.history['train_metrics'].append(train_metrics)
            self.history['val_metrics'].append(val_metrics)
            self.history['learning_rates'].append(current_lr)
            
            # –í—Ä–µ–º—è —ç–ø–æ—Ö–∏
            epoch_time = time.time() - epoch_start
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            logger.info(f"\n{'='*60}")
            logger.info(f"–≠–ø–æ—Ö–∞ {epoch+1}/{epochs} | –í—Ä–µ–º—è: {epoch_time:.1f}—Å")
            logger.info(f"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")
            logger.info(f"Learning Rate: {current_lr:.6f}")
            
            if self.config['task'] == 'regression':
                logger.info(f"Val MAE: {val_metrics['mae']:.4f} | "
                          f"Val RMSE: {val_metrics['rmse']:.4f} | "
                          f"Val R¬≤: {val_metrics['r2']:.4f}")
            else:
                logger.info(f"Val Accuracy: {val_metrics.get('accuracy', 0):.4f} | "
                          f"Val F1: {val_metrics.get('f1', val_metrics.get('macro_f1', 0)):.4f}")
                
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                self._save_checkpoint(epoch, val_loss, val_metrics)
                logger.info("‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å!")
            else:
                patience_counter += 1
                
            # Early stopping
            if patience_counter >= 10:
                logger.info("‚èπÔ∏è Early stopping triggered")
                break
                
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞–∂–¥—ã–µ 5 —ç–ø–æ—Ö
            if (epoch + 1) % 5 == 0:
                self._plot_training_progress(epoch + 1)
                
            # –û—á–∏—Å—Ç–∫–∞ GPU –∫—ç—à–∞
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
        # –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
        total_time = time.time() - start_time
        logger.info(f"\n‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {total_time/60:.1f} –º–∏–Ω—É—Ç")
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        self._plot_final_results()
        self._save_training_summary()
        
        logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        
    def _save_checkpoint(self, epoch, val_loss, val_metrics):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –º–æ–¥–µ–ª–∏"""
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –æ–±–µ—Ä–Ω—É—Ç–∞ –≤ DataParallel, –∏–∑–≤–ª–µ–∫–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model_to_save.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'val_loss': val_loss,
            'val_metrics': val_metrics,
            'config': self.config
        }
        
        checkpoint_path = self.log_dir / 'best_model.pth'
        torch.save(checkpoint, checkpoint_path)
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç: {checkpoint_path}")
        
    def _plot_training_progress(self, epoch):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Loss
        axes[0, 0].plot(self.history['train_loss'], label='Train Loss')
        axes[0, 0].plot(self.history['val_loss'], label='Val Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].set_title('Training and Validation Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # Learning Rate
        axes[0, 1].plot(self.history['learning_rates'])
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Learning Rate')
        axes[0, 1].set_title('Learning Rate Schedule')
        axes[0, 1].grid(True)
        
        # Primary Metric
        if self.config['task'] == 'regression':
            train_metric = [m['mae'] for m in self.history['train_metrics']]
            val_metric = [m['mae'] for m in self.history['val_metrics']]
            metric_name = 'MAE'
        else:
            train_metric = [m.get('accuracy', 0) for m in self.history['train_metrics']]
            val_metric = [m.get('accuracy', 0) for m in self.history['val_metrics']]
            metric_name = 'Accuracy'
            
        axes[1, 0].plot(train_metric, label=f'Train {metric_name}')
        axes[1, 0].plot(val_metric, label=f'Val {metric_name}')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel(metric_name)
        axes[1, 0].set_title(f'{metric_name} Progress')
        axes[1, 0].legend()
        axes[1, 0].grid(True)
        
        # GPU Memory Usage
        if self.history['gpu_memory']:
            axes[1, 1].plot(self.history['gpu_memory'])
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('GPU Memory (GB)')
            axes[1, 1].set_title('GPU Memory Usage')
            axes[1, 1].grid(True)
            
        plt.tight_layout()
        plt.savefig(self.log_dir / f'training_progress_epoch_{epoch}.png', dpi=150)
        plt.close()
        
        # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        del fig
        gc.collect()
        
    def _plot_final_results(self):
        """–§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        logger.info("üìä –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")
        
        # –°–≤–æ–¥–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫
        fig, ax = plt.subplots(1, 1, figsize=(10, 6))
        
        epochs = range(1, len(self.history['train_loss']) + 1)
        ax.plot(epochs, self.history['train_loss'], 'b-', label='Train Loss')
        ax.plot(epochs, self.history['val_loss'], 'r-', label='Val Loss')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Loss')
        ax.set_title('Training Summary')
        ax.legend()
        ax.grid(True)
        
        plt.tight_layout()
        plt.savefig(self.log_dir / 'training_summary.png', dpi=150)
        plt.close()
        
    def _save_training_summary(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        summary = {
            'config': self.config,
            'final_train_loss': float(self.history['train_loss'][-1]),
            'final_val_loss': float(self.history['val_loss'][-1]),
            'final_train_metrics': {k: float(v) for k, v in self.history['train_metrics'][-1].items()},
            'final_val_metrics': {k: float(v) for k, v in self.history['val_metrics'][-1].items()},
            'total_epochs': len(self.history['train_loss']),
            'best_val_loss': float(min(self.history['val_loss'])),
            'best_epoch': self.history['val_loss'].index(min(self.history['val_loss'])) + 1,
            'device': str(self.device),
            'gpu_count': torch.cuda.device_count() if torch.cuda.is_available() else 0
        }
        
        with open(self.log_dir / 'training_summary.json', 'w') as f:
            json.dump(summary, f, indent=2)
            
        # –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç
        with open(self.log_dir / 'final_report.txt', 'w') as f:
            f.write("=" * 60 + "\n")
            f.write("TEMPORAL FUSION TRANSFORMER V3 - –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢\n")
            f.write("=" * 60 + "\n\n")
            
            f.write(f"–î–∞—Ç–∞ –æ–±—É—á–µ–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {summary['device']}\n")
            f.write(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU: {summary['gpu_count']}\n")
            f.write(f"–ó–∞–¥–∞—á–∞: {self.config['task']}\n")
            f.write(f"–í—Å–µ–≥–æ —ç–ø–æ—Ö: {summary['total_epochs']}\n")
            f.write(f"–õ—É—á—à–∞—è —ç–ø–æ—Ö–∞: {summary['best_epoch']}\n")
            f.write(f"–õ—É—á—à–∏–π Val Loss: {summary['best_val_loss']:.4f}\n\n")
            
            f.write("–§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:\n")
            for metric, value in summary['final_val_metrics'].items():
                f.write(f"  {metric}: {value:.4f}\n")


def prepare_data_for_transformer(df, target_type='threshold_binary', test_size=0.2):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞"""
    logger.info("üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞")
    
    # –†–∞—Å—á–µ—Ç —Ç–∞—Ä–≥–µ—Ç–æ–≤
    target_calculator = TargetCalculator(lookahead_bars=4, price_threshold=0.5)
    
    if target_type == 'threshold_binary':
        df = target_calculator.calculate_threshold_binary(df)
    elif target_type == 'simple_regression':
        df = target_calculator.calculate_simple_regression(df)
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —Ç–∞—Ä–≥–µ—Ç–∞: {target_type}")
        
    target_column = f"target_{target_type}"
    
    # –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_columns = [col for col in df.columns if col not in ['symbol', 'timestamp', 'close'] 
                      and not col.startswith('target_')]
    
    # –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_selector = HierarchicalFeatureSelector(top_k=100)
    X = df[feature_columns]
    y = df[target_column]
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN
    mask = ~(X.isna().any(axis=1) | y.isna())
    X = X[mask]
    y = y[mask]
    
    X_selected, selected_features = feature_selector.select_features(X, y, feature_columns)
    
    # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
    n_samples = len(X_selected)
    train_size = int(n_samples * (1 - test_size))
    
    X_train = X_selected.iloc[:train_size]
    X_test = X_selected.iloc[train_size:]
    y_train = y.iloc[:train_size]
    y_test = y.iloc[train_size:]
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    logger.info(f"üìà Train: {len(X_train)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    logger.info(f"üìà Test: {len(X_test)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    if target_type != 'simple_regression':
        unique, counts = np.unique(y_train, return_counts=True)
        logger.info("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ train:")
        for val, count in zip(unique, counts):
            logger.info(f"   –ö–ª–∞—Å—Å {val}: {count} ({count/len(y_train)*100:.1f}%)")
            
    return (X_train_scaled, X_test_scaled, y_train.values, y_test.values, 
            selected_features, scaler)


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–µ—Ä–≤–µ—Ä–∞"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Temporal Fusion Transformer v3 Server')
    parser.add_argument('--task', type=str, default='classification',
                       choices=['classification', 'regression'])
    parser.add_argument('--target-type', type=str, default='threshold_binary',
                       choices=['threshold_binary', 'simple_regression'])
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--batch-size', type=int, default=512)  # –£–≤–µ–ª–∏—á–µ–Ω –¥–ª—è GPU
    parser.add_argument('--learning-rate', type=float, default=0.001)
    parser.add_argument('--hidden-dim', type=int, default=256)  # –£–≤–µ–ª–∏—á–µ–Ω –¥–ª—è GPU
    parser.add_argument('--num-workers', type=int, default=4)
    parser.add_argument('--limit', type=int, default=500000,
                       help='–õ–∏–º–∏—Ç –∑–∞–ø–∏—Å–µ–π –∏–∑ –ë–î')
    
    args = parser.parse_args()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
    check_gpu()
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    config = {
        'task': args.task,
        'input_dim': 100,  # –±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–æ
        'hidden_dim': args.hidden_dim,
        'num_heads': 8,
        'dropout_rate': 0.1,
        'output_dim': 1,
        'learning_rate': args.learning_rate,
        'weight_decay': 0.0001
    }
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –ª–æ–≥–æ–≤
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_dir = f"logs/transformer_v3_{timestamp}"
    
    logger.info("=" * 60)
    logger.info("üöÄ TEMPORAL FUSION TRANSFORMER V3 - SERVER VERSION")
    logger.info(f"üìä –ó–∞–¥–∞—á–∞: {config['task']}")
    logger.info(f"üéØ –¢–∏–ø —Ç–∞—Ä–≥–µ—Ç–∞: {args.target_type}")
    logger.info("=" * 60)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    try:
        conn = psycopg2.connect(
            host="localhost",
            port=5555,
            database="crypto_trading",
            user="ruslan"
        )
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Å–∏–º–≤–æ–ª—ã –¥–ª—è production mode
        query = f"""
        SELECT * FROM processed_market_data 
        WHERE symbol NOT LIKE '%TEST%'
        ORDER BY timestamp
        LIMIT {args.limit}
        """
        
        logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–ª–∏–º–∏—Ç: {args.limit})...")
        df = pd.read_sql(query, conn)
        conn.close()
        
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
        logger.info(f"üìä –°–∏–º–≤–æ–ª—ã: {df['symbol'].nunique()}")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return
        
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    (X_train, X_test, y_train, y_test, 
     selected_features, scaler) = prepare_data_for_transformer(df, args.target_type)
    
    # –û–±–Ω–æ–≤–ª—è–µ–º input_dim
    config['input_dim'] = X_train.shape[1]
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataLoaders —Å pin_memory –¥–ª—è GPU
    train_dataset = TensorDataset(
        torch.FloatTensor(X_train),
        torch.FloatTensor(y_train)
    )
    val_dataset = TensorDataset(
        torch.FloatTensor(X_test),
        torch.FloatTensor(y_test)
    )
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=True,
        num_workers=args.num_workers,
        pin_memory=True
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=args.batch_size, 
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=True
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = TemporalFusionTransformerV3(config)
    logger.info(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏: {sum(p.numel() for p in model.parameters()):,}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
    trainer = TransformerTrainer(model, config, log_dir)
    
    # –û–±—É—á–µ–Ω–∏–µ
    trainer.train(train_loader, val_loader, args.epochs)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    with open(f"{log_dir}/config.json", 'w') as f:
        json.dump(config, f, indent=2)
        
    with open(f"{log_dir}/selected_features.json", 'w') as f:
        json.dump(selected_features, f, indent=2)
        
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ scaler
    import joblib
    joblib.dump(scaler, f"{log_dir}/scaler.pkl")
    
    logger.info(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {log_dir}")
    

if __name__ == "__main__":
    main()