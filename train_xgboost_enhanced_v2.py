#!/usr/bin/env python3
"""
Enhanced XGBoost –¥–ª—è –∫—Ä–∏–ø—Ç–æ—Ç—Ä–µ–π–¥–∏–Ω–≥–∞ v2.1
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ —Å "–º–æ–Ω–µ—Ç–∫–æ–π" –∏ —É–ª—É—á—à–µ–Ω–∞ —Ä–∞–±–æ—Ç–∞ —Å –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- –í—Å–µ 92 –ø—Ä–∏–∑–Ω–∞–∫–∞ –∏–∑ TFT v2.1 (technical + market + OHLC + symbol)
- –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –±–∞—Ç—á–∞–º–∏ –∏–∑ PostgreSQL
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ regression –∏ binary classification
- Ensemble –º–æ–¥–µ–ª–µ–π —Å –≤–∑–≤–µ—à–µ–Ω–Ω—ã–º –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ–º
- Feature importance visualization
- GPU –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —á–µ—Ä–µ–∑ tree_method='gpu_hist'
- –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ (G-mean, profit-based)
- –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
- SMOTE –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤
- Focal Loss –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º
"""

import os
import sys
import time
import json
import pickle
import optuna
from optuna.pruners import MedianPruner
from optuna.samplers import TPESampler
import yaml
import warnings
import argparse
import gc
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Union

import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import (
    mean_absolute_error, mean_squared_error, r2_score,
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report, roc_curve,
    average_precision_score, matthews_corrcoef
)
from sklearn.model_selection import train_test_split, StratifiedKFold
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import psycopg2
from psycopg2.extras import RealDictCursor
import logging
import joblib
import hashlib
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à–∏ —É—Ç–∏–ª–∏—Ç—ã
    calculate_scale_pos_weight,
    find_optimal_threshold_gmean,
    find_optimal_threshold_profit,
    calibrate_probabilities,
    apply_smote,
    apply_random_oversampler,
    apply_smote_tomek,
    create_focal_loss_objective,
    ensemble_predictions_weighted,
    validate_binary_features
)
from fix_binary_features import recreate_binary_features, separate_features_for_smote, clip_technical_indicators

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
log_dir = f"logs/xgboost_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
os.makedirs(log_dir, exist_ok=True)
os.makedirs(f"{log_dir}/plots", exist_ok=True)
os.makedirs("trained_model", exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'{log_dir}/training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
warnings.filterwarnings('ignore')

# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
try:
    import GPUtil
    gpus = GPUtil.getGPUs()
    if gpus:
        logger.info(f"üñ•Ô∏è GPU –¥–æ—Å—Ç—É–ø–µ–Ω: {gpus[0].name}")
        USE_GPU = True
    else:
        logger.info("‚ö†Ô∏è GPU –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
        USE_GPU = False
except:
    logger.info("‚ö†Ô∏è GPUtil –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–≤–µ—Ä–∫–∞ GPU –ø—Ä–æ–ø—É—â–µ–Ω–∞")
    USE_GPU = False


class CacheManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—É—Å–∫–æ–≤"""
    
    def __init__(self, cache_dir='cache'):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
    def get_cache_key(self, params: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∫–ª—é—á–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        key_data = {
            'symbols_count': params.get('symbols_count'),
            'date_range': params.get('date_range'),
            'features_version': 'v2.1',  # –≤–µ—Ä—Å–∏—è feature engineering
            'task': params.get('task')
        }
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_str.encode()).hexdigest()[:16]
    
    def save(self, data, key: str, description: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        cache_file = self.cache_dir / f'{key}_{description}.pkl'
        metadata = {
            'created_at': datetime.now(),
            'data_shape': data.shape if hasattr(data, 'shape') else len(data),
            'description': description
        }
        
        with open(cache_file, 'wb') as f:
            pickle.dump({'data': data, 'metadata': metadata}, f)
        
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –∫—ç—à: {cache_file.name}")
        
    def load(self, key: str, description: str, max_age_days: int = 7):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏"""
        cache_file = self.cache_dir / f'{key}_{description}.pkl'
        
        if not cache_file.exists():
            return None
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–æ–∑—Ä–∞—Å—Ç–∞ —Ñ–∞–π–ª–∞
        age_days = (datetime.now() - datetime.fromtimestamp(
            cache_file.stat().st_mtime)).days
        
        if age_days > max_age_days:
            logger.warning(f"‚ö†Ô∏è –ö—ç—à —É—Å—Ç–∞—Ä–µ–ª ({age_days} –¥–Ω–µ–π): {cache_file.name}")
            return None
            
        with open(cache_file, 'rb') as f:
            cached = pickle.load(f)
            
        logger.info(f"üì¶ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ –∫—ç—à–∞: {cache_file.name}")
        logger.info(f"   –°–æ–∑–¥–∞–Ω: {cached['metadata']['created_at']}")
        logger.info(f"   –†–∞–∑–º–µ—Ä: {cached['metadata']['data_shape']}")
        
        return cached['data']


class AdvancedVisualizer:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, log_dir: str):
        self.log_dir = Path(log_dir)
        self.plots_dir = self.log_dir / 'plots'
        self.plots_dir.mkdir(exist_ok=True)
        
    def plot_data_overview(self, df: pd.DataFrame):
        """–û–±–∑–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Data Overview', fontsize=16)
        
        # 1. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        symbol_counts = df['symbol'].value_counts().head(20)
        axes[0,0].bar(range(len(symbol_counts)), symbol_counts.values)
        axes[0,0].set_title('–¢–æ–ø-20 —Å–∏–º–≤–æ–ª–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –¥–∞–Ω–Ω—ã—Ö')
        axes[0,0].set_xlabel('–°–∏–º–≤–æ–ª—ã')
        axes[0,0].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π')
        axes[0,0].tick_params(axis='x', rotation=45)
        
        # 2. –í—Ä–µ–º–µ–Ω–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ
        if 'timestamp' in df.columns:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω timestamp
                min_ts = df['timestamp'].min()
                max_ts = df['timestamp'].max()
                logger.info(f"   –î–∏–∞–ø–∞–∑–æ–Ω timestamp: {min_ts} - {max_ts}")
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ timestamp (—Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ –∏–ª–∏ –º–∞–ª–µ–Ω—å–∫–∏–µ)
                valid_mask = (df['timestamp'] > 0) & (df['timestamp'] < 2147483647)  # Unix timestamp –¥–æ 2038 –≥–æ–¥–∞
                if not valid_mask.all():
                    logger.warning(f"   ‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ timestamp: {(~valid_mask).sum()} –∑–∞–ø–∏—Å–µ–π")
                    df_valid = df[valid_mask].copy()
                else:
                    df_valid = df
                    
                df_valid['date'] = pd.to_datetime(df_valid['timestamp'], unit='s', errors='coerce')
                date_range = df_valid.groupby('symbol')['date'].agg(['min', 'max'])
                date_range['days'] = (date_range['max'] - date_range['min']).dt.days
                
                axes[0,1].scatter(range(len(date_range.head(20))), 
                                date_range['days'].head(20))
                axes[0,1].set_title('–í—Ä–µ–º–µ–Ω–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –ø–æ —Å–∏–º–≤–æ–ª–∞–º (–¥–Ω–∏)')
                axes[0,1].set_xlabel('–°–∏–º–≤–æ–ª—ã')
                axes[0,1].set_ylabel('–î–Ω–∏')
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞—Ç: {e}")
                axes[0,1].text(0.5, 0.5, '–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞—Ç', 
                             horizontalalignment='center', verticalalignment='center')
                axes[0,1].set_title('–í—Ä–µ–º–µ–Ω–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ')
        
        # 3. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ expected returns
        if 'buy_expected_return' in df.columns and 'sell_expected_return' in df.columns:
            axes[1,0].hist([df['buy_expected_return'].dropna(), 
                          df['sell_expected_return'].dropna()], 
                          bins=50, alpha=0.7, label=['Buy', 'Sell'])
            axes[1,0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Expected Returns')
            axes[1,0].set_xlabel('Return %')
            axes[1,0].set_ylabel('Frequency')
            axes[1,0].legend()
            axes[1,0].axvline(x=0, color='red', linestyle='--', alpha=0.5)
        
        # 4. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        key_features = ['rsi_val', 'macd_hist', 'adx_val', 'volume_ratio']
        available_features = [f for f in key_features if f in df.columns]
        if available_features:
            corr_data = df[available_features].corr()
            sns.heatmap(corr_data, annot=True, ax=axes[1,1], cmap='coolwarm',
                       center=0, vmin=-1, vmax=1)
            axes[1,1].set_title('–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
        
        plt.tight_layout()
        plt.savefig(self.plots_dir / 'data_overview.png', dpi=150, bbox_inches='tight')
        plt.close()
        logger.info(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: data_overview.png")
        
    def plot_feature_distributions(self, X: np.ndarray, feature_names: List[str], 
                                 sample_size: int = 10000):
        """–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        # –°–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        if len(X) > sample_size:
            indices = np.random.choice(len(X), sample_size, replace=False)
            X_sample = X[indices]
        else:
            X_sample = X
        
        # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        important_features = [
            'momentum_score', 'volume_strength_score', 'volatility_regime_score',
            'rsi_val', 'macd_bullish', 'adx_val', 'bb_position', 'atr_norm'
        ]
        
        available_features = []
        feature_indices = []
        for feature in important_features:
            if feature in feature_names:
                available_features.append(feature)
                feature_indices.append(feature_names.index(feature))
        
        if not available_features:
            logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω—ã –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
            return
            
        n_features = len(available_features)
        n_cols = 4
        n_rows = (n_features + n_cols - 1) // n_cols
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))
        fig.suptitle('Feature Distributions', fontsize=16)
        
        if n_rows == 1:
            axes = axes.reshape(1, -1)
        
        for i, (idx, feature) in enumerate(zip(feature_indices, available_features)):
            row = i // n_cols
            col = i % n_cols
            ax = axes[row, col]
            
            data = X_sample[:, idx]
            ax.hist(data, bins=50, alpha=0.7, edgecolor='black')
            ax.set_title(f'Distribution: {feature}')
            ax.set_xlabel('Value')
            ax.set_ylabel('Frequency')
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            mean_val = np.mean(data)
            median_val = np.median(data)
            ax.axvline(mean_val, color='red', linestyle='--', 
                      label=f'Mean: {mean_val:.2f}', linewidth=2)
            ax.axvline(median_val, color='green', linestyle='--', 
                      label=f'Median: {median_val:.2f}', linewidth=2)
            ax.legend()
        
        # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ subplot'—ã
        for i in range(n_features, n_rows * n_cols):
            row = i // n_cols
            col = i % n_cols
            fig.delaxes(axes[row, col])
            
        plt.tight_layout()
        plt.savefig(self.plots_dir / 'feature_distributions.png', dpi=150, bbox_inches='tight')
        plt.close()
        logger.info(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: feature_distributions.png")
        
    def plot_training_comparison(self, metrics_history: Dict):
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Model Performance Comparison', fontsize=16)
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        metrics = ['accuracy', 'precision', 'recall', 'f1']
        
        for i, metric in enumerate(metrics):
            ax = axes[i // 2, i % 2]
            
            for model_name, history in metrics_history.items():
                if metric in history:
                    values = history[metric]
                    if isinstance(values, list):
                        ax.plot(values, label=model_name, marker='o')
                    else:
                        ax.bar(model_name, values)
                    
            ax.set_title(f'{metric.capitalize()} Comparison')
            ax.set_xlabel('Model/Iteration')
            ax.set_ylabel(metric.capitalize())
            ax.legend()
            ax.grid(True, alpha=0.3)
            
        plt.tight_layout()
        plt.savefig(self.plots_dir / 'model_comparison.png', dpi=150, bbox_inches='tight')
        plt.close()
        logger.info(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: model_comparison.png")


class XGBoostEnhancedTrainer:
    """Enhanced XGBoost trainer —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –≤—Å–µ—Ö —Ñ–∏—á –∏–∑ TFT v2.1"""
    
    def __init__(self, config_path='config.yaml'):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞"""
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.db_config = self.config['database'].copy()
        if not self.db_config.get('password'):
            self.db_config.pop('password', None)
            
        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ TFT v2.1 - –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ –ë–î
        self.TECHNICAL_INDICATORS = [
            # –¢—Ä–µ–Ω–¥–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            'ema_15', 'adx_val', 'adx_plus_di', 'adx_minus_di', 'adx_diff',
            'macd_val', 'macd_signal', 'macd_hist', 'macd_signal_ratio',
            'sar', 'ichimoku_conv', 'ichimoku_base', 'ichimoku_diff',
            'aroon_up', 'aroon_down', 'dpo',
            # –û—Å—Ü–∏–ª–ª—è—Ç–æ—Ä—ã
            'rsi_val', 'rsi_dist_from_mid', 'stoch_k', 'stoch_d', 'stoch_diff',
            'cci_val', 'roc', 'williams_r', 'ult_osc', 'mfi',
            # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
            'atr_val', 'atr_norm', 'bb_position', 'bb_upper', 'bb_lower', 'bb_basis',
            'donchian_upper', 'donchian_lower',
            # –û–±—ä–µ–º
            'obv', 'cmf', 'volume_sma',
            # Vortex
            'vortex_vip', 'vortex_vin', 'vortex_ratio',
            # –ò–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã –∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
            'price_change_1', 'price_change_4', 'price_change_16',
            'volatility_4', 'volatility_16', 'volume_ratio',
            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            'hour', 'day_of_week', 'is_weekend'
        ]
        
        self.MARKET_FEATURES = [
            # BTC –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –∏ –º–µ—Ç—Ä–∏–∫–∏ (—ç—Ç–∏ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ)
            'btc_correlation_20', 'btc_correlation_60',
            'btc_return_1h', 'btc_return_4h', 'btc_volatility',
            'relative_strength_btc',
            'market_regime_low_vol', 'market_regime_med_vol', 'market_regime_high_vol',
            # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos'
        ]
        
        self.OHLC_FEATURES = [
            'open_ratio', 'high_ratio', 'low_ratio', 'hl_spread',
            'body_size', 'upper_shadow', 'lower_shadow', 'is_bullish',
            'log_return', 'log_volume',
            'price_to_ema15', 'price_to_ema50', 'price_to_vwap'
        ]
        
        # Symbol features –±—É–¥—É—Ç –¥–æ–±–∞–≤–ª–µ–Ω—ã –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏
        self.TOP_SYMBOLS = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'SOLUSDT', 'XRPUSDT',
                           'ADAUSDT', 'DOGEUSDT', 'AVAXUSDT', 'DOTUSDT', 'MATICUSDT']
        
        self.models = {}
        self.scalers = {}
        self.feature_names = None
        self.cache_manager = None
        self.visualizer = None
        self.use_cache = False
        self.force_reload = False
        
    def connect_db(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL"""
        try:
            conn = psycopg2.connect(**self.db_config)
            logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
            return conn
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î: {e}")
            raise
            
    def load_data_batch(self, symbols: List[str], conn) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–ø–∏—Å–∫–∞ —Å–∏–º–≤–æ–ª–æ–≤ —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        # –í–ê–ñ–ù–û: –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º –¢–û–õ–¨–ö–û –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        # –ù–ï –∑–∞–≥—Ä—É–∂–∞–µ–º: buy/sell_profit_target, buy/sell_loss_target, max_profit, realized_profit
        query = """
        SELECT 
            pm.symbol, pm.timestamp, pm.datetime,
            pm.technical_indicators,
            pm.buy_expected_return,  -- –¢–û–õ–¨–ö–û –¥–ª—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
            pm.sell_expected_return, -- –¢–û–õ–¨–ö–û –¥–ª—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
            rm.open, rm.high, rm.low, rm.close, rm.volume
        FROM processed_market_data pm
        JOIN raw_market_data rm ON pm.raw_data_id = rm.id
        WHERE pm.symbol = ANY(%s)
        ORDER BY pm.timestamp
        """
        
        df = pd.read_sql_query(query, conn, params=(symbols,))
        logger.info(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df):,} –∑–∞–ø–∏—Å–µ–π –¥–ª—è {symbols}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ—Ç–¥–µ–ª—å–Ω–æ –ø–µ—Ä–µ–¥ –∏—Ö —É–¥–∞–ª–µ–Ω–∏–µ–º
        target_columns = ['buy_expected_return', 'sell_expected_return']
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –∏–∑ JSON
        if 'technical_indicators' in df.columns:
            logger.info("   üìä –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –∏–∑ JSON...")
            indicators_df = pd.json_normalize(df['technical_indicators'])
            
            # –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤ technical_indicators –Ω–µ—Ç —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
            for target_col in target_columns:
                if target_col in indicators_df.columns:
                    logger.warning(f"   ‚ö†Ô∏è –û–ë–ù–ê–†–£–ñ–ï–ù–ê –£–¢–ï–ß–ö–ê: {target_col} –Ω–∞–π–¥–µ–Ω –≤ technical_indicators! –£–¥–∞–ª—è–µ–º...")
                    indicators_df = indicators_df.drop(columns=[target_col])
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –æ—Å–Ω–æ–≤–Ω—ã–º DataFrame
            df = pd.concat([df, indicators_df], axis=1)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            found_indicators = [col for col in indicators_df.columns if col in self.TECHNICAL_INDICATORS]
            logger.info(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(found_indicators)} –∏–∑ {len(self.TECHNICAL_INDICATORS)} –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤")
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞
            if len(symbols) == 2 and 'BTCUSDT' in symbols:
                missing_indicators = [ind for ind in self.TECHNICAL_INDICATORS if ind not in indicators_df.columns]
                if missing_indicators:
                    logger.warning(f"   ‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ {len(missing_indicators)} –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤: {missing_indicators[:5]}...")
                
                extra_indicators = [col for col in indicators_df.columns if col not in self.TECHNICAL_INDICATORS]
                if extra_indicators and len(extra_indicators) < 10:
                    logger.info(f"   üîç –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –≤ –ë–î: {extra_indicators}")
                    
                # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∑–Ω–∞—á–µ–Ω–∏–π –∫–ª—é—á–µ–≤—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
                logger.info("   üìä –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–Ω–∞—á–µ–Ω–∏–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤:")
                if 'rsi_val' in indicators_df.columns:
                    rsi_stats = indicators_df['rsi_val'].describe()
                    logger.info(f"      RSI: min={rsi_stats['min']:.2f}, max={rsi_stats['max']:.2f}, mean={rsi_stats['mean']:.2f}")
                    
                if 'macd_hist' in indicators_df.columns:
                    macd_stats = indicators_df['macd_hist'].describe()
                    logger.info(f"      MACD hist: min={macd_stats['min']:.4f}, max={macd_stats['max']:.4f}, mean={macd_stats['mean']:.4f}")
                    
                if 'adx_val' in indicators_df.columns:
                    adx_stats = indicators_df['adx_val'].describe()
                    logger.info(f"      ADX: min={adx_stats['min']:.2f}, max={adx_stats['max']:.2f}, mean={adx_stats['mean']:.2f}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å–æ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            for indicator in self.TECHNICAL_INDICATORS:
                if indicator not in df.columns:
                    df[indicator] = 0.0
        
        # –õ–æ–≥–∏—Ä—É–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        if len(symbols) == 2 and 'BTCUSDT' in symbols:  # –¢–æ–ª—å–∫–æ –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞
            logger.info(f"   üìã –í—Å–µ–≥–æ –∫–æ–ª–æ–Ω–æ–∫: {len(df.columns)}")
            if 'buy_expected_return' in df.columns and 'sell_expected_return' in df.columns:
                logger.info("   ‚úÖ –¶–µ–ª–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –Ω–∞–π–¥–µ–Ω—ã –∏ –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –¢–û–õ–¨–ö–û –∫–∞–∫ targets")
            
        return df
        
    def load_market_data(self, conn) -> Dict[str, pd.DataFrame]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö BTC –∏ –¥—Ä—É–≥–∏—Ö —Ç–æ–ø –º–æ–Ω–µ—Ç –¥–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π"""
        market_data = {}
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º BTC
        query = """
        SELECT timestamp, close, volume,
               (high - low) / close as volatility
        FROM raw_market_data
        WHERE symbol = 'BTCUSDT'
        ORDER BY timestamp
        """
        btc_df = pd.read_sql_query(query, conn)
        btc_df['return_1h'] = btc_df['close'].pct_change(4)
        btc_df['return_4h'] = btc_df['close'].pct_change(16)
        btc_df['volatility_20'] = btc_df['volatility'].rolling(20).mean()
        market_data['BTCUSDT'] = btc_df
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ BTC: {len(btc_df)} –∑–∞–ø–∏—Å–µ–π")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ø –∞–ª—å—Ç–∫–æ–∏–Ω—ã –¥–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
        alt_symbols = ['ETHUSDT', 'BNBUSDT', 'XRPUSDT']
        for symbol in alt_symbols:
            query = f"""
            SELECT timestamp, close
            FROM raw_market_data
            WHERE symbol = '{symbol}'
            ORDER BY timestamp
            """
            market_data[symbol] = pd.read_sql_query(query, conn)
            
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å: {alt_symbols}")
        return market_data
        
    def calculate_market_features(self, df: pd.DataFrame, market_data: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """–†–∞—Å—á–µ—Ç market features –∫–∞–∫ –≤ TFT v2.1"""
        logger.info("üîß –†–∞—Å—á–µ—Ç —Ä—ã–Ω–æ—á–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ OHLC features...")
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø timestamp –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ
        if df['timestamp'].dtype == 'int64' or df['timestamp'].dtype == 'float64':
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∞—Å—à—Ç–∞–± timestamp
            sample_ts = df['timestamp'].iloc[0]
            if sample_ts > 1e10:  # –í–µ—Ä–æ—è—Ç–Ω–æ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã
                df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms', errors='coerce')
            else:  # –°–µ–∫—É–Ω–¥—ã
                df['datetime'] = pd.to_datetime(df['timestamp'], unit='s', errors='coerce')
        else:
            df['datetime'] = pd.to_datetime(df['timestamp'], errors='coerce')
            
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        # hour –∏ day_of_week —É–∂–µ –º–æ–≥—É—Ç –±—ã—Ç—å –≤ –ë–î –∏–∑ prepare_dataset.py
        if 'hour' not in df.columns and 'datetime' in df.columns:
            df['hour'] = df['datetime'].dt.hour
        if 'day_of_week' not in df.columns and 'datetime' in df.columns:
            df['day_of_week'] = df['datetime'].dt.dayofweek
        
        # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º day_of_week –≤–º–µ—Å—Ç–æ dow)
        if 'hour' in df.columns:
            df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
            df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
        if 'day_of_week' in df.columns:
            df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
            df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
        # is_weekend —É–∂–µ —Å–æ–∑–¥–∞–µ—Ç—Å—è –≤ prepare_dataset.py –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ –ë–î
        # –ù–µ —Å–æ–∑–¥–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç
        # df['is_weekend'] = (df['dow'] >= 5).astype(int)
        
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–ª–æ–Ω–∫—É datetime –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ –Ω—É–∂–Ω–∞
        if 'datetime' in df.columns and 'datetime' not in self.MARKET_FEATURES:
            df = df.drop(columns=['datetime'])
        
        # Merge —Å BTC –¥–∞–Ω–Ω—ã–º–∏
        btc_df = market_data['BTCUSDT']
        df = df.merge(
            btc_df[['timestamp', 'return_1h', 'return_4h', 'volatility_20']],
            on='timestamp',
            how='left',
            suffixes=('', '_btc')
        )
        df.rename(columns={
            'return_1h': 'btc_return_1h',
            'return_4h': 'btc_return_4h',
            'volatility_20': 'btc_volatility'
        }, inplace=True)
        
        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å BTC (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
        # –ü—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º BTC —Ü–µ–Ω—É –∫ –∫–∞–∂–¥–æ–π –∑–∞–ø–∏—Å–∏ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
        df = df.merge(
            btc_df[['timestamp', 'close']].rename(columns={'close': 'btc_close'}),
            on='timestamp',
            how='left'
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –ø–æ—Å–ª–µ merge
        if df.columns.duplicated().any():
            logger.warning(f"–ù–∞–π–¥–µ–Ω—ã –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {df.columns[df.columns.duplicated()].tolist()}")
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, –æ—Å—Ç–∞–≤–ª—è—è –ø–µ—Ä–≤—É—é
            df = df.loc[:, ~df.columns.duplicated()]
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —á–µ—Ä–µ–∑ rolling (–±–µ–∑ concat –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –∫–æ–ª–æ–Ω–æ–∫)
        df = df.sort_values(['symbol', 'timestamp']).reset_index(drop=True)
        
        for window in [20, 60]:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–ª–æ–Ω–∫—É
            df[f'btc_correlation_{window}'] = 0.0
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
            for symbol in df['symbol'].unique():
                mask = df['symbol'] == symbol
                # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –∏ —Å—Ä–∞–∑—É –∑–∞–ø–æ–ª–Ω—è–µ–º NaN –Ω—É–ª—è–º–∏
                df.loc[mask, f'btc_correlation_{window}'] = (
                    df.loc[mask, 'close'].rolling(window)
                    .corr(df.loc[mask, 'btc_close'])
                    .fillna(0)
                )
        
        df.drop('btc_close', axis=1, inplace=True)
            
        # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è —Å–∏–ª–∞ –∫ BTC
        btc_returns = btc_df.set_index('timestamp')['close'].pct_change(20)
        df['symbol_return_20'] = df.groupby('symbol')['close'].transform(lambda x: x.pct_change(20))
        df = df.merge(
            btc_returns.reset_index().rename(columns={'close': 'btc_return_20'}),
            on='timestamp',
            how='left'
        )
        df['relative_strength_btc'] = df['symbol_return_20'] / df['btc_return_20'].replace(0, np.nan)
        df.drop(['symbol_return_20', 'btc_return_20'], axis=1, inplace=True)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ—Å–ª–µ –≤—Å–µ—Ö merge –æ–ø–µ—Ä–∞—Ü–∏–π
        if df.columns.duplicated().any():
            logger.warning(f"–î—É–±–ª–∏–∫–∞—Ç—ã –ø–æ—Å–ª–µ merge: {df.columns[df.columns.duplicated()].tolist()}")
            df = df.loc[:, ~df.columns.duplicated()]
        
        # Market regime
        btc_vol = df['btc_volatility'].fillna(df['btc_volatility'].mean())
        vol_percentiles = btc_vol.quantile([0.33, 0.67])
        df['market_regime_low_vol'] = (btc_vol <= vol_percentiles[0.33]).astype(int)
        df['market_regime_med_vol'] = ((btc_vol > vol_percentiles[0.33]) & (btc_vol <= vol_percentiles[0.67])).astype(int)
        df['market_regime_high_vol'] = (btc_vol > vol_percentiles[0.67]).astype(int)
        
        # OHLC features
        df['open_ratio'] = df['open'] / df['close']
        df['high_ratio'] = df['high'] / df['close']
        df['low_ratio'] = df['low'] / df['close']
        df['hl_spread'] = (df['high'] - df['low']) / df['close']
        df['body_size'] = np.abs(df['close'] - df['open']) / df['close']
        df['upper_shadow'] = (df['high'] - np.maximum(df['open'], df['close'])) / df['close']
        df['lower_shadow'] = (np.minimum(df['open'], df['close']) - df['low']) / df['close']
        df['is_bullish'] = (df['close'] > df['open']).astype(int)
        df['log_return'] = df.groupby('symbol')['close'].transform(
            lambda x: np.log(x / x.shift(1).replace(0, np.nan))
        )
        df['log_volume'] = np.log1p(df['volume'])
        
        # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if 'hour' in df.columns:
            df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
            df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
            logger.info("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏–µ —á–∞—Å–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")
        
        if 'day_of_week' in df.columns:
            df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
            df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
            logger.info("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –¥–Ω–µ–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")
        
        # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ (–ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–æ–∫)
        if 'ema_15' in df.columns:
            df['price_to_ema15'] = df['close'] / df['ema_15'].replace(0, np.nan)
        else:
            df['price_to_ema15'] = 1.0
            
        # EMA50 –Ω–µ—Ç –≤ –ë–î, –Ω–æ –º–æ–∂–Ω–æ —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å
        # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ ema_15
        df['price_to_ema50'] = 1.0
        df['price_to_vwap'] = 1.0
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∏ inf –∑–Ω–∞—á–µ–Ω–∏—è
        # –°–Ω–∞—á–∞–ª–∞ –∑–∞–º–µ–Ω—è–µ–º inf –Ω–∞ NaN
        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
        # –î–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ–º 0
        for col in df.columns:
            if 'correlation' in col:
                df[col].fillna(0, inplace=True)
            elif 'ratio' in col or 'relative' in col:
                df[col].fillna(1, inplace=True)
            else:
                # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö - forward fill, –∑–∞—Ç–µ–º 0
                df[col].fillna(method='ffill', inplace=True)
                df[col].fillna(0, inplace=True)
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –Ω–µ –ø–æ–ø–∞–ª–∏ –≤ market features
        target_columns = ['buy_expected_return', 'sell_expected_return', 'expected_return_buy', 'expected_return_sell']
        for target_col in target_columns:
            if target_col in df.columns and target_col not in ['buy_expected_return', 'sell_expected_return']:
                logger.error(f"üö® –û–ë–ù–ê–†–£–ñ–ï–ù–ê –£–¢–ï–ß–ö–ê –≤ market features: {target_col}! –≠—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é!")
                logger.warning(f"   –ö–æ–ª–æ–Ω–∫–∞ {target_col} –Ω–µ –¥–æ–ª–∂–Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –∫–∞–∫ –ø—Ä–∏–∑–Ω–∞–∫!")
        
        logger.info("‚úÖ –†—ã–Ω–æ—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã")
        return df
        
    def load_data_parallel(self, symbols: List[str], conn, batch_size: int = 10) -> pd.DataFrame:
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å–∏–º–≤–æ–ª–∞–º –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è"""
        logger.info(f"‚ö° –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {len(symbols)} —Å–∏–º–≤–æ–ª–æ–≤...")
        
        # –†–∞–∑–±–∏–≤–∞–µ–º —Å–∏–º–≤–æ–ª—ã –Ω–∞ –±–∞—Ç—á–∏
        symbol_batches = [symbols[i:i+batch_size] for i in range(0, len(symbols), batch_size)]
        
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = []
            for batch in symbol_batches:
                future = executor.submit(self.load_data_batch, batch, conn)
                futures.append(future)
            
            all_data = []
            for future in tqdm(as_completed(futures), total=len(futures), 
                             desc="–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞—Ç—á–µ–π"):
                try:
                    df_batch = future.result()
                    all_data.append(df_batch)
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞—Ç—á–∞: {e}")
                    
        if all_data:
            df = pd.concat(all_data, ignore_index=True)
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –≤—Å–µ–≥–æ {len(df):,} –∑–∞–ø–∏—Å–µ–π")
            return df
        else:
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
        
    def optimize_memory(self, df: pd.DataFrame) -> pd.DataFrame:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ DataFrame"""
        logger.info("üíæ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏...")
        start_mem = df.memory_usage().sum() / 1024**2
        
        for col in df.columns:
            col_type = df[col].dtype
            
            if col_type != 'object':
                c_min = df[col].min()
                c_max = df[col].max()
                
                if str(col_type)[:3] == 'int':
                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                        df[col] = df[col].astype(np.int8)
                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                        df[col] = df[col].astype(np.int16)
                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                        df[col] = df[col].astype(np.int32)
                else:
                    if col not in ['timestamp', 'datetime']:  # –ù–µ —Ç—Ä–æ–≥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
                        if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                            df[col] = df[col].astype(np.float32)
                            
        end_mem = df.memory_usage().sum() / 1024**2
        logger.info(f"   –ü–∞–º—è—Ç—å —É–º–µ–Ω—å—à–µ–Ω–∞ —Å {start_mem:.1f} MB –¥–æ {end_mem:.1f} MB "
                   f"({100 * (start_mem - end_mem) / start_mem:.1f}% —ç–∫–æ–Ω–æ–º–∏–∏)")
        
        return df
        
    def create_weighted_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∑–≤–µ—à–µ–Ω–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        logger.info("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –≤–∑–≤–µ—à–µ–Ω–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        # 1. –ú—É–ª—å—Ç–∏–ø–ª–∏–∫–∞—Ç–∏–≤–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è (a*b)
        if 'rsi_val' in df.columns and 'macd_hist' in df.columns:
            df['rsi_macd_interaction'] = df['rsi_val'] * df['macd_hist']
        
        if 'volume_ratio' in df.columns and 'volatility_4' in df.columns:
            df['volume_volatility_interaction'] = df['volume_ratio'] * df['volatility_4']
            
        if 'adx_val' in df.columns and 'strong_trend' in df.columns:
            df['adx_trend_strength'] = df['adx_val'] * df['strong_trend']
        
        # 2. –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è (a/b)
        if 'rsi_val' in df.columns and 'adx_val' in df.columns:
            df['rsi_to_adx'] = df['rsi_val'] / (df['adx_val'] + 1e-8)
            
        if 'volume_ratio' in df.columns and 'volatility_16' in df.columns:
            df['volume_to_volatility'] = df['volume_ratio'] / (df['volatility_16'] + 1e-8)
            
        if 'price_change_4' in df.columns and 'price_change_16' in df.columns:
            df['price_momentum_ratio'] = df['price_change_4'] / (df['price_change_16'] + 1e-8)
        
        # 3. –ê–¥–¥–∏—Ç–∏–≤–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ (a+b)
        momentum_features = ['rsi_val', 'macd_hist', 'roc_val']
        if all(f in df.columns for f in momentum_features):
            df['momentum_composite'] = df[momentum_features].sum(axis=1)
            
        volatility_features = ['atr_val', 'volatility_4']
        if all(f in df.columns for f in volatility_features):
            df['volatility_composite'] = df[volatility_features].sum(axis=1)
        
        # 4. –°–ª–æ–∂–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å –≤–µ—Å–∞–º–∏ –¥–ª—è —Ç–æ—á–µ–∫ –≤—Ö–æ–¥–∞
        if all(f in df.columns for f in ['rsi_val', 'macd_bullish', 'volume_spike', 'bb_position']):
            # –ü–∞—Ç—Ç–µ—Ä–Ω oversold reversal
            df['oversold_reversal_score'] = (
                np.maximum(0, 30 - df['rsi_val']) * 0.4 +  # –ß–µ–º –Ω–∏–∂–µ RSI –æ—Ç 30, —Ç–µ–º –≤—ã—à–µ –≤–µ—Å
                df['macd_bullish'] * 20 +                   # Bullish MACD crossover
                df['volume_spike'] * 15 +                   # Volume confirmation
                (df['bb_position'] < 0.2).astype(int) * 15  # Near lower Bollinger band
            )
            
        if all(f in df.columns for f in ['bb_position', 'adx_val', 'volume_spike', 'rsi_val']):
            # –ü–∞—Ç—Ç–µ—Ä–Ω breakout
            df['breakout_score'] = (
                (df['bb_position'] > 0.8).astype(int) * 20 +  # Near upper band
                (df['adx_val'] > 25).astype(int) * 15 +       # Strong trend
                df['volume_spike'] * 20 +                      # Volume breakout
                (df['rsi_val'] > 50).astype(int) * 10         # Momentum confirmation
            )
            
        # 5. Market regime –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        if all(f in df.columns for f in ['market_regime_low_vol', 'adx_val', 'volume_ratio']):
            # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è range trading - –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ
            # market_regime_low_vol —É–∂–µ —è–≤–ª—è–µ—Ç—Å—è –±–∏–Ω–∞—Ä–Ω—ã–º (0 –∏–ª–∏ 1)
            df['range_trading_score'] = (
                df['market_regime_low_vol'].astype(float) * 30 +
                (df['adx_val'] < 20).astype(float) * 20 +
                (df['volume_ratio'] < 1).astype(float) * 10
            )
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ score –Ω–µ –≤—Å–µ–≥–¥–∞ 0
            if df['range_trading_score'].sum() == 0:
                logger.warning("‚ö†Ô∏è range_trading_score –≤—Å–µ–≥–¥–∞ 0, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")
                logger.info(f"   market_regime_low_vol mean: {df['market_regime_low_vol'].mean():.3f}")
                logger.info(f"   adx_val < 20: {(df['adx_val'] < 20).mean():.3f}")
                logger.info(f"   volume_ratio < 1: {(df['volume_ratio'] < 1).mean():.3f}")
            
        # 6. Divergence –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        if 'rsi_val' in df.columns and 'price_change_4' in df.columns:
            # RSI divergence (—Ü–µ–Ω–∞ —Ä–∞—Å—Ç–µ—Ç, RSI –ø–∞–¥–∞–µ—Ç)
            df['rsi_divergence'] = (
                (df['price_change_4'] > 0) & 
                (df['rsi_val'].diff(4) < 0)
            ).astype(int)
            
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len([c for c in df.columns if any(p in c for p in ['interaction', 'score', 'composite', 'ratio'])])} –≤–∑–≤–µ—à–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        return df
    
    def add_rolling_statistics(self, df: pd.DataFrame) -> pd.DataFrame:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–∫–æ–ª—å–∑—è—â–∏—Ö —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –¥–ª—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        logger.info("üìä –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–∫–æ–ª—å–∑—è—â–∏—Ö —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫...")
        
        # RSI —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        if 'rsi_val' in df.columns:
            df['rsi_rolling_mean_10'] = df.groupby('symbol')['rsi_val'].transform(lambda x: x.rolling(10, min_periods=1).mean())
            df['rsi_rolling_std_10'] = df.groupby('symbol')['rsi_val'].transform(lambda x: x.rolling(10, min_periods=1).std())
            df['rsi_rolling_max_10'] = df.groupby('symbol')['rsi_val'].transform(lambda x: x.rolling(10, min_periods=1).max())
            df['rsi_rolling_min_10'] = df.groupby('symbol')['rsi_val'].transform(lambda x: x.rolling(10, min_periods=1).min())
        
        # Volume —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        if 'volume' in df.columns:
            df['volume_rolling_mean_20'] = df.groupby('symbol')['volume'].transform(lambda x: x.rolling(20, min_periods=1).mean())
            df['volume_rolling_std_20'] = df.groupby('symbol')['volume'].transform(lambda x: x.rolling(20, min_periods=1).std())
            # Volume trend
            df['volume_trend'] = df['volume_rolling_mean_20'] / df.groupby('symbol')['volume'].transform(lambda x: x.rolling(60, min_periods=1).mean())
        
        # Price momentum —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        if 'close' in df.columns:
            df['price_momentum_10'] = df.groupby('symbol')['close'].transform(lambda x: x.pct_change(10))
            df['price_momentum_20'] = df.groupby('symbol')['close'].transform(lambda x: x.pct_change(20))
            df['momentum_acceleration'] = df['price_momentum_10'] - df['price_momentum_20']
        
        # ATR —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        if 'atr_val' in df.columns:
            df['atr_rolling_mean_10'] = df.groupby('symbol')['atr_val'].transform(lambda x: x.rolling(10, min_periods=1).mean())
            df['atr_expansion'] = df['atr_val'] / df['atr_rolling_mean_10']
        
        logger.info("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏")
        return df
    
    def add_divergences(self, df: pd.DataFrame) -> pd.DataFrame:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–π –º–µ–∂–¥—É —Ü–µ–Ω–æ–π –∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏"""
        logger.info("üîÑ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–π...")
        
        # RSI –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è
        if 'rsi_val' in df.columns and 'price_change_4' in df.columns:
            # Bullish divergence: —Ü–µ–Ω–∞ –ø–∞–¥–∞–µ—Ç, RSI —Ä–∞—Å—Ç–µ—Ç
            df['rsi_bullish_divergence'] = (
                (df['price_change_4'] < 0) & 
                (df.groupby('symbol')['rsi_val'].transform(lambda x: x.diff(4)) > 0)
            ).astype(int)
            
            # Bearish divergence: —Ü–µ–Ω–∞ —Ä–∞—Å—Ç–µ—Ç, RSI –ø–∞–¥–∞–µ—Ç
            df['rsi_bearish_divergence'] = (
                (df['price_change_4'] > 0) & 
                (df.groupby('symbol')['rsi_val'].transform(lambda x: x.diff(4)) < 0)
            ).astype(int)
        
        # MACD –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è
        if 'macd_hist' in df.columns and 'price_change_4' in df.columns:
            df['macd_bullish_divergence'] = (
                (df['price_change_4'] < 0) & 
                (df.groupby('symbol')['macd_hist'].transform(lambda x: x.diff(4)) > 0)
            ).astype(int)
            
            df['macd_bearish_divergence'] = (
                (df['price_change_4'] > 0) & 
                (df.groupby('symbol')['macd_hist'].transform(lambda x: x.diff(4)) < 0)
            ).astype(int)
        
        # Volume-Price –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è
        if 'volume' in df.columns and 'close' in df.columns:
            volume_change = df.groupby('symbol')['volume'].transform(lambda x: x.pct_change(4))
            price_change = df.groupby('symbol')['close'].transform(lambda x: x.pct_change(4))
            
            # Volume —Ä–∞—Å—Ç–µ—Ç, —Ü–µ–Ω–∞ –ø–∞–¥–∞–µ—Ç - –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ
            df['volume_price_divergence'] = (
                (volume_change > 0.5) & (price_change < -0.01)
            ).astype(int)
        
        logger.info("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏")
        return df
    
    def add_candle_patterns(self, df: pd.DataFrame) -> pd.DataFrame:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Å–≤–µ—á–µ–π"""
        logger.info("üïØÔ∏è –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Å–≤–µ—á–µ–π...")
        
        # Hammer pattern
        df['is_hammer'] = (
            (df['lower_shadow'] > df['body_size'] * 2) &
            (df['upper_shadow'] < df['body_size'] * 0.5) &
            (df['body_size'] < df['hl_spread'] * 0.3)
        ).astype(int)
        
        # Doji pattern
        df['is_doji'] = (df['body_size'] < df['hl_spread'] * 0.1).astype(int)
        
        # Engulfing pattern
        prev_body = df.groupby('symbol')['body_size'].shift(1)
        prev_close = df.groupby('symbol')['close'].shift(1)
        prev_open = df.groupby('symbol')['open'].shift(1)
        
        # Bullish engulfing
        df['bullish_engulfing'] = (
            (df['close'] > df['open']) &  # –¢–µ–∫—É—â–∞—è —Å–≤–µ—á–∞ –∑–µ–ª–µ–Ω–∞—è
            (prev_close < prev_open) &    # –ü—Ä–µ–¥—ã–¥—É—â–∞—è —Å–≤–µ—á–∞ –∫—Ä–∞—Å–Ω–∞—è
            (df['body_size'] > prev_body * 1.5) &  # –¢–µ–∫—É—â–µ–µ —Ç–µ–ª–æ –±–æ–ª—å—à–µ
            (df['close'] > prev_open) &   # –ó–∞–∫—Ä—ã—Ç–∏–µ –≤—ã—à–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è
            (df['open'] < prev_close)     # –û—Ç–∫—Ä—ã—Ç–∏–µ –Ω–∏–∂–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∑–∞–∫—Ä—ã—Ç–∏—è
        ).astype(int)
        
        # Bearish engulfing
        df['bearish_engulfing'] = (
            (df['close'] < df['open']) &  # –¢–µ–∫—É—â–∞—è —Å–≤–µ—á–∞ –∫—Ä–∞—Å–Ω–∞—è
            (prev_close > prev_open) &    # –ü—Ä–µ–¥—ã–¥—É—â–∞—è —Å–≤–µ—á–∞ –∑–µ–ª–µ–Ω–∞—è
            (df['body_size'] > prev_body * 1.5) &
            (df['close'] < prev_open) &
            (df['open'] > prev_close)
        ).astype(int)
        
        # Pin bar pattern
        df['pin_bar'] = (
            ((df['lower_shadow'] > df['body_size'] * 3) | 
             (df['upper_shadow'] > df['body_size'] * 3)) &
            (df['body_size'] < df['hl_spread'] * 0.25)
        ).astype(int)
        
        logger.info("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å–≤–µ—á–µ–π")
        return df
    
    def add_volume_profile(self, df: pd.DataFrame) -> pd.DataFrame:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ volume profile –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        logger.info("üìà –î–æ–±–∞–≤–ª–µ–Ω–∏–µ volume profile...")
        
        # VWAP distance
        if 'vwap' in df.columns:
            df['vwap_distance'] = (df['close'] - df['vwap']) / df['vwap']
        elif 'close' in df.columns and 'volume' in df.columns:
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º VWAP –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            df['vwap'] = df.groupby('symbol').apply(
                lambda x: (x['close'] * x['volume']).rolling(20, min_periods=1).sum() / 
                         x['volume'].rolling(20, min_periods=1).sum()
            ).reset_index(level=0, drop=True)
            df['vwap_distance'] = (df['close'] - df['vwap']) / df['vwap']
        
        # Volume concentration
        if 'volume' in df.columns:
            # –ö–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è –æ–±—ä–µ–º–∞ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö N –±–∞—Ä–∞—Ö
            df['volume_concentration_5'] = df.groupby('symbol')['volume'].transform(
                lambda x: x.rolling(5, min_periods=1).sum() / x.rolling(20, min_periods=1).sum()
            )
        
        logger.info("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã volume profile –ø—Ä–∏–∑–Ω–∞–∫–∏")
        return df
        
    def create_symbol_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ symbol features (one-hot encoding)"""
        # One-hot –¥–ª—è —Ç–æ–ø —Å–∏–º–≤–æ–ª–æ–≤
        for symbol in self.TOP_SYMBOLS:
            df[f'is_{symbol.replace("USDT", "").lower()}'] = (df['symbol'] == symbol).astype(int)
            
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –º–æ–Ω–µ—Ç
        major_coins = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT']
        meme_coins = ['DOGEUSDT', 'SHIBUSDT', 'PEPEUSDT']
        defi_coins = ['UNIUSDT', 'AAVEUSDT', 'CRVUSDT', 'COMPUSDT']
        
        df['is_major'] = df['symbol'].isin(major_coins).astype(int)
        df['is_meme'] = df['symbol'].isin(meme_coins).astype(int)
        df['is_defi'] = df['symbol'].isin(defi_coins).astype(int)
        df['is_alt'] = (~df['symbol'].isin(major_coins)).astype(int)
        
        # –î—É–±–ª–∏—Ä—É–µ–º market regime –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        df['market_regime_low_vol'] = df['market_regime_low_vol']
        df['market_regime_med_vol'] = df['market_regime_med_vol']
        df['market_regime_high_vol'] = df['market_regime_high_vol']
        
        return df
        
    def prepare_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è XGBoost"""
        logger.info(f"\nüìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {df.shape}")
        
        # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –∫–∞–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –µ—Å—Ç—å –≤ –¥–∞–Ω–Ω—ã—Ö
        tech_indicators = [col for col in df.columns if col in self.TECHNICAL_INDICATORS]
        logger.info(f"\nüîç –ù–∞–π–¥–µ–Ω–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤: {len(tech_indicators)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        key_indicators = ['rsi_val', 'macd_hist', 'adx_val', 'bb_upper', 'bb_lower']
        missing_key = [ind for ind in key_indicators if ind not in df.columns]
        if missing_key:
            logger.warning(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–ª—é—á–µ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã: {missing_key}")
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –°–Ω–∞—á–∞–ª–∞ –∏–∑–≤–ª–µ–∫–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        if 'buy_expected_return' not in df.columns:
            logger.error("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ 'buy_expected_return'")
            logger.info(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
            raise KeyError("buy_expected_return not found in DataFrame")
            
        if 'sell_expected_return' not in df.columns:
            logger.error("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ 'sell_expected_return'")
            raise KeyError("sell_expected_return not found in DataFrame")
            
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        y_buy = df['buy_expected_return'].values
        y_sell = df['sell_expected_return'].values
        
        # –õ–û–ì–ò–†–û–í–ê–ù–ò–ï: –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        logger.info("\nüìä –¶–ï–õ–ï–í–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï:")
        logger.info(f"   buy_expected_return: min={y_buy.min():.2f}, max={y_buy.max():.2f}, mean={y_buy.mean():.2f}, std={y_buy.std():.2f}")
        logger.info(f"   sell_expected_return: min={y_sell.min():.2f}, max={y_sell.max():.2f}, mean={y_sell.mean():.2f}, std={y_sell.std():.2f}")
        
        # –í–ê–ñ–ù–û: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è (–í–°–ï –∫–æ–ª–æ–Ω–∫–∏ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –±—É–¥—É—â–µ–º)
        columns_to_exclude = [
            # –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
            'buy_expected_return', 'sell_expected_return',
            'expected_return_buy', 'expected_return_sell',
            # –ë–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏ (—Ç–æ–∂–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –±—É–¥—É—â–µ–º!)
            'buy_profit_target', 'buy_loss_target',
            'sell_profit_target', 'sell_loss_target',
            # –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–∏–±—ã–ª—å (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±—É–¥—É—â–µ–º!)
            'buy_max_profit', 'sell_max_profit',
            'buy_realized_profit', 'sell_realized_profit',
            # ID –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–Ω–µ –Ω—É–∂–Ω—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è)
            'id', 'raw_data_id', 'created_at', 'updated_at',
            'processing_version'
        ]
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–∫–ª—é—á–∞–µ–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ DataFrame
        excluded_found = [col for col in columns_to_exclude if col in df.columns]
        logger.info(f"\nüö´ –ò–°–ö–õ–Æ–ß–ê–ï–ú–´–ï –ö–û–õ–û–ù–ö–ò ({len(excluded_found)}):")
        for col in excluded_found:
            if col in df.columns:
                logger.info(f"   - {col}")
        
        # –£–¥–∞–ª—è–µ–º –≤—Å–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –æ–ø–∞—Å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        df_features = df.drop(columns=columns_to_exclude, errors='ignore')
        
        # –°–æ–∑–¥–∞–µ–º symbol features
        df_features = self.create_symbol_features(df_features)
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º is_bullish –µ—Å–ª–∏ –æ–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç -1
        if 'is_bullish' in df_features.columns:
            # –£–±–µ–¥–∏–º—Å—è —á—Ç–æ is_bullish —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ 0 –∏ 1
            df_features['is_bullish'] = df_features['is_bullish'].apply(lambda x: max(0, x))
        
        # –°–æ–∑–¥–∞–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        df_features = self.create_weighted_features(df_features)
        
        # –ò–Ω–∂–µ–Ω–µ—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–æ–∫)
        # –í–∞–∂–Ω–æ: –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω–∏ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
        if 'rsi_val' in df_features.columns:
            # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É RSI –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            rsi_stats = df_features['rsi_val'].describe()
            logger.info(f"   üìä RSI —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: mean={rsi_stats['mean']:.2f}, std={rsi_stats['std']:.2f}, min={rsi_stats['min']:.2f}, max={rsi_stats['max']:.2f}")
            df_features['rsi_oversold'] = (df_features['rsi_val'] < 30).astype(int)
            df_features['rsi_overbought'] = (df_features['rsi_val'] > 70).astype(int)
            oversold_pct = df_features['rsi_oversold'].mean() * 100
            overbought_pct = df_features['rsi_overbought'].mean() * 100
            logger.info(f"   üìä RSI oversold: {oversold_pct:.1f}%, overbought: {overbought_pct:.1f}%")
        else:
            logger.warning("‚ö†Ô∏è rsi_val –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–∞–Ω–Ω—ã—Ö!")
            df_features['rsi_oversold'] = 0
            df_features['rsi_overbought'] = 0
            
        # –ü–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º macd_bullish, —á—Ç–æ–±—ã —É–±—Ä–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è -1
        if 'macd_hist' in df_features.columns:
            # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É macd_hist –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            macd_stats = df_features['macd_hist'].describe()
            logger.info(f"   üìä MACD hist —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: mean={macd_stats['mean']:.4f}, std={macd_stats['std']:.4f}, min={macd_stats['min']:.4f}, max={macd_stats['max']:.4f}")
            df_features['macd_bullish'] = (df_features['macd_hist'] > 0).astype(int)
            bullish_pct = df_features['macd_bullish'].mean() * 100
            logger.info(f"   üìä MACD bullish –ø—Ä–æ—Ü–µ–Ω—Ç: {bullish_pct:.1f}%")
        elif 'macd_bullish' in df_features.columns:
            # –ï—Å–ª–∏ macd_bullish —É–∂–µ –µ—Å—Ç—å, –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è -1 –Ω–∞ 0
            df_features['macd_bullish'] = df_features['macd_bullish'].apply(lambda x: max(0, x))
        else:
            logger.warning("‚ö†Ô∏è –ù–∏ macd_hist, –Ω–∏ macd_bullish –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –¥–∞–Ω–Ω—ã—Ö!")
            df_features['macd_bullish'] = 0
            
        if 'bb_position' in df_features.columns:
            df_features['bb_near_lower'] = ((df_features['close'] - df_features['bb_position'] * 2) < 0.02).astype(int)
            df_features['bb_near_upper'] = ((df_features['bb_position'] * 2 - df_features['close']) < 0.02).astype(int)
        else:
            df_features['bb_near_lower'] = 0
            df_features['bb_near_upper'] = 0
            
        if 'adx_val' in df_features.columns:
            df_features['strong_trend'] = (df_features['adx_val'] > 25).astype(int)
        else:
            df_features['strong_trend'] = 0
            
        if 'volume_ratio' in df_features.columns:
            df_features['volume_spike'] = (df_features['volume_ratio'] > 2).astype(int)
        else:
            df_features['volume_spike'] = 0
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        # –ò—Å–∫–ª—é—á–∞–µ–º is_bullish, —Ç–∞–∫ –∫–∞–∫ –æ–Ω —É–∂–µ –µ—Å—Ç—å –≤ OHLC_FEATURES
        symbol_features = [col for col in df_features.columns if col.startswith('is_') and col != 'is_bullish']
        engineered_features = ['rsi_oversold', 'rsi_overbought', 'macd_bullish',
                              'bb_near_lower', 'bb_near_upper', 'strong_trend', 'volume_spike']
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        weighted_features = [
            'rsi_macd_interaction', 'volume_volatility_interaction', 'adx_trend_strength',
            'rsi_to_adx', 'volume_to_volatility', 'price_momentum_ratio',
            'momentum_composite', 'volatility_composite',
            'oversold_reversal_score', 'breakout_score', 'range_trading_score',
            'rsi_divergence'
        ]
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –≤–∑–≤–µ—à–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        weighted_features = [f for f in weighted_features if f in df_features.columns]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        rolling_features = [
            # RSI —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            'rsi_mean_10', 'rsi_std_10', 'rsi_mean_30', 'rsi_std_30', 'rsi_mean_60', 'rsi_std_60',
            # –û–±—ä–µ–º–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            'volume_mean_10', 'volume_spike_10', 'volume_mean_30', 'volume_spike_30', 
            'volume_mean_60', 'volume_spike_60',
            # –ú–æ–º–µ–Ω—Ç—É–º
            'momentum_10', 'momentum_accel_10', 'momentum_30', 'momentum_accel_30',
            'momentum_60', 'momentum_accel_60',
            # ATR
            'atr_ratio_10', 'atr_ratio_30', 'atr_ratio_60'
        ]
        rolling_features = [f for f in rolling_features if f in df_features.columns]
        
        # –î–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏
        divergence_features = [
            'rsi_bullish_divergence', 'rsi_bearish_divergence',
            'macd_price_divergence', 'volume_price_divergence'
        ]
        divergence_features = [f for f in divergence_features if f in df_features.columns]
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Å–≤–µ—á–µ–π
        candle_features = [
            'hammer_pattern', 'doji_pattern', 'bullish_engulfing', 'bearish_engulfing',
            'pin_bar_bullish', 'pin_bar_bearish'
        ]
        candle_features = [f for f in candle_features if f in df_features.columns]
        
        # Volume profile
        volume_profile_features = [
            'vwap_distance', 'volume_concentration', 'relative_volume_level',
            'accumulation_distribution', 'ad_oscillator'
        ]
        volume_profile_features = [f for f in volume_profile_features if f in df_features.columns]
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        all_features = (self.TECHNICAL_INDICATORS + self.MARKET_FEATURES + 
                       self.OHLC_FEATURES + symbol_features + engineered_features + 
                       weighted_features + rolling_features + divergence_features + 
                       candle_features + volume_profile_features)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ DataFrame
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        features_with_duplicates = [f for f in all_features if f in df_features.columns]
        self.feature_names = list(dict.fromkeys(features_with_duplicates))
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥—É–±–ª–∏–∫–∞—Ç–∞—Ö
        if len(features_with_duplicates) != len(self.feature_names):
            logger.info(f"   ‚úÖ –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(features_with_duplicates) - len(self.feature_names)}")
            duplicates = [f for f in features_with_duplicates if features_with_duplicates.count(f) > 1]
            unique_duplicates = list(set(duplicates))
            if unique_duplicates:
                logger.info(f"   üìã –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {unique_duplicates[:10]}")
        
        # –í–ê–ñ–ù–û: –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏ —É—Ç–µ—á–µ–∫ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
        dangerous_patterns = [
            'expected_return', 'profit_target', 'loss_target', 
            'max_profit', 'realized_profit', '_return_buy', '_return_sell'
        ]
        
        features_to_remove = []
        for feature in self.feature_names:
            for pattern in dangerous_patterns:
                if pattern in feature.lower():
                    features_to_remove.append(feature)
                    logger.error(f"üö® –û–ü–ê–°–ù–´–ô –ü–†–ò–ó–ù–ê–ö –û–ë–ù–ê–†–£–ñ–ï–ù: {feature} (—Å–æ–¥–µ—Ä–∂–∏—Ç '{pattern}')! –£–¥–∞–ª—è–µ–º...")
                    break
        
        # –£–¥–∞–ª—è–µ–º –æ–ø–∞—Å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        for feature in features_to_remove:
            if feature in self.feature_names:
                self.feature_names.remove(feature)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        missing_features = [f for f in all_features if f not in df_features.columns]
        if missing_features:
            logger.warning(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(missing_features)}): {missing_features[:10]}...")
        
        logger.info(f"\n‚úÖ –§–ò–ù–ê–õ–¨–ù–´–ô –ù–ê–ë–û–† –ü–†–ò–ó–ù–ê–ö–û–í:")
        logger.info(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {len(self.feature_names)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ {len(all_features)} –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö")
        
        # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        logger.info("\nüìã –°–ü–ò–°–û–ö –í–°–ï–• –ò–°–ü–û–õ–¨–ó–£–ï–ú–´–• –ü–†–ò–ó–ù–ê–ö–û–í:")
        tech_features = [f for f in self.TECHNICAL_INDICATORS if f in self.feature_names]
        market_features = [f for f in self.MARKET_FEATURES if f in self.feature_names]
        ohlc_features = [f for f in self.OHLC_FEATURES if f in self.feature_names]
        symbol_features_used = [f for f in symbol_features if f in self.feature_names]
        engineered_features_used = [f for f in engineered_features if f in self.feature_names]
        
        logger.info(f"\n   üìà –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã ({len(tech_features)}):")
        for i, f in enumerate(tech_features[:10]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
            logger.info(f"      {i+1}. {f}")
        if len(tech_features) > 10:
            logger.info(f"      ... –∏ –µ—â–µ {len(tech_features)-10}")
            
        logger.info(f"\n   üåç –†—ã–Ω–æ—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(market_features)}):")
        for f in market_features:
            logger.info(f"      - {f}")
            
        logger.info(f"\n   üìä OHLC –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(ohlc_features)}):")
        for f in ohlc_features:
            logger.info(f"      - {f}")
            
        logger.info(f"\n   üè∑Ô∏è Symbol –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(symbol_features_used)}):")
        for f in symbol_features_used[:5]:
            logger.info(f"      - {f}")
        if len(symbol_features_used) > 5:
            logger.info(f"      ... –∏ –µ—â–µ {len(symbol_features_used)-5}")
        
        # –õ–æ–≥–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –µ—Å–ª–∏ –µ—Å—Ç—å
        if weighted_features:
            logger.info(f"\n   ‚öñÔ∏è –í–∑–≤–µ—à–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(weighted_features)}):")
            for f in weighted_features[:5]:
                logger.info(f"      - {f}")
        
        if rolling_features:
            logger.info(f"\n   üìà –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ ({len(rolling_features)}):")
            for f in rolling_features[:5]:
                logger.info(f"      - {f}")
        
        if divergence_features:
            logger.info(f"\n   üîÑ –î–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ ({len(divergence_features)}):")
            for f in divergence_features:
                logger.info(f"      - {f}")
        
        if candle_features:
            logger.info(f"\n   üïØÔ∏è –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Å–≤–µ—á–µ–π ({len(candle_features)}):")
            for f in candle_features:
                logger.info(f"      - {f}")
        
        if volume_profile_features:
            logger.info(f"\n   üìâ Volume profile ({len(volume_profile_features)}):")
            for f in volume_profile_features:
                logger.info(f"      - {f}")
        
        logger.info(f"üìä –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {len(self.feature_names)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        X = df_features[self.feature_names].values
        
        logger.info(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {len(self.feature_names)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        logger.info(f"   - –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã: {len([f for f in self.TECHNICAL_INDICATORS if f in self.feature_names])}")
        logger.info(f"   - Market features: {len([f for f in self.MARKET_FEATURES if f in self.feature_names])}")
        logger.info(f"   - OHLC features: {len([f for f in self.OHLC_FEATURES if f in self.feature_names])}")
        logger.info(f"   - Symbol features: {len(symbol_features)}")
        logger.info(f"   - Engineered features: {len([f for f in engineered_features if f in self.feature_names])}")
        logger.info(f"   - Weighted features: {len(weighted_features)}")
        logger.info(f"   - Rolling statistics: {len(rolling_features)}")
        logger.info(f"   - Divergences: {len(divergence_features)}")
        logger.info(f"   - Candle patterns: {len(candle_features)}")
        logger.info(f"   - Volume profile: {len(volume_profile_features)}")
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π
        logger.info(f"üìè –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: X.shape={X.shape}, y_buy.shape={y_buy.shape}, y_sell.shape={y_sell.shape}")
        
        # –§–ò–ù–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê: —É–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –Ω–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏ —Ü–µ–ª–µ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
        logger.info("\nüîç –ü–†–û–í–ï–†–ö–ê –ù–ê –£–¢–ï–ß–ö–£ –î–ê–ù–ù–´–•:")
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –ø–µ—Ä–≤—ã—Ö 5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —Ü–µ–ª–µ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
        for i in range(min(5, X.shape[1])):
            corr_buy = np.corrcoef(X[:, i], y_buy)[0, 1]
            corr_sell = np.corrcoef(X[:, i], y_sell)[0, 1]
            if abs(corr_buy) > 0.9 or abs(corr_sell) > 0.9:
                logger.error(f"   üö® –í–´–°–û–ö–ê–Ø –ö–û–†–†–ï–õ–Ø–¶–ò–Ø! –ü—Ä–∏–∑–Ω–∞–∫ {self.feature_names[i]}: "
                           f"corr_buy={corr_buy:.3f}, corr_sell={corr_sell:.3f}")
            else:
                logger.info(f"   ‚úÖ –ü—Ä–∏–∑–Ω–∞–∫ {self.feature_names[i]}: "
                          f"corr_buy={corr_buy:.3f}, corr_sell={corr_sell:.3f}")
        
        return X, y_buy, y_sell
        
    def create_xgboost_model(self, task: str, num_classes: int = None) -> xgb.XGBModel:
        """–°–æ–∑–¥–∞–Ω–∏–µ XGBoost –º–æ–¥–µ–ª–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π"""
        base_params = {
            'n_estimators': 3000,
            'max_depth': 8,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ —Å–ª–æ–∂–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            'learning_rate': 0.01,  # –ú–∞–ª—ã–π learning rate –¥–ª—è —Ç–æ—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            'subsample': 0.8,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö
            'colsample_bytree': 0.8,  # –ë–æ–ª—å—à–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –¥–µ—Ä–µ–≤–æ
            'colsample_bylevel': 0.8,
            'gamma': 0.1,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –±–æ–ª—å—à–µ–π –≥–∏–±–∫–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
            'reg_alpha': 0.1,  # –ú–µ–Ω—å—à–µ L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
            'reg_lambda': 1.0,  # –£–º–µ—Ä–µ–Ω–Ω–∞—è L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
            'min_child_weight': 3,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ —Ä–µ–¥–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            'max_delta_step': 1,
            'random_state': 42,
            'n_jobs': -1,
            'verbosity': 0,
            'early_stopping_rounds': 100
        }
        
        # GPU –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        if USE_GPU:
            base_params['tree_method'] = 'gpu_hist'
            # predictor='gpu_predictor' —É—Å—Ç–∞—Ä–µ–ª –≤ –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏—è—Ö XGBoost
            
        if task == 'regression':
            model = xgb.XGBRegressor(
                **base_params,
                objective='reg:squarederror',
                eval_metric='rmse'
            )
        elif task == 'binary':
            model = xgb.XGBClassifier(
                **base_params,
                objective='binary:logistic',
                eval_metric='logloss'  # –ò–∑–º–µ–Ω–µ–Ω–æ —Å 'auc' –Ω–∞ 'logloss' –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            )
        else:  # multiclass
            model = xgb.XGBClassifier(
                **base_params,
                objective='multi:softprob',
                num_class=num_classes,
                eval_metric='mlogloss'
            )
            
        return model
        
    def optimize_threshold(self, y_true: np.ndarray, y_pred_proba: np.ndarray, method='gmean') -> float:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        logger.info(f"üéØ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ –º–µ—Ç–æ–¥–æ–º {method}...")
        
        if method == 'gmean':
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º G-mean –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ sensitivity –∏ specificity
            best_threshold = find_optimal_threshold_gmean(y_true, y_pred_proba)
        elif method == 'profit':
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–∏–±—ã–ª–∏
            best_threshold = find_optimal_threshold_profit(y_true, y_pred_proba, 
                                                         profit_per_tp=1.5,  # –ü—Ä–∏–±—ã–ª—å –æ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞
                                                         loss_per_fp=1.0)    # –ü–æ—Ç–µ—Ä–∏ –æ—Ç –ª–æ–∂–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞
        else:
            # –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥ —á–µ—Ä–µ–∑ F1
            thresholds = np.arange(0.1, 0.9, 0.01)
            best_f1 = 0
            best_threshold = 0.5
            best_precision = 0
            best_recall = 0
            
            for threshold in thresholds:
                y_pred = (y_pred_proba > threshold).astype(int)
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –Ω–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
                if y_pred.sum() == 0:
                    continue
                    
                precision = precision_score(y_true, y_pred, zero_division=0)
                recall = recall_score(y_true, y_pred, zero_division=0)
                f1 = f1_score(y_true, y_pred, zero_division=0)
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π –∫—Ä–∏—Ç–µ—Ä–∏–π –¥–ª—è —Ç—Ä–µ–π–¥–∏–Ω–≥–∞
                # Precision –≤–∞–∂–Ω–µ–µ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –ª–æ–∂–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
                weighted_score = 0.7 * precision + 0.3 * recall
                
                if f1 > best_f1 or (f1 >= best_f1 * 0.95 and weighted_score > best_precision * 0.7 + best_recall * 0.3):
                    best_f1 = f1
                    best_threshold = threshold
                    best_precision = precision
                    best_recall = recall
            
            logger.info(f"   –õ—É—á—à–∏–π –ø–æ—Ä–æ–≥: {best_threshold:.2f} (F1: {best_f1:.3f}, Precision: {best_precision:.3f}, Recall: {best_recall:.3f})")
        
        return best_threshold
        
    def plot_training_progress(self, eval_results: dict, model_name: str):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç—Ä–∏–∫—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
        train_metrics = list(eval_results['validation_0'].keys())
        val_metrics = list(eval_results['validation_1'].keys())
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–µ—Ç—Ä–∏–∫—É
        metric_name = train_metrics[0] if train_metrics else 'loss'
        
        # –ì—Ä–∞—Ñ–∏–∫ loss
        epochs = range(len(eval_results['validation_0'][metric_name]))
        axes[0].plot(epochs, eval_results['validation_0'][metric_name], 'b-', label='Train')
        axes[0].plot(epochs, eval_results['validation_1'][metric_name], 'r-', label='Validation')
        axes[0].set_xlabel('Iteration')
        axes[0].set_ylabel(metric_name.capitalize())
        axes[0].set_title(f'Training Progress: {model_name}')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # Feature importance
        if hasattr(self.models[model_name], 'feature_importances_'):
            importance = self.models[model_name].feature_importances_
            indices = np.argsort(importance)[-20:]
            
            axes[1].barh(range(20), importance[indices])
            axes[1].set_yticks(range(20))
            axes[1].set_yticklabels([self.feature_names[i] for i in indices])
            axes[1].set_xlabel('Feature Importance')
            axes[1].set_title('Top 20 Features')
            axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{log_dir}/plots/{model_name}_training_progress.png', dpi=150)
        plt.close()
        
    def plot_evaluation(self, y_true: np.ndarray, y_pred: np.ndarray, 
                       y_pred_proba: np.ndarray, model_name: str, task: str):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        if task == 'regression':
            self._plot_regression_results(y_true, y_pred, model_name)
        else:
            self._plot_classification_results(y_true, y_pred, y_pred_proba, model_name)
            
    def _plot_regression_results(self, y_true: np.ndarray, y_pred: np.ndarray, model_name: str):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'XGBoost Regression Results: {model_name}', fontsize=16)
        
        # Scatter plot
        axes[0, 0].scatter(y_true, y_pred, alpha=0.5, s=10)
        axes[0, 0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)
        axes[0, 0].set_xlabel('True Values')
        axes[0, 0].set_ylabel('Predictions')
        axes[0, 0].set_title('Predictions vs True Values')
        axes[0, 0].grid(True, alpha=0.3)
        
        # Residuals
        residuals = y_true - y_pred
        axes[0, 1].scatter(y_pred, residuals, alpha=0.5, s=10)
        axes[0, 1].axhline(y=0, color='r', linestyle='--')
        axes[0, 1].set_xlabel('Predictions')
        axes[0, 1].set_ylabel('Residuals')
        axes[0, 1].set_title('Residual Plot')
        axes[0, 1].grid(True, alpha=0.3)
        
        # Distribution
        axes[1, 0].hist(y_true, bins=50, alpha=0.5, label='True', density=True)
        axes[1, 0].hist(y_pred, bins=50, alpha=0.5, label='Predicted', density=True)
        axes[1, 0].set_xlabel('Expected Return (%)')
        axes[1, 0].set_ylabel('Density')
        axes[1, 0].set_title('Distribution Comparison')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # Metrics
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        r2 = r2_score(y_true, y_pred)
        
        axes[1, 1].axis('off')
        metrics_text = f"""
Performance Metrics:
  MAE:  {mae:.4f}%
  RMSE: {rmse:.4f}%
  R¬≤:   {r2:.4f}
  
  Mean True:  {np.mean(y_true):.3f}%
  Mean Pred:  {np.mean(y_pred):.3f}%
  Std True:   {np.std(y_true):.3f}%
  Std Pred:   {np.std(y_pred):.3f}%
        """
        axes[1, 1].text(0.1, 0.5, metrics_text, fontsize=12, family='monospace',
                       verticalalignment='center')
        
        plt.tight_layout()
        plt.savefig(f'{log_dir}/plots/{model_name}_evaluation.png', dpi=150)
        plt.close()
        
    def _plot_classification_results(self, y_true: np.ndarray, y_pred: np.ndarray,
                                   y_pred_proba: np.ndarray, model_name: str):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'XGBoost Classification Results: {model_name}', fontsize=16)
        
        # Confusion Matrix
        cm = confusion_matrix(y_true, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])
        axes[0, 0].set_xlabel('Predicted')
        axes[0, 0].set_ylabel('Actual')
        axes[0, 0].set_title('Confusion Matrix')
        
        # ROC Curve
        fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
        auc = roc_auc_score(y_true, y_pred_proba)
        axes[0, 1].plot(fpr, tpr, 'b-', lw=2, label=f'ROC (AUC = {auc:.3f})')
        axes[0, 1].plot([0, 1], [0, 1], 'r--', lw=2)
        axes[0, 1].set_xlabel('False Positive Rate')
        axes[0, 1].set_ylabel('True Positive Rate')
        axes[0, 1].set_title('ROC Curve')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # Probability Distribution
        axes[1, 0].hist(y_pred_proba[y_true == 0], bins=30, alpha=0.5, 
                       label='Class 0', density=True)
        axes[1, 0].hist(y_pred_proba[y_true == 1], bins=30, alpha=0.5,
                       label='Class 1', density=True)
        axes[1, 0].set_xlabel('Predicted Probability')
        axes[1, 0].set_ylabel('Density')
        axes[1, 0].set_title('Probability Distribution')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # Metrics
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, zero_division=0)
        recall = recall_score(y_true, y_pred, zero_division=0)
        f1 = f1_score(y_true, y_pred, zero_division=0)
        
        axes[1, 1].axis('off')
        metrics_text = f"""
Performance Metrics:
  Accuracy:  {accuracy:.2%}
  Precision: {precision:.2%}
  Recall:    {recall:.2%}
  F1-Score:  {f1:.3f}
  ROC-AUC:   {auc:.3f}
  
Confusion Matrix:
  TN: {cm[0,0]:,}  FP: {cm[0,1]:,}
  FN: {cm[1,0]:,}  TP: {cm[1,1]:,}
        """
        axes[1, 1].text(0.1, 0.5, metrics_text, fontsize=12, family='monospace',
                       verticalalignment='center')
        
        plt.tight_layout()
        plt.savefig(f'{log_dir}/plots/{model_name}_evaluation.png', dpi=150)
        plt.close()
        
    def train_ensemble(self, task: str = 'classification_binary', 
                      ensemble_size: int = 1, test_mode: bool = False,
                      use_cache: bool = False, force_reload: bool = False,
                      no_smote: bool = False, classification_threshold: float = 0.5,
                      balance_method: str = 'smote'):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º balance_method –∫–∞–∫ –∞—Ç—Ä–∏–±—É—Ç –∫–ª–∞—Å—Å–∞
        self.balance_method = balance_method if not no_smote else 'none'
        
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ Enhanced XGBoost v2.0")
        logger.info(f"üìä –†–µ–∂–∏–º: {task}")
        logger.info(f"üéØ –†–∞–∑–º–µ—Ä –∞–Ω—Å–∞–º–±–ª—è: {ensemble_size}")
        if task == 'classification_binary':
            logger.info(f"üîÑ –ú–µ—Ç–æ–¥ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: {self.balance_method}")
            logger.info(f"üéØ –ü–æ—Ä–æ–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {classification_threshold}%")
        if test_mode:
            logger.info("‚ö° –¢–ï–°–¢–û–í–´–ô –†–ï–ñ–ò–ú: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
        if use_cache:
            logger.info("üíæ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫—ç—à–∞: –í–ö–õ")
            if force_reload:
                logger.info("üîÑ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—ç—à–∞")
                
        start_time = time.time()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤
        if use_cache:
            self.cache_manager = CacheManager()
        self.visualizer = AdvancedVisualizer(log_dir)
        
        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
        conn = self.connect_db()
        
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            logger.info("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL...")
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤
            query = "SELECT DISTINCT symbol FROM processed_market_data ORDER BY symbol"
            symbols_df = pd.read_sql_query(query, conn)
            all_symbols = symbols_df['symbol'].tolist()
            
            if test_mode:
                # –í —Ç–µ—Å—Ç–æ–≤–æ–º —Ä–µ–∂–∏–º–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ 2 —Å–∏–º–≤–æ–ª–∞
                symbols_to_load = ['BTCUSDT', 'ETHUSDT']
                logger.info(f"‚ö° –¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º: –∑–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ {symbols_to_load}")
            else:
                symbols_to_load = all_symbols
                
            logger.info(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(symbols_to_load)} —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
            
            # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –∫—ç—à–∞
            df = None
            cache_key = None
            
            if use_cache and self.cache_manager:
                cache_key = self.cache_manager.get_cache_key({
                    'symbols_count': len(symbols_to_load),
                    'task': task,
                    'date_range': 'full'  # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç
                })
                
                if not force_reload:
                    df = self.cache_manager.load(cache_key, 'raw_data')
            
            if df is None:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                df = self.load_data_parallel(symbols_to_load, conn, batch_size=10)
                
                # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –ø–∞–º—è—Ç—å
                df = self.optimize_memory(df)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
                if use_cache and self.cache_manager and cache_key:
                    self.cache_manager.save(df, cache_key, 'raw_data')
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º market data
            logger.info("üìä –ó–∞–≥—Ä—É–∑–∫–∞ —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π...")
            market_data = self.load_market_data(conn)
            
            # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –∫—ç—à–∞
            features_cache = None
            if use_cache and self.cache_manager and not force_reload:
                features_cache = self.cache_manager.load(cache_key, 'prepared_features')
            
            if features_cache is not None:
                # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –∏–∑ –∫—ç—à–∞
                df_features = features_cache['df_features']
                X = features_cache['X']
                y_buy = features_cache['y_buy']
                y_sell = features_cache['y_sell']
                self.feature_names = features_cache['feature_names']
            else:
                # –†–∞—Å—á–µ—Ç market features
                df = self.calculate_market_features(df, market_data)
                
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                logger.info("üîÑ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
                df = self.add_rolling_statistics(df)
                df = self.add_divergences(df)
                df = self.add_candle_patterns(df)
                df = self.add_volume_profile(df)
                
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                logger.info("üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...")
                X, y_buy, y_sell = self.prepare_features(df)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
                if use_cache and self.cache_manager and cache_key:
                    features_cache = {
                        'df_features': df,
                        'X': X,
                        'y_buy': y_buy,
                        'y_sell': y_sell,
                        'feature_names': self.feature_names
                    }
                    self.cache_manager.save(features_cache, cache_key, 'prepared_features')
                    
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
            if hasattr(df, 'columns'):
                self.visualizer.plot_data_overview(df)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–∏—Å–∫–ª—é—á–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)
            logger.info("üîÑ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            binary_features = [
                'rsi_oversold', 'rsi_overbought', 'macd_bullish',
                'bb_near_lower', 'bb_near_upper', 'strong_trend', 'volume_spike',
                'is_bullish', 'is_weekend', 'is_major', 'is_meme', 'is_defi', 'is_alt',
                'market_regime_low_vol', 'market_regime_med_vol', 'market_regime_high_vol'
            ]
            # –î–æ–±–∞–≤–ª—è–µ–º one-hot –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
            binary_features.extend([f for f in self.feature_names if f.startswith('is_')])
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å–≤–µ—á–µ–π –∏ –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏
            binary_features.extend([
                'is_hammer', 'is_doji', 'bullish_engulfing', 'bearish_engulfing',
                'pin_bar', 'rsi_bullish_divergence', 'rsi_bearish_divergence',
                'macd_bullish_divergence', 'macd_bearish_divergence', 'volume_price_divergence'
            ])
            
            # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            binary_indices = []
            continuous_indices = []
            for i, feature in enumerate(self.feature_names):
                if feature in binary_features:
                    binary_indices.append(i)
                else:
                    continuous_indices.append(i)
            
            logger.info(f"   üìä –ë–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(binary_indices)}")
            logger.info(f"   üìä –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(continuous_indices)}")
            
            # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é X –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
            X_scaled = X.copy()
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            if continuous_indices:
                scaler = RobustScaler()
                X_scaled[:, continuous_indices] = scaler.fit_transform(X[:, continuous_indices])
                self.scalers['features'] = scaler
                self.scalers['binary_indices'] = binary_indices
                self.scalers['continuous_indices'] = continuous_indices
            else:
                logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏")
            
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if self.feature_names:
                self.visualizer.plot_feature_distributions(X_scaled, self.feature_names)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if task in ['classification_binary', 'classification_multiclass']:
                logger.info("üîÑ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏...")
                if task == 'classification_binary':
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                    y_buy_class = (y_buy > classification_threshold).astype(int)
                    y_sell_class = (y_sell > classification_threshold).astype(int)
                    
                    logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∏–Ω–∞—Ä–Ω—ã—Ö –º–µ—Ç–æ–∫ (–ø–æ—Ä–æ–≥ > {classification_threshold}%):")
                    logger.info(f"   Buy - –ö–ª–∞—Å—Å 1 (–≤—Ö–æ–¥–∏—Ç—å): {np.mean(y_buy_class):.1%}")
                    logger.info(f"   Sell - –ö–ª–∞—Å—Å 1 (–≤—Ö–æ–¥–∏—Ç—å): {np.mean(y_sell_class):.1%}")
                    
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                    logger.info(f"\nüìà –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ expected_return:")
                    logger.info(f"   Buy > 0%: {(y_buy > 0).mean():.1%}, Buy > 0.5%: {(y_buy > 0.5).mean():.1%}, Buy > 1%: {(y_buy > 1.0).mean():.1%}")
                    logger.info(f"   Sell > 0%: {(y_sell > 0).mean():.1%}, Sell > 0.5%: {(y_sell > 0.5).mean():.1%}, Sell > 1%: {(y_sell > 1.0).mean():.1%}")
                else:
                    # Multiclass: 4 –∫–ª–∞—Å—Å–∞ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–æ—á–µ–∫ –≤—Ö–æ–¥–∞
                    # –ö–ª–∞—Å—Å 0: –ù–µ –≤—Ö–æ–¥–∏—Ç—å (< 0.5%)
                    # –ö–ª–∞—Å—Å 1: –ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ (0.5% - 1.5%)
                    # –ö–ª–∞—Å—Å 2: –•–æ—Ä–æ—à–∞—è —Ç–æ—á–∫–∞ (1.5% - 3%)
                    # –ö–ª–∞—Å—Å 3: –û—Ç–ª–∏—á–Ω–∞—è —Ç–æ—á–∫–∞ (> 3%)
                    bins = [-np.inf, 0.5, 1.5, 3.0, np.inf]
                    y_buy_class = pd.cut(y_buy, bins=bins, labels=[0, 1, 2, 3]).astype(int)
                    y_sell_class = pd.cut(y_sell, bins=bins, labels=[0, 1, 2, 3]).astype(int)
                    
                    logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤—ã—Ö –º–µ—Ç–æ–∫:")
                    for direction, y_class in [('Buy', y_buy_class), ('Sell', y_sell_class)]:
                        logger.info(f"   {direction}:")
                        for i in range(4):
                            pct = (y_class == i).mean() * 100
                            logger.info(f"     –ö–ª–∞—Å—Å {i}: {pct:.1f}%")
                    
                # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
                test_size = 0.15
                val_size = 0.15
                
                n = len(X_scaled)
                train_end = int(n * (1 - test_size - val_size))
                val_end = int(n * (1 - test_size))
                
                X_train = X_scaled[:train_end]
                X_val = X_scaled[train_end:val_end]
                X_test = X_scaled[val_end:]
                
                model_configs = [
                    ('buy', y_buy_class if task.startswith('classification') else y_buy),
                    ('sell', y_sell_class if task.startswith('classification') else y_sell)
                ]
            else:
                # –†–µ–≥—Ä–µ—Å—Å–∏—è
                test_size = 0.15
                val_size = 0.15
                
                n = len(X_scaled)
                train_end = int(n * (1 - test_size - val_size))
                val_end = int(n * (1 - test_size))
                
                X_train = X_scaled[:train_end]
                X_val = X_scaled[train_end:val_end]
                X_test = X_scaled[val_end:]
                
                model_configs = [
                    ('buy_return_predictor', y_buy),
                    ('sell_return_predictor', y_sell)
                ]
                
            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
            results = {}
            metrics_history = {}  # –î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            
            for direction, y_values in model_configs:
                logger.info(f"\n{'='*60}")
                logger.info(f"–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è: {direction}")
                logger.info(f"{'='*60}")
                
                y_train = y_values[:train_end]
                y_val = y_values[train_end:val_end]
                y_test = y_values[val_end:]
                
                ensemble_predictions = []
                ensemble_models = []
                ensemble_weights = []  # –í–µ—Å–∞ –¥–ª—è –≤–∑–≤–µ—à–µ–Ω–Ω–æ–≥–æ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è
                
                # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–µ—Ä–µ–¥ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π
                if self.feature_names is not None:
                    binary_features = ['macd_bullish', 'rsi_oversold', 'rsi_overbought', 
                                     'strong_trend', 'volume_spike', 'is_bullish']
                    
                    for feat in binary_features:
                        if feat in self.feature_names:
                            feat_idx = self.feature_names.index(feat)
                            # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ X_train –∏ X_val (–ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º -1 –≤ 0)
                            X_train[:, feat_idx] = np.where(X_train[:, feat_idx] > 0, 1, 0)
                            X_val[:, feat_idx] = np.where(X_val[:, feat_idx] > 0, 1, 0)
                            
                            # –õ–æ–≥–∏—Ä—É–µ–º –µ—Å–ª–∏ –±—ã–ª–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
                            unique_train = np.unique(X_train[:, feat_idx])
                            if len(unique_train) != 2 or not np.array_equal(unique_train, [0, 1]):
                                logger.warning(f"‚ö†Ô∏è –ò—Å–ø—Ä–∞–≤–ª–µ–Ω –ø—Ä–∏–∑–Ω–∞–∫ {feat}: unique values = {unique_train}")
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É –∫–ª–∞—Å—Å–æ–≤ (—Ç–æ–ª—å–∫–æ –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)
                if task == 'classification_binary' and direction != 'regression' and not no_smote:
                    balance_method = getattr(self, 'balance_method', 'smote')  # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–æ–¥ –∏–∑ –∞—Ç—Ä–∏–±—É—Ç–∞
                    
                    if balance_method == 'random':
                        logger.info("\nüîÑ –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é RandomOverSampler...")
                        X_train_balanced, y_train_balanced = apply_random_oversampler(
                            X_train, y_train, sampling_strategy=0.5
                        )
                    elif balance_method == 'smote':
                        logger.info("\nüîÑ –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é SMOTE...")
                        X_train_balanced, y_train_balanced = apply_smote(
                            X_train, y_train, sampling_strategy=0.5
                        )
                        
                        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –ø–æ—Å–ª–µ SMOTE
                        if self.feature_names is not None:
                            logger.info("‚úÇÔ∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤...")
                            X_train_balanced = clip_technical_indicators(X_train_balanced, self.feature_names)
                            
                            # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ—Å–ª–µ SMOTE –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
                            logger.info("üîÑ –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ SMOTE...")
                            X_train_balanced = recreate_binary_features(X_train_balanced, self.feature_names, direction)
                    else:  # none
                        logger.info("\n‚ö†Ô∏è –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
                        X_train_balanced = X_train
                        y_train_balanced = y_train
                elif no_smote and task == 'classification_binary':
                    logger.info("\n‚ö†Ô∏è –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
                    X_train_balanced = X_train
                    y_train_balanced = y_train
                else:
                    X_train_balanced = X_train
                    y_train_balanced = y_train
                
                # –í–∞–ª–∏–¥–∞—Ü–∏—è –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
                if task == 'classification_binary' and self.feature_names is not None:
                    binary_features = ['macd_bullish', 'rsi_oversold', 'rsi_overbought', 
                                     'strong_trend', 'volume_spike', 'is_bullish']
                    
                    existing_binary_features = [f for f in binary_features if f in self.feature_names]
                    if existing_binary_features:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                        df_check = pd.DataFrame(X_train_balanced[:1000], columns=self.feature_names)
                        validate_binary_features(df_check, existing_binary_features)
                    else:
                        logger.warning("‚ö†Ô∏è –ë–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ feature_names")
                
                # Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –ø–µ—Ä–≤–æ–π –º–æ–¥–µ–ª–∏ –∞–Ω—Å–∞–º–±–ª—è
                if ensemble_size > 0:
                    logger.info("\nüîß –ó–∞–ø—É—Å–∫ Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...")
                    best_params = self.optimize_hyperparameters(
                        X_train_balanced, y_train_balanced, X_val, y_val, task, direction
                    )
                    logger.info(f"‚úÖ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞–π–¥–µ–Ω—ã: {best_params}")
                else:
                    best_params = None
                
                for i in range(ensemble_size):
                    model_name = f"{direction}_xgboost_v2_{i}"
                    logger.info(f"\nüöÄ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ {i+1}/{ensemble_size}: {model_name}")
                    
                    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
                    if task == 'regression':
                        model = self.create_xgboost_model('regression')
                    elif task == 'classification_binary':
                        model = self.create_xgboost_model('binary')
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç scale_pos_weight
                        scale_pos_weight = calculate_scale_pos_weight(y_train)
                        model.set_params(scale_pos_weight=scale_pos_weight)
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º max_delta_step –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                        model.set_params(max_delta_step=1)
                    else:
                        num_classes = len(np.unique(y_train))
                        model = self.create_xgboost_model('multiclass', num_classes)
                        
                        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–µ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
                        from sklearn.utils.class_weight import compute_class_weight
                        class_weights = compute_class_weight(
                            'balanced',
                            classes=np.unique(y_train),
                            y=y_train
                        )
                        sample_weights = np.ones(len(y_train))
                        for i, cls in enumerate(np.unique(y_train)):
                            sample_weights[y_train == cls] = class_weights[i]
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–ª—É—á–∞–π–Ω—ã–µ —Å–µ–º–µ–Ω–∞ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≤ –∞–Ω—Å–∞–º–±–ª–µ
                    model.set_params(random_state=42 + i * 100)
                    
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ Optuna
                    if best_params and i == 0:  # –¢–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤–æ–π –º–æ–¥–µ–ª–∏
                        try:
                            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–∏–º–µ–Ω—è–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                            logger.info(f"üìù –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã Optuna: {best_params}")
                            model.set_params(**best_params)
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ Optuna: {e}")
                            logger.warning("–ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏")
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º
                    logger.info("üìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º:")
                    important_params = ['tree_method', 'objective', 'eval_metric', 'n_estimators', 
                                      'max_depth', 'learning_rate', 'scale_pos_weight', 'subsample',
                                      'colsample_bytree', 'gamma', 'min_child_weight']
                    for param in important_params:
                        if hasattr(model, param):
                            value = getattr(model, param)
                            logger.info(f"   {param}: {value}")
                    
                    # –û–±—É—á–µ–Ω–∏–µ —Å early stopping
                    eval_set = [(X_train_balanced, y_train_balanced), (X_val, y_val)]
                    
                    if test_mode:
                        model.set_params(n_estimators=100)  # –ú–µ–Ω—å—à–µ –¥–µ—Ä–µ–≤—å–µ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞
                        
                    # –î–ª—è –Ω–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏ XGBoost –∏—Å–ø–æ–ª—å–∑—É–µ–º callbacks
                    if task == 'classification_multiclass' and 'sample_weights' in locals():
                        # –î–ª—è multiclass –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Å–∞
                        model.fit(
                            X_train_balanced, y_train_balanced,
                            sample_weight=sample_weights,
                            eval_set=eval_set,
                            verbose=True
                        )
                    else:
                        model.fit(
                            X_train_balanced, y_train_balanced,
                            eval_set=eval_set,
                            verbose=True
                        )
                    
                    self.models[model_name] = model
                    ensemble_models.append(model)
                    
                    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–µ
                    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–ª—è –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è
                    if task == 'regression':
                        y_val_pred = model.predict(X_val)
                        val_score = r2_score(y_val, y_val_pred)
                        y_pred = model.predict(X_test)
                        ensemble_predictions.append(y_pred)
                    else:
                        y_val_pred_proba = model.predict_proba(X_val)
                        if task == 'classification_binary':
                            val_score = roc_auc_score(y_val, y_val_pred_proba[:, 1])
                            y_pred_proba = model.predict_proba(X_test)[:, 1]
                            ensemble_predictions.append(y_pred_proba)
                        else:
                            val_score = accuracy_score(y_val, y_val_pred_proba.argmax(axis=1))
                            y_pred_proba = model.predict_proba(X_test)
                            ensemble_predictions.append(y_pred_proba)
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                    ensemble_weights.append(val_score)
                    logger.info(f"   –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π —Å–∫–æ—Ä –º–æ–¥–µ–ª–∏: {val_score:.4f}")
                    
                    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è
                    if hasattr(model, 'evals_result'):
                        self.plot_training_progress(model.evals_result(), model_name)
                        
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
                if ensemble_weights:
                    weights = np.array(ensemble_weights)
                    weights = weights / weights.sum()
                    logger.info(f"\nüìä –í–µ—Å–∞ –º–æ–¥–µ–ª–µ–π –≤ –∞–Ω—Å–∞–º–±–ª–µ: {weights}")
                else:
                    weights = None
                
                # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∞–Ω—Å–∞–º–±–ª—è
                if task == 'regression':
                    y_pred_ensemble = ensemble_predictions_weighted(ensemble_predictions, weights, method='soft')
                    
                    # –ú–µ—Ç—Ä–∏–∫–∏
                    mae = mean_absolute_error(y_test, y_pred_ensemble)
                    rmse = np.sqrt(mean_squared_error(y_test, y_pred_ensemble))
                    r2 = r2_score(y_test, y_pred_ensemble)
                    
                    logger.info(f"\nüìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω—Å–∞–º–±–ª—è {direction}:")
                    logger.info(f"   MAE: {mae:.4f}%")
                    logger.info(f"   RMSE: {rmse:.4f}%")
                    logger.info(f"   R¬≤: {r2:.4f}")
                    
                    self.plot_evaluation(y_test, y_pred_ensemble, None, f"{direction}_ensemble", 'regression')
                    
                elif task == 'classification_binary':
                    # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ
                    y_pred_proba_ensemble = ensemble_predictions_weighted(ensemble_predictions, weights, method='soft')
                    
                    # –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
                    logger.info("\nüìä –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π...")
                    y_pred_proba_calibrated, calibrator = calibrate_probabilities(
                        ensemble_models[0], X_train_balanced, y_train_balanced, X_test, method='isotonic'
                    )
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –µ—Å–ª–∏ –æ–Ω–∏ –ª—É—á—à–µ
                    if roc_auc_score(y_test, y_pred_proba_calibrated) > roc_auc_score(y_test, y_pred_proba_ensemble):
                        logger.info("   ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏")
                        y_pred_proba_ensemble = y_pred_proba_calibrated
                    
                    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞
                    logger.info(f"üéØ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ –¥–ª—è {direction}...")
                    best_threshold = self.optimize_threshold(y_test, y_pred_proba_ensemble, method='gmean')
                    logger.info(f"   –õ—É—á—à–∏–π –ø–æ—Ä–æ–≥: {best_threshold:.2f}")
                    
                    y_pred_ensemble = (y_pred_proba_ensemble > best_threshold).astype(int)
                    
                    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                    accuracy = accuracy_score(y_test, y_pred_ensemble)
                    precision = precision_score(y_test, y_pred_ensemble, zero_division=0)
                    recall = recall_score(y_test, y_pred_ensemble, zero_division=0)
                    f1 = f1_score(y_test, y_pred_ensemble, zero_division=0)
                    auc = roc_auc_score(y_test, y_pred_proba_ensemble)
                    
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                    pr_auc = average_precision_score(y_test, y_pred_proba_ensemble)
                    mcc = matthews_corrcoef(y_test, y_pred_ensemble)
                    
                    # G-mean
                    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_ensemble).ravel()
                    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
                    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
                    gmean = np.sqrt(sensitivity * specificity)
                    
                    logger.info(f"\nüìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω—Å–∞–º–±–ª—è {direction}:")
                    logger.info(f"   Accuracy: {accuracy:.2%}")
                    logger.info(f"   Precision: {precision:.2%}")
                    logger.info(f"   Recall: {recall:.2%}")
                    logger.info(f"   F1-Score: {f1:.3f}")
                    logger.info(f"   ROC-AUC: {auc:.3f}")
                    logger.info(f"   PR-AUC: {pr_auc:.3f}")
                    logger.info(f"   MCC: {mcc:.3f}")
                    logger.info(f"   G-mean: {gmean:.3f}")
                    logger.info(f"   Sensitivity: {sensitivity:.3f}")
                    logger.info(f"   Specificity: {specificity:.3f}")
                    
                    self.plot_evaluation(y_test, y_pred_ensemble, y_pred_proba_ensemble,
                                       f"{direction}_ensemble", 'classification')
                    
                    # –ê–Ω–∞–ª–∏–∑ –≤—ã–∏–≥—Ä—ã—à–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
                    self.analyze_winning_patterns(ensemble_models[0], X_test, y_test, 
                                                y_pred_ensemble, y_pred_proba_ensemble, 
                                                f"{direction}_ensemble")
                    
                    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤
                    X_test_df = pd.DataFrame(X_test, columns=self.feature_names)
                    filtered_signals = self.filter_trading_signals(y_pred_ensemble, y_pred_proba_ensemble, 
                                                                  X_test_df, min_confidence=0.6)
                    
                    # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                    filtered_accuracy = accuracy_score(y_test[filtered_signals > -1], 
                                                     filtered_signals[filtered_signals > -1])
                    logger.info(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏:")
                    logger.info(f"   Accuracy: {filtered_accuracy:.2%}")
                    
                    results[direction] = {
                        'threshold': best_threshold,
                        'metrics': {
                            'accuracy': accuracy,
                            'precision': precision,
                            'recall': recall,
                            'f1': f1,
                            'auc': auc
                        }
                    }
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
                    metrics_history[f"{direction}_ensemble"] = {
                        'accuracy': accuracy,
                        'precision': precision,
                        'recall': recall,
                        'f1': f1
                    }
                    
                else:  # multiclass
                    # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è multiclass
                    y_pred_proba_ensemble = np.mean(ensemble_predictions, axis=0)
                    y_pred_ensemble = y_pred_proba_ensemble.argmax(axis=1)
                    
                    # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è multiclass
                    accuracy = accuracy_score(y_test, y_pred_ensemble)
                    
                    # Classification report –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                    from sklearn.metrics import classification_report
                    class_report = classification_report(y_test, y_pred_ensemble, output_dict=True)
                    
                    logger.info(f"\nüìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω—Å–∞–º–±–ª—è {direction} (multiclass):")
                    logger.info(f"   Accuracy: {accuracy:.2%}")
                    logger.info(f"\nüìä –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º:")
                    
                    class_names = ['–ù–µ –≤—Ö–æ–¥–∏—Ç—å', '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ', '–•–æ—Ä–æ—à–æ', '–û—Ç–ª–∏—á–Ω–æ']
                    for i in range(len(class_names)):
                        if str(i) in class_report:
                            metrics = class_report[str(i)]
                            logger.info(f"   –ö–ª–∞—Å—Å {i} ({class_names[i]}):")
                            logger.info(f"     Precision: {metrics['precision']:.2%}")
                            logger.info(f"     Recall: {metrics['recall']:.2%}")
                            logger.info(f"     F1-Score: {metrics['f1-score']:.3f}")
                            logger.info(f"     Support: {metrics['support']}")
                    
                    # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤ —Å —Ö–æ—Ä–æ—à–∏–º–∏ —Å–∏–≥–Ω–∞–ª–∞–º–∏ (2 –∏ 3)
                    good_signals = (y_pred_ensemble >= 2)
                    if good_signals.sum() > 0:
                        self.analyze_winning_patterns(ensemble_models[0], X_test[good_signals], 
                                                    y_test[good_signals], 
                                                    y_pred_ensemble[good_signals], 
                                                    y_pred_proba_ensemble[good_signals], 
                                                    f"{direction}_ensemble")
                    
                    results[direction] = {
                        'metrics': {
                            'accuracy': accuracy,
                            'class_report': class_report
                        }
                    }
                    
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
            logger.info("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
            for name, model in self.models.items():
                joblib.dump(model, f'trained_model/{name}.pkl')
                logger.info(f"   ‚úÖ {name}.pkl")
                
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler
            joblib.dump(self.scalers['features'], 'trained_model/scaler_xgboost_v2.pkl')
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                'model_version': '2.1',
                'type': 'xgboost_enhanced_balanced',
                'task_type': task,
                'ensemble_size': ensemble_size,
                'total_features': len(self.feature_names),
                'feature_names': self.feature_names,
                'training_time': time.time() - start_time,
                'test_mode': test_mode,
                'results': results if task.startswith('classification') else {},
                'improvements': [
                    'SMOTE –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤',
                    '–í–∑–≤–µ—à–µ–Ω–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –≤ –∞–Ω—Å–∞–º–±–ª–µ',
                    '–ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π',
                    'G-mean –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞',
                    '–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (PR-AUC, MCC, G-mean)'
                ]
            }
            
            with open('trained_model/metadata_xgboost_v2.json', 'w') as f:
                json.dump(metadata, f, indent=2)
                
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            try:
                feature_config = {
                    'technical_indicators': self.TECHNICAL_INDICATORS,
                    'market_features': self.MARKET_FEATURES,
                    'ohlc_features': self.OHLC_FEATURES,
                    'total_features': len(self.feature_names)
                }
                
                with open('trained_model/feature_config_xgboost_v2.json', 'w') as f:
                    json.dump(feature_config, f, indent=2)
                logger.info("   ‚úÖ feature_config_xgboost_v2.json")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ feature_config: {e}")
                logger.error(f"   –¢–∏–ø –æ—à–∏–±–∫–∏: {type(e).__name__}")
                import traceback
                logger.error(f"   Traceback:\n{traceback.format_exc()}")
                
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
            if metrics_history:
                try:
                    self.visualizer.plot_training_comparison(metrics_history)
                    logger.info("   ‚úÖ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
                    import traceback
                    logger.error(f"   Traceback:\n{traceback.format_exc()}")
            
            # –ê–Ω–∞–ª–∏–∑ feature importance –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
            try:
                self._analyze_feature_importance()
                logger.info("   ‚úÖ –ê–Ω–∞–ª–∏–∑ feature importance –∑–∞–≤–µ—Ä—à–µ–Ω")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ feature importance: {e}")
                import traceback
                logger.error(f"   Traceback:\n{traceback.format_exc()}")
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
            try:
                self._create_final_report(metadata, results if task.startswith('classification') else {})
                logger.info("   ‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞: {e}")
                import traceback
                logger.error(f"   Traceback:\n{traceback.format_exc()}")
            
            total_time = time.time() - start_time
            logger.info(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {total_time/60:.1f} –º–∏–Ω—É—Ç")
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–ø–∏—Å—å –ª–æ–≥–æ–≤
            for handler in logger.handlers:
                handler.flush()
            
        finally:
            conn.close()
    
    def _analyze_feature_importance(self):
        """–ê–Ω–∞–ª–∏–∑ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è feature importance –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        logger.info("\nüìä –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        if not self.models:
            logger.warning("–ù–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            return
        
        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º importance –ø–æ –≤—Å–µ–º –º–æ–¥–µ–ª—è–º
        aggregated_importance = {}
        
        for model_name, model in self.models.items():
            if hasattr(model, 'feature_importances_'):
                importance = model.feature_importances_
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π
                if len(importance) != len(self.feature_names):
                    logger.error(f"–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π –¥–ª—è {model_name}: "
                               f"importance={len(importance)}, features={len(self.feature_names)}")
                    continue
                
                # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º
                for i, feature in enumerate(self.feature_names):
                    if feature not in aggregated_importance:
                        aggregated_importance[feature] = []
                    aggregated_importance[feature].append(importance[i])
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ importance
        mean_importance = {}
        for feature, values in aggregated_importance.items():
            mean_importance[feature] = np.mean(values)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        sorted_features = sorted(mean_importance.items(), key=lambda x: x[1], reverse=True)
        
        # –í—ã–≤–æ–¥–∏–º —Ç–æ–ø-20
        logger.info("\nüèÜ –¢–û–ü-20 –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í (—É—Å—Ä–µ–¥–Ω–µ–Ω–æ –ø–æ –≤—Å–µ–º –º–æ–¥–µ–ª—è–º):")
        for i, (feature, importance) in enumerate(sorted_features[:20]):
            logger.info(f"{i+1:2d}. {feature:40s} {importance:.4f}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        logger.info("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:")
        suspicious_features = ['expected_return', 'buy_expected', 'sell_expected', 'target', 'label']
        for feature, importance in sorted_features:
            for suspicious in suspicious_features:
                if suspicious in feature.lower():
                    logger.error(f"üö® –ü–û–î–û–ó–†–ò–¢–ï–õ–¨–ù–´–ô –ü–†–ò–ó–ù–ê–ö: {feature} (importance={importance:.4f})")
                    logger.error("   –í–æ–∑–º–æ–∂–Ω–∞ —É—Ç–µ—á–∫–∞ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π!")
        
        # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
        self._plot_feature_importance(sorted_features[:30])
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª (–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º float32 –≤ float –¥–ª—è JSON)
        with open(f'{log_dir}/feature_importance.json', 'w') as f:
            json.dump({k: float(v) for k, v in sorted_features}, f, indent=2)
    
    def _plot_feature_importance(self, sorted_features):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ feature importance"""
        features = [f[0] for f in sorted_features]
        importances = [f[1] for f in sorted_features]
        
        plt.figure(figsize=(12, 10))
        plt.barh(range(len(features)), importances)
        plt.yticks(range(len(features)), features)
        plt.xlabel('Feature Importance')
        plt.title('Top 30 Feature Importances (Averaged across all models)')
        plt.tight_layout()
        plt.savefig(f'{log_dir}/plots/feature_importance.png', dpi=150)
        plt.close()
        
        logger.info(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {log_dir}/plots/feature_importance.png")
            
    def _create_final_report(self, metadata: dict, results: dict):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ —Å feature importance"""
        report = []
        report.append("="*80)
        report.append("–§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ - Enhanced XGBoost v2.0")
        report.append("="*80)
        report.append(f"–î–∞—Ç–∞ –æ–±—É—á–µ–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"–¢–∏–ø –∑–∞–¥–∞—á–∏: {metadata['task_type']}")
        report.append(f"–†–∞–∑–º–µ—Ä –∞–Ω—Å–∞–º–±–ª—è: {metadata['ensemble_size']}")
        report.append(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {metadata['total_features']}")
        report.append(f"–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {metadata['training_time']/60:.1f} –º–∏–Ω—É—Ç")
        
        if results:
            report.append("\n–†–ï–ó–£–õ–¨–¢–ê–¢–´:")
            for direction, data in results.items():
                report.append(f"\n{direction.upper()}:")
                report.append(f"  –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {data['threshold']:.2f}")
                metrics = data['metrics']
                report.append(f"  Accuracy: {metrics['accuracy']:.2%}")
                report.append(f"  Precision: {metrics['precision']:.2%}")
                report.append(f"  Recall: {metrics['recall']:.2%}")
                report.append(f"  F1-Score: {metrics['f1']:.3f}")
                report.append(f"  ROC-AUC: {metrics['auc']:.3f}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ø –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if hasattr(self, 'models') and self.models:
            report.append("\n–¢–û–ü-10 –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í:")
            try:
                for model_name, model in self.models.items():
                    if hasattr(model, 'feature_importances_'):
                        importance = model.feature_importances_
                        if len(importance) != len(self.feature_names):
                            logger.warning(f"–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π –¥–ª—è {model_name}: importance={len(importance)}, features={len(self.feature_names)}")
                            continue
                        indices = np.argsort(importance)[-10:][::-1]
                        report.append(f"\n{model_name}:")
                        for idx in indices:
                            if idx < len(self.feature_names):
                                report.append(f"  - {self.feature_names[idx]}: {importance[idx]:.4f}")
                        break  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é –º–æ–¥–µ–ª—å
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ feature importance: {e}")
                report.append("  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
                
        report.append("\n–ú–û–î–ï–õ–ò –°–û–•–†–ê–ù–ï–ù–´ –í:")
        report.append("  trained_model/*_xgboost_v2_*.pkl")
        report.append("  trained_model/scaler_xgboost_v2.pkl")
        report.append("  trained_model/metadata_xgboost_v2.json")
        report.append("="*80)
        
        report_text = '\n'.join(report)
        print(report_text)
        
        with open(f'{log_dir}/final_report.txt', 'w', encoding='utf-8') as f:
            f.write(report_text)
    
    def analyze_winning_patterns(self, model, X_test, y_test, y_pred, y_pred_proba, model_name: str):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è –∫—Ä–∏–ø—Ç–æ—Ç—Ä–µ–π–¥–∏–Ω–≥–∞"""
        logger.info(f"\nüèÜ –ê–Ω–∞–ª–∏–∑ –≤—ã–∏–≥—Ä—ã—à–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è {model_name}:")
        
        # –ù–∞—Ö–æ–¥–∏–º –ø—Ä–∏–º–µ—Ä—ã —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        if len(y_pred_proba.shape) > 1:  # multiclass
            max_proba = y_pred_proba.max(axis=1)
        else:
            max_proba = y_pred_proba
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        percentile_75 = np.percentile(max_proba, 75)
        threshold = max(0.6, percentile_75)
            
        high_confidence = max_proba > threshold
        high_conf_correct = (y_pred == y_test) & high_confidence & (y_test > 0)  # –¢–æ–ª—å–∫–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã "–≤—Ö–æ–¥–∏—Ç—å"
        
        if high_conf_correct.sum() == 0:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç —Å–∏–≥–Ω–∞–ª–æ–≤ —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é")
            return
            
        # –ü–æ–ª—É—á–∞–µ–º DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        X_test_df = pd.DataFrame(X_test, columns=self.feature_names)
        winning_patterns = X_test_df[high_conf_correct]
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {high_conf_correct.sum()} –≤—ã–∏–≥—Ä—ã—à–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é > {threshold:.1%}")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        pattern_features = [
            'momentum_score', 'volume_strength_score', 'volatility_regime_score',
            'oversold_reversal_score', 'breakout_score', 'range_trading_score',
            'asia_session_score', 'rsi_val', 'macd_bullish', 'volume_spike', 
            'strong_trend', 'bb_position', 'adx_val', 'atr_norm',
            'btc_correlation_20', 'market_regime_high_vol'
        ]
        
        logger.info("üîç –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –≤—ã–∏–≥—Ä—ã—à–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤:")
        
        for feature in pattern_features:
            if feature in winning_patterns.columns:
                if feature.endswith('_score'):
                    # –î–ª—è score –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∏ –ø—Ä–æ—Ü–µ–Ω—Ç > –ø–æ—Ä–æ–≥–∞
                    avg = winning_patterns[feature].mean()
                    threshold = 30 if 'oversold' in feature else 40
                    high_score_pct = (winning_patterns[feature] > threshold).mean() * 100
                    logger.info(f"   {feature}: —Å—Ä–µ–¥–Ω–µ–µ={avg:.1f}, >{threshold}={high_score_pct:.1f}%")
                elif feature in ['macd_bullish', 'volume_spike', 'strong_trend']:
                    # –î–ª—è –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    pct = winning_patterns[feature].mean() * 100
                    logger.info(f"   {feature}: {pct:.1f}% —Å–∏–≥–Ω–∞–ª–æ–≤")
                else:
                    # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö - —Å—Ä–µ–¥–Ω–µ–µ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ
                    avg = winning_patterns[feature].mean()
                    std = winning_patterns[feature].std()
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª—É—á–∞—è —Å –æ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º (std = NaN)
                    if np.isnan(std) or len(winning_patterns) == 1:
                        logger.info(f"   {feature}: {avg:.2f}")
                    else:
                        logger.info(f"   {feature}: {avg:.2f} \u00b1 {std:.2f}")
        
        # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
        if 'oversold_reversal_score' in winning_patterns.columns and 'macd_bullish' in winning_patterns.columns:
            combo1 = (winning_patterns['oversold_reversal_score'] > 30) & (winning_patterns['macd_bullish'] == 1)
            combo1_pct = combo1.mean() * 100
            logger.info(f"\nüéØ –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã:")
            logger.info(f"   Oversold + MACD bullish: {combo1_pct:.1f}% —Å–∏–≥–Ω–∞–ª–æ–≤")
            
        if 'breakout_score' in winning_patterns.columns and 'volume_spike' in winning_patterns.columns:
            combo2 = (winning_patterns['breakout_score'] > 40) & (winning_patterns['volume_spike'] == 1)
            combo2_pct = combo2.mean() * 100
            logger.info(f"   Breakout + Volume spike: {combo2_pct:.1f}% —Å–∏–≥–Ω–∞–ª–æ–≤")
    
    def optimize_hyperparameters(self, X_train, y_train, X_val, y_val, task, direction):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é Optuna - —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è"""
        
        # –û—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏ Optuna
        optuna.logging.set_verbosity(optuna.logging.WARNING)
        
        def objective(trial):
            # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã –¥–ª—è –∫—Ä–∏–ø—Ç–æ—Ç—Ä–µ–π–¥–∏–Ω–≥–∞
            params = {
                'max_depth': trial.suggest_int('max_depth', 6, 15),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
                'subsample': trial.suggest_float('subsample', 0.8, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.8, 1.0),
                'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.8, 1.0),
                'gamma': trial.suggest_float('gamma', 0.0, 0.5),
                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),
                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 2.0),
                'min_child_weight': trial.suggest_int('min_child_weight', 1, 5),
                'grow_policy': trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide']),
                'max_bin': trial.suggest_int('max_bin', 128, 512),
            }
            
            # –ù–ï –≤–∫–ª—é—á–∞–µ–º tree_method –≤ params, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –±–∞–∑–æ–≤—ã–π
            # tree_method –±—É–¥–µ—Ç –≤–∑—è—Ç –∏–∑ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (gpu_hist –µ—Å–ª–∏ GPU –¥–æ—Å—Ç—É–ø–µ–Ω)
            
            # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            if task == 'regression':
                model = xgb.XGBRegressor(
                    **params,
                    n_estimators=500,  # –ë–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞
                    objective='reg:squarederror',
                    eval_metric='rmse',
                    early_stopping_rounds=50,
                    random_state=42,
                    n_jobs=-1,
                    verbosity=0
                )
            elif task == 'classification_binary':
                model = xgb.XGBClassifier(
                    **params,
                    n_estimators=1000,
                    objective='binary:logistic',
                    eval_metric='logloss',
                    early_stopping_rounds=50,
                    random_state=42,
                    n_jobs=-1,
                    verbosity=0
                )
                # –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
                scale_pos_weight = len(y_train[y_train == 0]) / (len(y_train[y_train == 1]) + 1e-8)
                model.set_params(scale_pos_weight=scale_pos_weight)
            else:  # multiclass
                num_classes = len(np.unique(y_train))
                model = xgb.XGBClassifier(
                    **params,
                    n_estimators=1000,
                    objective='multi:softprob',
                    num_class=num_classes,
                    eval_metric='mlogloss',
                    early_stopping_rounds=50,
                    random_state=42,
                    n_jobs=-1,
                    verbosity=0
                )
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            eval_set = [(X_train, y_train), (X_val, y_val)]
            model.fit(
                X_train, y_train,
                eval_set=eval_set,
                verbose=False
            )
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            if task == 'regression':
                y_pred = model.predict(X_val)
                score = -mean_squared_error(y_val, y_pred)  # –ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º MSE
            else:
                y_pred = model.predict(X_val)
                score = accuracy_score(y_val, y_pred)  # –ú–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ–º accuracy
            
            return score
        
        # –°–æ–∑–¥–∞–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ Optuna
        study = optuna.create_study(
            direction='maximize' if task != 'regression' else 'minimize',
            sampler=TPESampler(seed=42),
            pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=10)
        )
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é
        logger.info(f"üîç –ù–∞—á–∞–ª–æ Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è {direction}...")
        study.optimize(objective, n_trials=50, show_progress_bar=True)  # –ë–æ–ª—å—à–µ –ø—Ä–æ–±
        
        # –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        best_params = study.best_params
        logger.info(f"‚úÖ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {best_params}")
        logger.info(f"üèÜ –õ—É—á—à–∏–π —Å–∫–æ—Ä: {study.best_value:.4f}")
        
        return best_params
    
    def filter_trading_signals(self, predictions, probabilities, features_df, min_confidence=0.6):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è –∫—Ä–∏–ø—Ç–æ—Ç—Ä–µ–π–¥–∏–Ω–≥–∞"""
        filtered_signals = predictions.copy()
        
        # –§–∏–ª—å—Ç—Ä 1: –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        if len(probabilities.shape) > 1:  # multiclass
            max_proba = probabilities.max(axis=1)
        else:
            max_proba = probabilities
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        percentile_60 = np.percentile(max_proba, 60)
        adaptive_threshold = max(min_confidence, percentile_60)
            
        low_confidence = max_proba < adaptive_threshold
        filtered_signals[low_confidence] = 0  # –ù–µ –≤—Ö–æ–¥–∏—Ç—å
        
        # –§–∏–ª—å—Ç—Ä 2: –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        
        # 2.1 –ù–µ —Ç–æ—Ä–≥–æ–≤–∞—Ç—å –≤ –æ—á–µ–Ω—å –Ω–∏–∑–∫–æ–π –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        if 'atr_norm' in features_df.columns:
            very_low_vol = features_df['atr_norm'] < features_df['atr_norm'].quantile(0.1)
            filtered_signals[very_low_vol] = 0
            
        # 2.2 –¢—Ä–µ–±—É–µ–º —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Å–∏–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
        pattern_cols = ['momentum_score', 'volume_strength_score', 'oversold_reversal_score', 
                       'breakout_score', 'asia_session_score']
        pattern_cols = [col for col in pattern_cols if col in features_df.columns]
        
        if pattern_cols:
            # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ —Å–≤–æ–π –ø–æ—Ä–æ–≥
            pattern_thresholds = {
                'momentum_score': 0.3,
                'volume_strength_score': 0.3,
                'oversold_reversal_score': 0.4,
                'breakout_score': 0.4,
                'asia_session_score': 0.5
            }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ–≥–æ —Å–∏–ª—å–Ω–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
            has_strong_pattern = False
            for col in pattern_cols:
                threshold = pattern_thresholds.get(col, 0.3)
                has_strong_pattern |= (features_df[col] > threshold)
            
            filtered_signals[~has_strong_pattern] = 0
            
        # 2.3 –ò—Å–∫–ª—é—á–∞–µ–º —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏ (–≤–æ–∑–º–æ–∂–Ω–æ –ø–µ—Ä–µ–∫—É–ø–ª–µ–Ω–Ω–æ—Å—Ç—å/–ø–µ—Ä–µ–ø—Ä–æ–¥–∞–Ω–Ω–æ—Å—Ç—å)
        if 'rsi_val' in features_df.columns:
            extreme_rsi = (features_df['rsi_val'] < 20) | (features_df['rsi_val'] > 80)
            # –ù–æ —Ä–∞–∑—Ä–µ—à–∞–µ–º –µ—Å–ª–∏ –µ—Å—Ç—å —Å–∏–ª—å–Ω—ã–π —Ä–∞–∑–≤–æ—Ä–æ—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
            if 'oversold_reversal_score' in features_df.columns:
                strong_reversal = features_df['oversold_reversal_score'] > 0.5
                filtered_signals[extreme_rsi & ~strong_reversal] = 0
            
        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        total_signals = (predictions > 0).sum()
        filtered_count = (filtered_signals > 0).sum()
        logger.info(f"\nüéØ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤:")
        logger.info(f"   –ò—Å—Ö–æ–¥–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã: {total_signals}")
        logger.info(f"   –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {filtered_count}")
        logger.info(f"   –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {total_signals - filtered_count} ({(1 - filtered_count/total_signals)*100:.1f}%)")
        
        return filtered_signals


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description='Enhanced XGBoost Training v2.0 —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º')
    parser.add_argument('--task', type=str, default='classification_binary',
                       choices=['regression', 'classification_binary', 'classification_multiclass'],
                       help='–¢–∏–ø –∑–∞–¥–∞—á–∏')
    parser.add_argument('--ensemble_size', type=int, default=1,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –≤ –∞–Ω—Å–∞–º–±–ª–µ')
    parser.add_argument('--test_mode', action='store_true',
                       help='–¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º (2 —Å–∏–º–≤–æ–ª–∞, –º–µ–Ω—å—à–µ —ç–ø–æ—Ö)')
    parser.add_argument('--config', type=str, default='config.yaml',
                       help='–ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏')
    parser.add_argument('--use-cache', action='store_true',
                       help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã')
    parser.add_argument('--no-cache', action='store_true',
                       help='–û—Ç–∫–ª—é—á–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é')
    parser.add_argument('--force-reload', action='store_true',
                       help='–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å –∫—ç—à')
    parser.add_argument('--cache-dir', type=str, default='cache',
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –∫—ç—à–∞')
    parser.add_argument('--debug', action='store_true',
                       help='–†–µ–∂–∏–º –æ—Ç–ª–∞–¥–∫–∏ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ª–æ–≥–∞–º–∏')
    parser.add_argument('--no-smote', action='store_true',
                       help='–û—Ç–∫–ª—é—á–∏—Ç—å SMOTE –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É –∫–ª–∞—Å—Å–æ–≤')
    parser.add_argument('--balance-method', type=str, default='smote',
                       choices=['smote', 'random', 'none'],
                       help='–ú–µ—Ç–æ–¥ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤: smote, random –∏–ª–∏ none')
    parser.add_argument('--classification-threshold', type=float, default=0.5,
                       help='–ü–æ—Ä–æ–≥ –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.5%)')
    
    args = parser.parse_args()
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —É—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.setLevel(logging.DEBUG)
        logger.info("üêõ –†–µ–∂–∏–º –æ—Ç–ª–∞–¥–∫–∏ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∂–∏–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
    if args.no_cache:
        use_cache = False
        logger.info("üö´ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ")
    else:
        use_cache = args.use_cache
    
    # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä
    trainer = XGBoostEnhancedTrainer(config_path=args.config)
    trainer.train_ensemble(
        task=args.task,
        ensemble_size=args.ensemble_size,
        test_mode=args.test_mode,
        use_cache=use_cache,
        force_reload=args.force_reload,
        no_smote=args.no_smote,
        classification_threshold=args.classification_threshold,
        balance_method=args.balance_method
    )


if __name__ == "__main__":
    main()