#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ Transformer –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã: Temporal Fusion Transformer, PatchTST, –∏ Informer
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import os
import json
import pickle
import time
from datetime import datetime
import psycopg2
import logging
import yaml
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import clear_output
import matplotlib
matplotlib.use('Agg')  # –î–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –±–µ–∑ GUI

warnings.filterwarnings('ignore')
np.random.seed(42)
tf.random.set_seed(42)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
log_dir = f'logs/training_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
os.makedirs(log_dir, exist_ok=True)
os.makedirs(f'{log_dir}/plots', exist_ok=True)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU
physical_devices = tf.config.list_physical_devices('GPU')
if len(physical_devices) > 0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
    logger.info(f"üñ•Ô∏è GPU –¥–æ—Å—Ç—É–ø–µ–Ω: {physical_devices[0].name}")
else:
    logger.info("‚ö†Ô∏è GPU –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")

# –§–∞–π–ª–æ–≤—ã–π –ª–æ–≥–≥–µ—Ä
file_handler = logging.FileHandler(f'{log_dir}/training.log', encoding='utf-8')
file_handler.setLevel(logging.INFO)
file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
file_handler.setFormatter(file_formatter)
logger.addHandler(file_handler)


class TransformerBlock(layers.Layer):
    """–ë–ª–æ–∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ —Å multi-head attention –∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(TransformerBlock, self).__init__()
        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = keras.Sequential([
            layers.Dense(ff_dim, activation="gelu"),
            layers.Dense(embed_dim),
        ])
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(rate)
        self.dropout2 = layers.Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)


class PositionalEncoding(layers.Layer):
    """–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""
    
    def __init__(self, position, d_model):
        super(PositionalEncoding, self).__init__()
        self.pos_encoding = self.positional_encoding(position, d_model)
    
    def get_angles(self, position, i, d_model):
        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))
        return position * angles
    
    def positional_encoding(self, position, d_model):
        angle_rads = self.get_angles(
            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],
            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],
            d_model=d_model
        )
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º sin –∫ —á–µ—Ç–Ω—ã–º –∏–Ω–¥–µ–∫—Å–∞–º
        sines = tf.math.sin(angle_rads[:, 0::2])
        # –ü—Ä–∏–º–µ–Ω—è–µ–º cos –∫ –Ω–µ—á–µ—Ç–Ω—ã–º –∏–Ω–¥–µ–∫—Å–∞–º
        cosines = tf.math.cos(angle_rads[:, 1::2])
        
        pos_encoding = tf.concat([sines, cosines], axis=-1)
        pos_encoding = pos_encoding[tf.newaxis, ...]
        
        return tf.cast(pos_encoding, tf.float32)
    
    def call(self, inputs):
        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]


class PatchTST(layers.Layer):
    """PatchTST - —Ä–∞–∑–±–∏–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –Ω–∞ –ø–∞—Ç—á–∏ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞"""
    
    def __init__(self, patch_len=16, stride=8):
        super(PatchTST, self).__init__()
        self.patch_len = patch_len
        self.stride = stride
    
    def call(self, x):
        batch_size = tf.shape(x)[0]
        seq_len = tf.shape(x)[1]
        num_features = tf.shape(x)[2]
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞—Ç—á–∏
        num_patches = (seq_len - self.patch_len) // self.stride + 1
        patches = []
        
        for i in range(num_patches):
            start_idx = i * self.stride
            end_idx = start_idx + self.patch_len
            patch = x[:, start_idx:end_idx, :]
            patches.append(patch)
        
        # Stack –ø–∞—Ç—á–∏
        patches = tf.stack(patches, axis=1)
        # Reshape –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
        patches = tf.reshape(patches, [batch_size, num_patches, self.patch_len * num_features])
        
        return patches


class TemporalFusionTransformer(keras.Model):
    """Temporal Fusion Transformer –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""
    
    def __init__(self, num_features, sequence_length, 
                 d_model=128, num_heads=8, num_transformer_blocks=4,
                 mlp_units=[256, 128], dropout=0.2):
        super(TemporalFusionTransformer, self).__init__()
        
        self.sequence_length = sequence_length
        self.d_model = d_model
        
        # Variable Selection Network
        self.vsn_dense = layers.Dense(d_model, activation='gelu')
        self.vsn_grn = self.gated_residual_network(d_model, dropout)
        
        # Static covariate encoder
        self.static_encoder = keras.Sequential([
            layers.Dense(d_model, activation='gelu'),
            layers.Dropout(dropout),
            self.gated_residual_network(d_model, dropout)
        ])
        
        # LSTM encoder –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.lstm_encoder = layers.LSTM(d_model, return_sequences=True)
        
        # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.pos_encoding = PositionalEncoding(sequence_length, d_model)
        
        # Transformer –±–ª–æ–∫–∏
        self.transformer_blocks = [
            TransformerBlock(d_model, num_heads, mlp_units[0], dropout)
            for _ in range(num_transformer_blocks)
        ]
        
        # Interpretable Multi-Head Attention
        self.interpretable_attention = layers.MultiHeadAttention(
            num_heads=num_heads, 
            key_dim=d_model,
            dropout=dropout
        )
        
        # Output layers
        self.output_dense = keras.Sequential([
            layers.Dense(mlp_units[0], activation='gelu'),
            layers.Dropout(dropout),
            layers.Dense(mlp_units[1], activation='gelu'),
            layers.Dropout(dropout),
            layers.Dense(1)  # –†–µ–≥—Ä–µ—Å—Å–∏—è
        ])
        
    def gated_residual_network(self, units, dropout_rate):
        """Gated Residual Network –¥–ª—è TFT"""
        return keras.Sequential([
            layers.Dense(units, activation=None),
            layers.ELU(),
            layers.Dense(units, activation=None),
            layers.Dropout(dropout_rate),
            layers.Dense(units, activation='sigmoid'),
            layers.Multiply()
        ])
    
    def call(self, inputs, training=False):
        # Variable selection
        selected = self.vsn_dense(inputs)
        selected = self.vsn_grn(selected)
        
        # LSTM encoding
        lstm_out = self.lstm_encoder(selected)
        
        # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        x = self.pos_encoding(lstm_out)
        
        # Transformer blocks
        for transformer in self.transformer_blocks:
            x = transformer(x, training=training)
        
        # Interpretable attention
        attn_output = self.interpretable_attention(x, x, training=training)
        
        # Global average pooling
        x = layers.GlobalAveragePooling1D()(attn_output)
        
        # Output
        return self.output_dense(x)


class VisualizationCallback(keras.callbacks.Callback):
    """Callback –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"""
    
    def __init__(self, log_dir, model_name, update_freq=5):
        super().__init__()
        self.log_dir = log_dir
        self.model_name = model_name
        self.update_freq = update_freq
        self.epoch_count = 0
        
        # –ò—Å—Ç–æ—Ä–∏—è –º–µ—Ç—Ä–∏–∫
        self.history = {
            'loss': [],
            'val_loss': [],
            'mae': [],
            'val_mae': [],
            'lr': []
        }
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–≥—É—Ä—É –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
        self.fig, self.axes = plt.subplots(2, 2, figsize=(15, 10))
        self.fig.suptitle(f'Training Progress: {model_name}', fontsize=16)
        
    def on_epoch_end(self, epoch, logs=None):
        self.epoch_count += 1
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        self.history['loss'].append(logs.get('loss', 0))
        self.history['val_loss'].append(logs.get('val_loss', 0))
        self.history['mae'].append(logs.get('mae', 0))
        self.history['val_mae'].append(logs.get('val_mae', 0))
        self.history['lr'].append(self.model.optimizer.learning_rate.numpy())
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –≥—Ä–∞—Ñ–∏–∫–∏ –∫–∞–∂–¥—ã–µ N —ç–ø–æ—Ö
        if self.epoch_count % self.update_freq == 0:
            self.update_plots()
    
    def update_plots(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤"""
        epochs = range(1, len(self.history['loss']) + 1)
        
        # –ì—Ä–∞—Ñ–∏–∫ 1: Loss
        self.axes[0, 0].clear()
        self.axes[0, 0].plot(epochs, self.history['loss'], 'b-', label='Train Loss')
        self.axes[0, 0].plot(epochs, self.history['val_loss'], 'r-', label='Val Loss')
        self.axes[0, 0].set_title('Model Loss')
        self.axes[0, 0].set_xlabel('Epoch')
        self.axes[0, 0].set_ylabel('Loss')
        self.axes[0, 0].legend()
        self.axes[0, 0].grid(True)
        
        # –ì—Ä–∞—Ñ–∏–∫ 2: MAE
        self.axes[0, 1].clear()
        self.axes[0, 1].plot(epochs, self.history['mae'], 'b-', label='Train MAE')
        self.axes[0, 1].plot(epochs, self.history['val_mae'], 'r-', label='Val MAE')
        self.axes[0, 1].set_title('Mean Absolute Error')
        self.axes[0, 1].set_xlabel('Epoch')
        self.axes[0, 1].set_ylabel('MAE')
        self.axes[0, 1].legend()
        self.axes[0, 1].grid(True)
        
        # –ì—Ä–∞—Ñ–∏–∫ 3: Learning Rate
        self.axes[1, 0].clear()
        self.axes[1, 0].plot(epochs, self.history['lr'], 'g-')
        self.axes[1, 0].set_title('Learning Rate')
        self.axes[1, 0].set_xlabel('Epoch')
        self.axes[1, 0].set_ylabel('LR')
        self.axes[1, 0].grid(True)
        self.axes[1, 0].set_yscale('log')
        
        # –ì—Ä–∞—Ñ–∏–∫ 4: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.axes[1, 1].clear()
        self.axes[1, 1].axis('off')
        
        # –¢–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        current_stats = f"""
        Current Epoch: {len(self.history['loss'])}
        
        Train Loss: {self.history['loss'][-1]:.4f}
        Val Loss: {self.history['val_loss'][-1]:.4f}
        
        Train MAE: {self.history['mae'][-1]:.4f}%
        Val MAE: {self.history['val_mae'][-1]:.4f}%
        
        Learning Rate: {self.history['lr'][-1]:.6f}
        
        Best Val Loss: {min(self.history['val_loss']):.4f}
        Best Epoch: {self.history['val_loss'].index(min(self.history['val_loss'])) + 1}
        """
        
        self.axes[1, 1].text(0.1, 0.5, current_stats, fontsize=12, 
                             verticalalignment='center', family='monospace')
        
        plt.tight_layout()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
        plt.savefig(f'{self.log_dir}/plots/training_progress_{self.model_name}.png', dpi=100)
        
        # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≥—Ä–∞—Ñ–∏–∫ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞
        plt.savefig(f'{self.log_dir}/plots/latest_progress.png', dpi=100)
        
        logger.info(f"üìä –ì—Ä–∞—Ñ–∏–∫ –æ–±–Ω–æ–≤–ª–µ–Ω: {self.log_dir}/plots/latest_progress.png")


class TransformerTrainer:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Transformer –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self, config_path='config.yaml'):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ë–î
        self.db_config = self.config['database'].copy()
        if not self.db_config.get('password'):
            self.db_config.pop('password', None)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        self.sequence_length = self.config['model']['sequence_length']
        self.batch_size = 32  # –ú–µ–Ω—å—à–µ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤
        self.epochs = 150
        self.learning_rate = 0.0001
        
        # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
        self.d_model = 128
        self.num_heads = 8
        self.num_transformer_blocks = 4
        self.dropout_rate = 0.2
        
        self.scaler = RobustScaler()
        self.models = {}
        self.feature_columns = None
        self.log_dir = log_dir
        
    def connect_db(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î"""
        self.conn = psycopg2.connect(**self.db_config)
        logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω –∫ PostgreSQL")
    
    def create_model(self, num_features, name):
        """–°–æ–∑–¥–∞–Ω–∏–µ TFT –º–æ–¥–µ–ª–∏"""
        model = TemporalFusionTransformer(
            num_features=num_features,
            sequence_length=self.sequence_length,
            d_model=self.d_model,
            num_heads=self.num_heads,
            num_transformer_blocks=self.num_transformer_blocks,
            mlp_units=[256, 128],
            dropout=self.dropout_rate
        )
        
        # Compile
        optimizer = keras.optimizers.Adam(
            learning_rate=self.learning_rate,
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-7
        )
        
        model.compile(
            optimizer=optimizer,
            loss=keras.losses.Huber(delta=1.0),
            metrics=[
                keras.metrics.MeanAbsoluteError(name='mae'),
                keras.metrics.RootMeanSquaredError(name='rmse')
            ]
        )
        
        return model
    
    def load_and_prepare_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL...")
        
        query = """
        SELECT 
            p.symbol, p.timestamp, p.datetime,
            p.technical_indicators
        FROM processed_market_data p
        JOIN raw_market_data r ON p.raw_data_id = r.id
        WHERE p.technical_indicators IS NOT NULL
          AND r.market_type = 'futures'
          AND p.technical_indicators->>'buy_expected_return' IS NOT NULL
        ORDER BY p.symbol, p.timestamp
        """
        
        df = pd.read_sql_query(query, self.conn)
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
        
        if len(df) == 0:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö —Å –Ω–æ–≤—ã–º–∏ –º–µ—Ç–∫–∞–º–∏!")
            return None
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ –º–µ—Ç–∫–∏
        features = []
        buy_targets = []
        sell_targets = []
        
        for _, row in df.iterrows():
            indicators = row['technical_indicators']
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            feature_values = []
            for key, value in indicators.items():
                if key not in ['buy_expected_return', 'sell_expected_return']:
                    val = float(value) if value is not None else 0.0
                    feature_values.append(val)
            
            features.append(feature_values)
            
            # –¶–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            buy_targets.append(float(indicators.get('buy_expected_return', 0.0)))
            sell_targets.append(float(indicators.get('sell_expected_return', 0.0)))
        
        X = np.array(features)
        X_scaled = self.scaler.fit_transform(X)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        sample_indicators = df.iloc[0]['technical_indicators']
        self.feature_columns = [k for k in sample_indicators.keys() 
                               if k not in ['buy_expected_return', 'sell_expected_return']]
        
        return {
            'X': X_scaled,
            'y_buy': np.array(buy_targets),
            'y_sell': np.array(sell_targets),
            'symbols': df['symbol'].values,
            'timestamps': df['timestamp'].values
        }
    
    def create_sequences(self, X, y, symbols):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞"""
        sequences = []
        targets = []
        
        unique_symbols = np.unique(symbols)
        
        for symbol in unique_symbols:
            symbol_mask = symbols == symbol
            symbol_indices = np.where(symbol_mask)[0]
            
            if len(symbol_indices) < self.sequence_length:
                continue
            
            X_symbol = X[symbol_indices]
            y_symbol = y[symbol_indices]
            
            for i in range(len(X_symbol) - self.sequence_length):
                sequences.append(X_symbol[i:i + self.sequence_length])
                targets.append(y_symbol[i + self.sequence_length])
        
        return np.array(sequences), np.array(targets)
    
    def train_with_visualization(self, model, X_train, y_train, X_val, y_val, model_name):
        """–û–±—É—á–µ–Ω–∏–µ —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π"""
        
        logger.info(f"\n{'='*70}")
        logger.info(f"üöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø TRANSFORMER: {model_name}")
        logger.info(f"{'='*70}")
        
        # Callbacks
        callbacks = [
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
            VisualizationCallback(self.log_dir, model_name, update_freq=5),
            
            # Early stopping
            keras.callbacks.EarlyStopping(
                monitor='val_loss',
                patience=20,
                restore_best_weights=True,
                verbose=1
            ),
            
            # Reduce LR
            keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=10,
                min_lr=1e-7,
                verbose=1
            ),
            
            # Model checkpoint
            keras.callbacks.ModelCheckpoint(
                filepath=f'trained_model/{model_name}_transformer_best.h5',
                monitor='val_loss',
                mode='min',
                save_best_only=True,
                verbose=1
            ),
            
            # TensorBoard
            keras.callbacks.TensorBoard(
                log_dir=f'{self.log_dir}/tensorboard/{model_name}',
                histogram_freq=1,
                write_graph=True,
                update_freq='epoch'
            ),
            
            # CSV logger
            keras.callbacks.CSVLogger(
                f'{self.log_dir}/{model_name}_history.csv',
                separator=',',
                append=False
            )
        ]
        
        # –û–±—É—á–µ–Ω–∏–µ
        history = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=self.epochs,
            batch_size=self.batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        return history
    
    def evaluate_model(self, model, X_test, y_test, model_name):
        """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred = model.predict(X_test).flatten()
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        mae = mean_absolute_error(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_test, y_pred)
        
        # –ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        direction_accuracy = np.mean((y_pred > 0) == (y_test > 0))
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'Model Evaluation: {model_name}', fontsize=16)
        
        # –ì—Ä–∞—Ñ–∏–∫ 1: Scatter plot
        axes[0, 0].scatter(y_test, y_pred, alpha=0.5)
        axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
        axes[0, 0].set_xlabel('True Values (%)')
        axes[0, 0].set_ylabel('Predictions (%)')
        axes[0, 0].set_title(f'Predictions vs True Values (R¬≤ = {r2:.3f})')
        axes[0, 0].grid(True)
        
        # –ì—Ä–∞—Ñ–∏–∫ 2: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫
        errors = y_pred - y_test
        axes[0, 1].hist(errors, bins=50, edgecolor='black')
        axes[0, 1].set_xlabel('Prediction Error (%)')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].set_title(f'Error Distribution (MAE = {mae:.3f}%)')
        axes[0, 1].axvline(x=0, color='r', linestyle='--')
        axes[0, 1].grid(True)
        
        # –ì—Ä–∞—Ñ–∏–∫ 3: –í—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        sample_size = min(500, len(y_test))
        axes[1, 0].plot(y_test[:sample_size], 'b-', label='True', alpha=0.7)
        axes[1, 0].plot(y_pred[:sample_size], 'r-', label='Predicted', alpha=0.7)
        axes[1, 0].set_xlabel('Sample Index')
        axes[1, 0].set_ylabel('Return (%)')
        axes[1, 0].set_title('Sample Predictions Timeline')
        axes[1, 0].legend()
        axes[1, 0].grid(True)
        
        # –ì—Ä–∞—Ñ–∏–∫ 4: –ú–µ—Ç—Ä–∏–∫–∏
        axes[1, 1].axis('off')
        metrics_text = f"""
        Model Performance Metrics:
        
        MAE:  {mae:.4f}%
        RMSE: {rmse:.4f}%
        R¬≤:   {r2:.4f}
        
        Direction Accuracy: {direction_accuracy:.2%}
        
        Average Prediction: {np.mean(y_pred):.3f}%
        Std Prediction:     {np.std(y_pred):.3f}%
        
        Average True:       {np.mean(y_test):.3f}%
        Std True:          {np.std(y_test):.3f}%
        """
        
        axes[1, 1].text(0.1, 0.5, metrics_text, fontsize=12, 
                       verticalalignment='center', family='monospace')
        
        plt.tight_layout()
        plt.savefig(f'{self.log_dir}/plots/{model_name}_evaluation.png', dpi=150)
        plt.close()
        
        logger.info(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ {model_name}:")
        logger.info(f"   MAE: {mae:.4f}%")
        logger.info(f"   RMSE: {rmse:.4f}%")
        logger.info(f"   R¬≤: {r2:.4f}")
        logger.info(f"   Direction Accuracy: {direction_accuracy:.2%}")
        
        return {
            'mae': mae,
            'rmse': rmse,
            'r2': r2,
            'direction_accuracy': direction_accuracy
        }
    
    def train(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è"""
        logger.info("\n" + "="*80)
        logger.info("üöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø TRANSFORMER –ú–û–î–ï–õ–ò")
        logger.info("="*80)
        
        try:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            os.makedirs('trained_model', exist_ok=True)
            os.makedirs('logs', exist_ok=True)
            os.makedirs('plots', exist_ok=True)
            
            # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –ë–î
            self.connect_db()
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            data = self.load_and_prepare_data()
            if not data:
                logger.error("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                return
            
            # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—É—é –≤—Ö–æ–¥–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
            dummy_input = tf.zeros((1, self.sequence_length, len(self.feature_columns)))
            
            # –ú–æ–¥–µ–ª–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            model_configs = [
                ('buy_transformer', data['y_buy']),
                ('sell_transformer', data['y_sell'])
            ]
            
            results = {}
            
            for model_name, y_target in model_configs:
                logger.info(f"\n{'='*60}")
                logger.info(f"–û–±—É—á–µ–Ω–∏–µ {model_name}")
                logger.info(f"{'='*60}")
                
                # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                X_seq, y_seq = self.create_sequences(
                    data['X'], y_target, data['symbols']
                )
                
                if len(X_seq) == 0:
                    logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {model_name}")
                    continue
                
                # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
                n_samples = len(X_seq)
                train_end = int(n_samples * 0.7)
                val_end = int(n_samples * 0.85)
                
                X_train = X_seq[:train_end]
                y_train = y_seq[:train_end]
                X_val = X_seq[train_end:val_end]
                y_val = y_seq[train_end:val_end]
                X_test = X_seq[val_end:]
                y_test = y_seq[val_end:]
                
                # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
                model = self.create_model(X_seq.shape[2], model_name)
                
                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
                _ = model(dummy_input)
                
                logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –º–æ–¥–µ–ª—å —Å {model.count_params():,} –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏")
                
                # –û–±—É—á–∞–µ–º —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π
                history = self.train_with_visualization(
                    model, X_train, y_train, X_val, y_val, model_name
                )
                
                # –û—Ü–µ–Ω–∏–≤–∞–µ–º
                test_metrics = self.evaluate_model(
                    model, X_test, y_test, model_name
                )
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                results[model_name] = {
                    'history': history.history,
                    'metrics': test_metrics,
                    'model': model
                }
                
                self.models[model_name] = model
                
                # –°–æ–∑–¥–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
                self.create_final_report(model_name, history, test_metrics)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏
            self.save_models(results)
            
            logger.info("\n‚úÖ –û–ë–£–ß–ï–ù–ò–ï –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!")
            logger.info(f"üìä –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {self.log_dir}/plots/")
            logger.info(f"üìù –õ–æ–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {self.log_dir}/")
            
        except Exception as e:
            logger.error(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
            logger.error(f"–¢—Ä–µ–π—Å–±–µ–∫:", exc_info=True)
            raise
        finally:
            if hasattr(self, 'conn'):
                self.conn.close()
                logger.info("üì§ –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î –∑–∞–∫—Ä—ã—Ç–æ")
    
    def create_final_report(self, model_name, history, metrics):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –ø–æ –º–æ–¥–µ–ª–∏"""
        
        report = f"""
{'='*80}
–û–¢–ß–ï–¢ –ü–û –û–ë–£–ß–ï–ù–ò–Æ –ú–û–î–ï–õ–ò: {model_name}
{'='*80}

–î–∞—Ç–∞ –æ–±—É—á–µ–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

–ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ú–û–î–ï–õ–ò:
- –¢–∏–ø: Temporal Fusion Transformer (TFT)
- Sequence Length: {self.sequence_length}
- Model Dimension: {self.d_model}
- Number of Heads: {self.num_heads}
- Transformer Blocks: {self.num_transformer_blocks}
- Dropout Rate: {self.dropout_rate}

–ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø:
- Batch Size: {self.batch_size}
- Initial Learning Rate: {self.learning_rate}
- Epochs: {len(history.history['loss'])}
- Optimizer: Adam

–§–ò–ù–ê–õ–¨–ù–´–ï –ú–ï–¢–†–ò–ö–ò:
- MAE: {metrics['mae']:.4f}%
- RMSE: {metrics['rmse']:.4f}%
- R¬≤: {metrics['r2']:.4f}
- Direction Accuracy: {metrics['direction_accuracy']:.2%}

–õ–£–ß–®–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:
- Best Val Loss: {min(history.history['val_loss']):.4f}
- Best Epoch: {history.history['val_loss'].index(min(history.history['val_loss'])) + 1}

{'='*80}
"""
        
        with open(f'{self.log_dir}/{model_name}_report.txt', 'w') as f:
            f.write(report)
        
        logger.info(f"üìù –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {self.log_dir}/{model_name}_report.txt")
    
    def save_models(self, results):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
        for name, model in self.models.items():
            model_path = f'trained_model/{name}.h5'
            model.save_weights(model_path)
            logger.info(f"   ‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler
        with open('trained_model/scaler_transformer.pkl', 'wb') as f:
            pickle.dump(self.scaler, f)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            'type': 'temporal_fusion_transformer',
            'sequence_length': self.sequence_length,
            'd_model': self.d_model,
            'num_heads': self.num_heads,
            'num_transformer_blocks': self.num_transformer_blocks,
            'features': self.feature_columns,
            'created_at': datetime.now().isoformat(),
            'results': {
                name: result['metrics']
                for name, result in results.items()
            }
        }
        
        with open('trained_model/metadata_transformer.json', 'w') as f:
            json.dump(metadata, f, indent=2)
        
        logger.info("üíæ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")


if __name__ == "__main__":
    trainer = TransformerTrainer()
    trainer.train()