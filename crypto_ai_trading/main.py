#!/usr/bin/env python3
"""
Crypto AI Trading System - –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞
–ó–∞—â–∏—Ç–∞ –æ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –≤—Å—Ç—Ä–æ–µ–Ω–∞ –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
–ü–æ–¥–¥–µ—Ä–∂–∫–∞ production —Ä–µ–∂–∏–º–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
"""

import argparse
import yaml
from pathlib import Path
import torch
import pandas as pd
import numpy as np
from datetime import datetime
import warnings
import sys
import os
import json
from typing import Dict, List, Tuple, Optional

warnings.filterwarnings('ignore')

from utils.logger import get_logger

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.enabled = True
    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ float32 matmul precision –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–∞ –Ω–æ–≤—ã—Ö GPU
    torch.set_float32_matmul_precision('high')
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è Ampere+ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã (RTX 5090)
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

# –í–µ—Ä—Å–∏—è —Å–∏—Å—Ç–µ–º—ã
__version__ = "3.0.0"

def load_config(config_path: str) -> dict:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


class ProductionConfig:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ production –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π"""
    
    def __init__(self, config_path: str, production_mode: bool = False):
        self.config = self.load_config(config_path)
        if production_mode:
            self.validate_config()
            self.apply_production_settings()
    
    def load_config(self, config_path: str) -> dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
        if not Path(config_path).exists():
            raise FileNotFoundError(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {config_path}")
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        return config
    
    def validate_config(self):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        required_keys = [
            'model', 'loss', 'data', 'performance', 
            'database', 'risk_management'
        ]
        
        for key in required_keys:
            if key not in self.config:
                raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑–¥–µ–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {key}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è production
        if self.config['model']['learning_rate'] < 0.0001:
            print("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –æ—á–µ–Ω—å –Ω–∏–∑–∫–∏–π learning rate –º–æ–∂–µ—Ç –∑–∞–º–µ–¥–ª–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ")
        
        if self.config['loss']['task_weights']['directions'] < 5.0:
            print("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–∏–∑–∫–∏–π –≤–µ—Å direction loss –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø–ª–æ—Ö–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è")
    
    def apply_production_settings(self):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ production-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫"""
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.config['logging'] = self.config.get('logging', {})
        self.config['logging']['level'] = 'INFO'
        self.config['logging']['save_to_file'] = True
        
        # –í–∫–ª—é—á–∞–µ–º –≤—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        self.config['validation'] = {
            'check_data_quality': True,
            'check_model_performance': True,
            'minimum_direction_accuracy': 0.6,
            'minimum_win_rate': 0.45,
            'maximum_flat_predictions': 0.7
        }
        
        # –ó–∞—â–∏—Ç–∞ –æ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        self.config['model']['early_stopping_patience'] = 25
        self.config['model']['min_delta'] = 0.0001
        
        return self.config


class ModelValidator:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤ production"""
    
    def __init__(self, config: dict, logger):
        self.config = config
        self.logger = logger
        self.validation_results = {}
    
    def validate_model(self, model: torch.nn.Module, val_loader) -> bool:
        """–ü–æ–ª–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
        self.logger.info("üîç –ó–∞–ø—É—Å–∫ production –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏...")
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
        if not self._validate_architecture(model):
            return False
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if not self._validate_performance(model, val_loader):
            return False
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        if not self._validate_prediction_diversity(model, val_loader):
            return False
        
        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏
        if not self._validate_robustness(model, val_loader):
            return False
        
        self._save_validation_report()
        return True
    
    def _validate_architecture(self, model: torch.nn.Module) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã"""
        self.logger.info("  üìê –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã...")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        required_modules = ['direction_head', 'future_returns_head', 'long_levels_head']
        
        for module_name in required_modules:
            if not hasattr(model, module_name):
                self.logger.error(f"    ‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –º–æ–¥—É–ª—å: {module_name}")
                return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π
        try:
            batch_size = 32
            seq_len = self.config['model']['context_window']
            n_features = self.config['model']['input_size']
            
            dummy_input = torch.randn(batch_size, seq_len, n_features).to(next(model.parameters()).device)
            with torch.no_grad():
                output = model(dummy_input)
            
            expected_output_size = self.config['model']['output_size']
            if output.shape[-1] != expected_output_size:
                self.logger.error(f"    ‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–∞: {output.shape[-1]} != {expected_output_size}")
                return False
            
            self.logger.info("    ‚úÖ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞")
            return True
            
        except Exception as e:
            self.logger.error(f"    ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã: {e}")
            return False
    
    def _validate_performance(self, model: torch.nn.Module, val_loader) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
        self.logger.info("  üìä –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")
        
        from training.optimized_trainer import OptimizedTrainer
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π trainer –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        trainer = OptimizedTrainer(model, self.config, device=next(model.parameters()).device)
        
        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics = trainer.validate_with_enhanced_metrics(val_loader)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
        min_requirements = self.config['validation']
        
        direction_accuracy = metrics.get('direction_accuracy_overall', 0)
        win_rate = metrics.get('win_rate_overall', 0)
        
        self.validation_results['direction_accuracy'] = direction_accuracy
        self.validation_results['win_rate'] = win_rate
        
        if direction_accuracy < min_requirements['minimum_direction_accuracy']:
            self.logger.error(f"    ‚ùå Direction accuracy —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∞—è: {direction_accuracy:.3f} < {min_requirements['minimum_direction_accuracy']}")
            return False
        
        if win_rate < min_requirements['minimum_win_rate']:
            self.logger.error(f"    ‚ùå Win rate —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π: {win_rate:.3f} < {min_requirements['minimum_win_rate']}")
            return False
        
        self.logger.info(f"    ‚úÖ –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞ (Accuracy: {direction_accuracy:.3f}, Win Rate: {win_rate:.3f})")
        return True
    
    def _validate_prediction_diversity(self, model: torch.nn.Module, val_loader) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
        self.logger.info("  üé≤ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
        
        from training.optimized_trainer import OptimizedTrainer
        
        trainer = OptimizedTrainer(model, self.config, device=next(model.parameters()).device)
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—ã–π –±–∞—Ç—á –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        for inputs, targets, _ in val_loader:
            inputs = inputs.to(next(model.parameters()).device)
            targets = targets.to(next(model.parameters()).device)
            break
        
        with torch.no_grad():
            outputs = model(inputs)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º direction –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        direction_metrics = trainer.compute_direction_metrics(outputs, targets)
        
        pred_entropy = direction_metrics.get('pred_entropy_overall', 0)
        flat_ratio = direction_metrics.get('pred_flat_ratio_overall', 1.0)
        
        self.validation_results['prediction_entropy'] = pred_entropy
        self.validation_results['flat_prediction_ratio'] = flat_ratio
        
        max_flat = self.config['validation']['maximum_flat_predictions']
        
        if flat_ratio > max_flat:
            self.logger.error(f"    ‚ùå –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ FLAT –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {flat_ratio:.1%} > {max_flat:.1%}")
            return False
        
        if pred_entropy < 0.3:
            self.logger.error(f"    ‚ùå –°–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {pred_entropy:.3f}")
            return False
        
        self.logger.info(f"    ‚úÖ –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ (Entropy: {pred_entropy:.3f}, FLAT: {flat_ratio:.1%})")
        return True
    
    def _validate_robustness(self, model: torch.nn.Module, val_loader) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∫ —à—É–º—É"""
        self.logger.info("  üõ°Ô∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏...")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—ã–π –±–∞—Ç—á
        for inputs, targets, _ in val_loader:
            inputs = inputs.to(next(model.parameters()).device)
            break
        
        with torch.no_grad():
            # –û–±—ã—á–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            outputs_normal = model(inputs)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –Ω–µ–±–æ–ª—å—à–∏–º —à—É–º–æ–º
            noise = torch.randn_like(inputs) * 0.01
            outputs_noisy = model(inputs + noise)
            
            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º direction –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            if hasattr(outputs_normal, '_direction_logits'):
                pred_normal = torch.argmax(outputs_normal._direction_logits, dim=-1)
                pred_noisy = torch.argmax(outputs_noisy._direction_logits, dim=-1)
                
                consistency = (pred_normal == pred_noisy).float().mean().item()
                
                self.validation_results['noise_robustness'] = consistency
                
                if consistency < 0.9:
                    self.logger.error(f"    ‚ùå –ù–∏–∑–∫–∞—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —à—É–º—É: {consistency:.3f}")
                    return False
                
                self.logger.info(f"    ‚úÖ –ú–æ–¥–µ–ª—å —É—Å—Ç–æ–π—á–∏–≤–∞ –∫ —à—É–º—É (consistency: {consistency:.3f})")
                return True
            else:
                self.logger.warning("    ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å (–Ω–µ—Ç direction_logits)")
                return True
    
    def _save_validation_report(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        report_path = Path("validation_reports") / f"validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report_path.parent.mkdir(exist_ok=True)
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "version": __version__,
            "validation_results": self.validation_results,
            "passed": True
        }
        
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        self.logger.info(f"  üíæ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")


class ProductionInference:
    """–ö–ª–∞—Å—Å –¥–ª—è production inference —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –æ—à–∏–±–æ–∫"""
    
    def __init__(self, model_path: str, config: dict, logger):
        self.config = config
        self.logger = logger
        self.model = self._load_model(model_path)
        self.device = next(self.model.parameters()).device
    
    def _load_model(self, model_path: str) -> torch.nn.Module:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        if not Path(model_path).exists():
            raise FileNotFoundError(f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
        
        from models.patchtst_unified import create_unified_model
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º checkpoint
        checkpoint = torch.load(model_path, map_location='cpu')
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ checkpoint –µ—Å–ª–∏ –µ—Å—Ç—å
        if isinstance(checkpoint, dict) and 'config' in checkpoint:
            saved_config = checkpoint['config']
            if 'model' in saved_config:
                self.config['model'].update(saved_config['model'])
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        model = create_unified_model(self.config)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        
        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ –Ω—É–∂–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)
        model.eval()
        
        self.logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path}")
        return model
    
    def predict(self, data: torch.Tensor) -> Dict[str, torch.Tensor]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            self.model.eval()
            with torch.no_grad():
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
                if len(data.shape) == 2:
                    data = data.unsqueeze(0)  # –î–æ–±–∞–≤–ª—è–µ–º batch dimension
                
                # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
                data = data.to(self.device)
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                outputs = self.model(data)
                
                # –ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                results = self._parse_outputs(outputs)
                
                # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                if self._validate_predictions(results):
                    return results
                else:
                    raise ValueError("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–µ –ø—Ä–æ—à–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é")
                    
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            return self._get_safe_defaults()
    
    def _parse_outputs(self, outputs: torch.Tensor) -> Dict[str, torch.Tensor]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—ã—Ö–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏ –≤ —É–¥–æ–±–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç"""
        results = {
            'future_returns': outputs[:, 0:4].cpu(),
            'directions': outputs[:, 4:8].cpu(),
            'long_levels': torch.sigmoid(outputs[:, 8:12]).cpu(),
            'short_levels': torch.sigmoid(outputs[:, 12:16]).cpu(),
            'risk_metrics': outputs[:, 16:20].cpu()
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª–∞—Å—Å—ã direction –µ—Å–ª–∏ –µ—Å—Ç—å –ª–æ–≥–∏—Ç—ã
        if hasattr(outputs, '_direction_logits'):
            direction_probs = torch.softmax(outputs._direction_logits, dim=-1)
            direction_classes = torch.argmax(direction_probs, dim=-1)
            results['direction_classes'] = direction_classes.cpu()
            results['direction_probs'] = direction_probs.cpu()
        
        return results
    
    def _validate_predictions(self, results: Dict[str, torch.Tensor]) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ —Ä–∞–∑—É–º–Ω–æ—Å—Ç—å"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º future returns –≤ —Ä–∞–∑—É–º–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö (-50%, +50%)
        returns = results['future_returns']
        if torch.abs(returns).max() > 0.5:
            self.logger.warning("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è returns")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤ [0, 1]
        for key in ['long_levels', 'short_levels']:
            probs = results[key]
            if probs.min() < 0 or probs.max() > 1:
                self.logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤ {key}")
                return False
        
        return True
    
    def _get_safe_defaults(self) -> Dict[str, torch.Tensor]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
        batch_size = 1
        return {
            'future_returns': torch.zeros(batch_size, 4),
            'directions': torch.full((batch_size, 4), 2),  # FLAT
            'long_levels': torch.zeros(batch_size, 4),
            'short_levels': torch.zeros(batch_size, 4),
            'risk_metrics': torch.zeros(batch_size, 4),
            'direction_classes': torch.full((batch_size, 4), 2),  # FLAT
            'error': True
        }


def load_cached_data_if_exists(logger) -> tuple:
    """–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    
    Returns:
        tuple: (train_data, val_data, test_data, feature_cols, target_cols) –∏–ª–∏ (None, None, None, None, None)
    """
    logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    processed_dir = Path("data/processed")
    train_file = processed_dir / "train_data.parquet"
    val_file = processed_dir / "val_data.parquet"
    test_file = processed_dir / "test_data.parquet"
    
    if all(f.exists() for f in [train_file, val_file, test_file]):
        logger.info("‚úÖ –ù–∞–π–¥–µ–Ω—ã –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –∑–∞–≥—Ä—É–∂–∞–µ–º...")
        
        train_data = pd.read_parquet(train_file)
        val_data = pd.read_parquet(val_file)
        test_data = pd.read_parquet(test_file)
        
        logger.info(f"üìä –†–∞–∑–º–µ—Ä—ã –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
        logger.info(f"   - Train: {len(train_data):,} –∑–∞–ø–∏—Å–µ–π")
        logger.info(f"   - Val: {len(val_data):,} –∑–∞–ø–∏—Å–µ–π")
        logger.info(f"   - Test: {len(test_data):,} –∑–∞–ø–∏—Å–µ–π")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        from data.constants import (
            get_feature_columns, get_target_columns, 
            validate_data_structure, TRADING_TARGET_VARIABLES
        )
        
        try:
            data_info = validate_data_structure(train_data)
            feature_cols = data_info['feature_cols']
            target_cols = data_info['target_cols']
            
            logger.info(f"üìà –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
            logger.info(f"   - –í—Å–µ–≥–æ –∫–æ–ª–æ–Ω–æ–∫: {len(train_data.columns)}")
            logger.info(f"   - –ü—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏: {len(feature_cols)}")
            logger.info(f"   - –¶–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö: {len(target_cols)}")
            logger.info(f"   - –°–ª—É–∂–µ–±–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: {len(train_data.columns) - len(feature_cols) - len(target_cols)}")
            
            return train_data, val_data, test_data, feature_cols, target_cols
            
        except ValueError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
            return None, None, None, None, None
    else:
        logger.info("‚ùå –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        missing_files = [f.name for f in [train_file, val_file, test_file] if not f.exists()]
        logger.info(f"   –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∞–π–ª—ã: {missing_files}")
        return None, None, None, None, None

def create_unified_data_loaders(train_data, val_data, test_data, feature_cols, target_cols, config, logger):
    """–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤ –¥–ª—è –≤—Å–µ—Ö —Ä–µ–∂–∏–º–æ–≤
    
    Args:
        train_data, val_data, test_data: DataFrame'—ã —Å –¥–∞–Ω–Ω—ã–º–∏
        feature_cols, target_cols: —Å–ø–∏—Å–∫–∏ –∫–æ–ª–æ–Ω–æ–∫
        config: –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        logger: –ª–æ–≥–≥–µ—Ä
        
    Returns:
        tuple: (train_loader, val_loader, test_loader)
    """
    logger.info("üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö DataLoader'–æ–≤...")
    
    from data.dataset import create_data_loaders
    from data.precomputed_dataset import create_precomputed_data_loaders
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º PrecomputedDataset –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    use_precomputed = config.get('performance', {}).get('use_precomputed_dataset', True)
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —á—Ç–æ–±—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–º –¥–∞–Ω–Ω—ã–º
    config_updated = config.copy()
    config_updated['model']['input_features'] = len(feature_cols)
    config_updated['model']['n_features'] = len(feature_cols)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –º–æ–¥–µ–ª–∏
    task_type = config['model'].get('task_type', 'regression')
    
    if task_type == 'trading':
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –í–°–ï –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ –∫—ç—à–∞
        config_updated['model']['target_variables'] = target_cols
        logger.info(f"‚úÖ –¢–æ—Ä–≥–æ–≤–∞—è –º–æ–¥–µ–ª—å: –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ {len(target_cols)} —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö")
        logger.info(f"   –ü–µ—Ä–≤—ã–µ 5 –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö: {target_cols[:5]}")
    else:
        # –î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –≤—ã–±–∏—Ä–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
        main_target = [col for col in target_cols if col.startswith('future_return_')]
        if main_target:
            config_updated['model']['target_variable'] = main_target[0]
            logger.info(f"‚úÖ –†–µ–≥—Ä–µ—Å—Å–∏—è: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é {main_target[0]}")
        else:
            logger.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–∞ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏!")
            raise ValueError("–ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–µ–π —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    if use_precomputed:
        logger.info("üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ–º PrecomputedDataset –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏")
        train_loader, val_loader, test_loader = create_precomputed_data_loaders(
            train_data=train_data,
            val_data=val_data, 
            test_data=test_data,
            config=config_updated,
            feature_cols=feature_cols,
            target_cols=target_cols
        )
    else:
        logger.info("üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Dataset")
        train_loader, val_loader, test_loader = create_data_loaders(
            train_data=train_data,
            val_data=val_data, 
            test_data=test_data,
            config=config_updated,
            feature_cols=feature_cols,
            target_cols=target_cols
        )
    
    logger.info("‚úÖ DataLoader'—ã —Å–æ–∑–¥–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ")
    return train_loader, val_loader, test_loader, config_updated

def prepare_data(config: dict, logger):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç data leakage"""
    logger.start_stage("data_preparation")
    
    logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL...")
    
    # –ò–º–ø–æ—Ä—Ç –∑–¥–µ—Å—å –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏—Ö –∏–º–ø–æ—Ä—Ç–æ–≤
    from data.data_loader import CryptoDataLoader
    from data.feature_engineering import FeatureEngineer
    from data.dataset import create_data_loaders, TradingDataset
    
    data_loader = CryptoDataLoader(config)
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤
    if config['data']['symbols'] == 'all':
        available_symbols = data_loader.get_available_symbols()
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –¥–µ–º–æ
        max_symbols = config.get('data', {}).get('max_symbols', 10)
        symbols_to_load = available_symbols[:max_symbols]
        logger.info(f"üìä –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–≤—ã–µ {max_symbols} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ {len(available_symbols)}: {symbols_to_load}")
    else:
        symbols_to_load = config['data']['symbols']
        logger.info(f"üìä –ó–∞–≥—Ä—É–∂–∞–µ–º —É–∫–∞–∑–∞–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã: {symbols_to_load}")
    
    raw_data = data_loader.load_data(
        symbols=symbols_to_load,
        start_date=config['data']['start_date'],
        end_date=config['data']['end_date']
    )
    
    logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö...")
    quality_report = data_loader.validate_data_quality(raw_data)
    
    for symbol, report in quality_report.items():
        if report['anomalies']:
            logger.warning(f"–ê–Ω–æ–º–∞–ª–∏–∏ –≤ –¥–∞–Ω–Ω—ã—Ö {symbol}: {report['anomalies']}")
    
    logger.info("üõ†Ô∏è –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç data leakage...")
    feature_engineer = FeatureEngineer(config)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç data leakage
    train_data, val_data, test_data = feature_engineer.create_features_with_train_split(
        raw_data,
        train_ratio=config['data']['train_ratio'],
        val_ratio=config['data']['val_ratio']
    )
    
    logger.info("üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ datasets...")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤ —á–µ—Ä–µ–∑ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É
    from data.constants import get_feature_columns, get_target_columns, validate_data_structure
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
    data_info = validate_data_structure(train_data)
    feature_cols = data_info['feature_cols']
    target_cols = data_info['target_cols']
    
    train_loader, val_loader, test_loader, _ = create_unified_data_loaders(
        train_data, val_data, test_data, feature_cols, target_cols, config, logger
    )
    
    logger.info(f"üìä –†–∞–∑–º–µ—Ä—ã datasets:")
    logger.info(f"   - Train: {len(train_data)} –∑–∞–ø–∏—Å–µ–π")
    logger.info(f"   - Val: {len(val_data)} –∑–∞–ø–∏—Å–µ–π")
    logger.info(f"   - Test: {len(test_data)} –∑–∞–ø–∏—Å–µ–π")
    
    logger.end_stage("data_preparation", 
                    train_size=len(train_data),
                    val_size=len(val_data),
                    test_size=len(test_data))
    
    return train_loader, val_loader, test_loader

def train_model(config: dict, train_loader, val_loader, logger):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
    import time
    logger.start_stage("model_training")
    
    logger.info("üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ PatchTST...")
    
    # –í–†–ï–ú–ï–ù–ù–û–ï –†–ï–®–ï–ù–ò–ï: –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –≤–º–µ—Å—Ç–æ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞—Ç—á–∞
    # TODO: –∏—Å–ø—Ä–∞–≤–∏—Ç—å –º–µ–¥–ª–µ–Ω–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞ —Å HDF5
    logger.info("üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö...")
    n_features = 240  # –ò–∑–≤–µ—Å—Ç–Ω–æ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    n_targets = 20    # –ò–∑–≤–µ—Å—Ç–Ω–æ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    
    # –ó–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–æ –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º—ã —Å –º–µ–¥–ª–µ–Ω–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π
    # import time
    # start_time = time.time()
    # logger.info("üìä –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞...")
    # sample_batch = next(iter(train_loader))
    # logger.info(f"‚úÖ –ü–µ—Ä–≤—ã–π –±–∞—Ç—á –ø–æ–ª—É—á–µ–Ω –∑–∞ {time.time() - start_time:.2f} —Å–µ–∫—É–Ω–¥")
    # X_sample, y_sample, _ = sample_batch
    # 
    # n_features = X_sample.shape[-1]  # –ü–æ—Å–ª–µ–¥–Ω—è—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
    # n_targets = y_sample.shape[-1] if y_sample is not None else 1
    
    logger.info(f"üìä –í—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {n_features}, –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: {n_targets}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
    config_input_size = config['model'].get('input_size', 100)
    config_output_size = config['model'].get('output_size', 1)
    task_type = config['model'].get('task_type', 'regression')
    
    if n_features != config_input_size:
        logger.warning(f"‚ö†Ô∏è –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç: –¥–∞–Ω–Ω—ã–µ={n_features}, –∫–æ–Ω—Ñ–∏–≥={config_input_size}")
        logger.info(f"üîß –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–ª—è–µ–º input_size –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
        config['model']['input_size'] = n_features
    
    if task_type == 'trading':
        # –î–ª—è —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ü–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
        if config['model']['name'] == 'UnifiedPatchTST':  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
            logger.info(f"üìä –¢–æ—Ä–≥–æ–≤–∞—è –º–æ–¥–µ–ª—å: {n_targets} —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö - –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–∏–±–∫—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É")
            config['model']['output_size'] = n_targets
        else:
            logger.info(f"üìä –¢–æ—Ä–≥–æ–≤–∞—è –º–æ–¥–µ–ª—å: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è PatchTSTForTrading —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –≤—ã—Ö–æ–¥–∞–º–∏")
    else:
        if n_targets != config_output_size:
            logger.warning(f"‚ö†Ô∏è –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Ü–µ–ª–µ–π –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç: –¥–∞–Ω–Ω—ã–µ={n_targets}, –∫–æ–Ω—Ñ–∏–≥={config_output_size}")
            logger.info(f"üîß –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–ª—è–µ–º output_size –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
            config['model']['output_size'] = n_targets
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∞–±—Ä–∏–∫—É –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    from models.patchtst import create_patchtst_model
    from models.patchtst_unified import create_unified_model, UnifiedPatchTSTForTrading
    
    # Ensemble –æ–±—É—á–µ–Ω–∏–µ
    ensemble_count = config.get('training', {}).get('ensemble_count', 1)
    
    if ensemble_count > 1:
        logger.info(f"üé≠ –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –∏–∑ {ensemble_count} –º–æ–¥–µ–ª–µ–π")
        models = []
        
        for i in range(ensemble_count):
            logger.info(f"üìä –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ {i+1}/{ensemble_count}")
            
            # –í–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –≤ –∞–Ω—Å–∞–º–±–ª–µ
            model_config = config.copy()
            model_config['model']['random_seed'] = config.get('model', {}).get('random_seed', 42) + i
            
            # –ù–µ–±–æ–ª—å—à–∏–µ –≤–∞—Ä–∏–∞—Ü–∏–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
            if i > 0:
                # –í–∞—Ä–∏–∞—Ü–∏—è dropout
                original_dropout = model_config['model'].get('dropout', 0.1)
                model_config['model']['dropout'] = original_dropout + (i * 0.05)
                
                # –í–∞—Ä–∏–∞—Ü–∏—è learning rate  
                original_lr = model_config['model'].get('learning_rate', 2e-5)
                model_config['model']['learning_rate'] = original_lr * (1 + (i-1) * 0.2)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            if task_type == 'trading' and n_targets > 10:
                model_config['model']['name'] = 'UnifiedPatchTST'
                model_config['model']['output_size'] = n_targets
                model = create_unified_model(model_config)
            elif model_config['model']['name'] == 'UnifiedPatchTST':
                model = create_unified_model(model_config)
            else:
                model = create_patchtst_model(model_config)
            
            models.append(model)
        
        logger.info(f"‚úÖ –ê–Ω—Å–∞–º–±–ª—å –∏–∑ {len(models)} –º–æ–¥–µ–ª–µ–π —Å–æ–∑–¥–∞–Ω")
        
        # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã, –æ–±—É—á–∞–µ–º –ø–µ—Ä–≤—É—é –º–æ–¥–µ–ª—å (–≤ –±—É–¥—É—â–µ–º –º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å)
        model = models[0] 
        logger.info(f"üéØ –û–±—É—á–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–π –º–æ–¥–µ–ª–∏ –∏–∑ –∞–Ω—Å–∞–º–±–ª—è (–º–æ–¥–µ–ª—å 1/{ensemble_count})")
        
    else:
        # –û–¥–∏–Ω–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å
        # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º UnifiedPatchTST –¥–ª—è 20 —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        if task_type == 'trading' and n_targets > 10:
            logger.info(f"üéØ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {n_targets} —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö - –∏—Å–ø–æ–ª—å–∑—É–µ–º UnifiedPatchTST")
            config['model']['name'] = 'UnifiedPatchTST'
            config['model']['output_size'] = n_targets
            
            model_start_time = time.time()
            logger.info("üî® –í—ã–∑–æ–≤ create_unified_model...")
            model = create_unified_model(config)
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞ –∑–∞ {time.time() - model_start_time:.2f} —Å–µ–∫—É–Ω–¥")
            logger.info(f"‚úÖ UnifiedPatchTST —Å–æ–∑–¥–∞–Ω —Å {n_targets} –≤—ã—Ö–æ–¥–∞–º–∏ –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏")
            logger.info("‚ö†Ô∏è torch.compile –æ—Ç–∫–ª—é—á–µ–Ω –¥–ª—è RTX 5090 (sm_120) - —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –≤ –±—É–¥—É—â–∏—Ö –≤–µ—Ä—Å–∏—è—Ö PyTorch")
        elif config['model']['name'] == 'UnifiedPatchTST':
            model = create_unified_model(config)
            logger.info("üìä –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è UnifiedPatchTST —Å 20 –≤—ã—Ö–æ–¥–∞–º–∏")
            logger.info("‚ö†Ô∏è torch.compile –æ—Ç–∫–ª—é—á–µ–Ω –¥–ª—è RTX 5090 (sm_120)")
        else:
            model = create_patchtst_model(config)
            # –õ–æ–≥–∏—Ä—É–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏
            if hasattr(model, 'long_model'):
                logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è PatchTSTForTrading —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π LONG/SHORT")
            else:
                logger.info("üìä –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∞–∑–æ–≤–∞—è PatchTSTForPrediction")
    
    # –í–ê–ñ–ù–û: –Ø–≤–Ω–æ –ø–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ GPU –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º —Ç—Ä–µ–π–Ω–µ—Ä–∞
    if torch.cuda.is_available():
        device = torch.device('cuda')
        model = model.to(device)
        logger.info(f"üî• –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–º–µ—â–µ–Ω–∞ –Ω–∞ GPU: {torch.cuda.get_device_name(0)}")
        logger.info(f"üíæ GPU –ø–∞–º—è—Ç—å –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    else:
        device = torch.device('cpu')
        logger.warning("‚ö†Ô∏è GPU –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
    if config.get('production', {}).get('staged_training', {}).get('enabled', False):
        logger.info("üéØ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (StagedTrainer)")
        from training.staged_trainer import StagedTrainer
        trainer = StagedTrainer(model, config, device=device)
    else:
        # –°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç—Ä–µ–π–Ω–µ—Ä–∞ —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        from training.optimized_trainer import OptimizedTrainer
        trainer = OptimizedTrainer(model, config, device=device)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {next(model.parameters()).device}")
    logger.info(f"‚úÖ –¢—Ä–µ–π–Ω–µ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç: {trainer.device}")
    
    # DataLoader'—ã —É–∂–µ —Å–æ–∑–¥–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö –Ω–∞–ø—Ä—è–º—É—é
    
    # –û–±—É—á–µ–Ω–∏–µ
    logger.info("üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    training_results = trainer.train(
        train_loader=train_loader,
        val_loader=val_loader
    )
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    if hasattr(trainer, 'checkpoint_dir'):
        # –î–ª—è OptimizedTrainer
        best_model_path = trainer.checkpoint_dir / "best_model.pth"
    else:
        # –î–ª—è StagedTrainer - —Å–æ–∑–¥–∞–µ–º –ø—É—Ç—å –≤—Ä—É—á–Ω—É—é
        from pathlib import Path
        checkpoint_dir = Path(config['model'].get('checkpoint_dir', 'models_saved'))
        checkpoint_dir.mkdir(exist_ok=True, parents=True)
        best_model_path = checkpoint_dir / "best_model.pth"
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        torch.save({
            'model_state_dict': model.state_dict(),
            'config': config,
            'training_results': training_results
        }, best_model_path)
        logger.info(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {best_model_path}")
    
    logger.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_path}")
    
    logger.end_stage("model_training", model_path=str(best_model_path))
    
    return model, best_model_path, train_loader

def backtest_strategy(config: dict, model, test_loader, train_loader, logger):
    """–ë—ç–∫—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å UnifiedPatchTST"""
    logger.start_stage("backtesting")
    
    logger.info("üí∞ –ó–∞–ø—É—Å–∫ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –±—ç–∫—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è UnifiedPatchTST...")
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π UnifiedBacktester
    from trading.unified_backtester import UnifiedBacktester
    
    # –°–æ–∑–¥–∞–µ–º –±—ç–∫—Ç–µ—Å—Ç–µ—Ä
    backtester = UnifiedBacktester(config)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏ –º–æ–¥–µ–ª–∏
    logger.info("üîÆ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    try:
        # –ó–∞–ø—É—Å–∫ –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥–∞
        backtest_results = backtester.run_backtest(model, test_loader)
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥–µ: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ –æ—à–∏–±–∫–µ
        backtest_results = {
            'total_trades': 0,
            'win_rate': 0,
            'total_return': 0,
            'sharpe_ratio': 0,
            'max_drawdown': 0,
            'profit_factor': 0,
            'final_balance': config['backtesting']['initial_capital']
        }
    
    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    logger.info("üìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ë–≠–ö–¢–ï–°–¢–ò–ù–ì–ê:")
    logger.info(f"  –ù–∞—á–∞–ª—å–Ω—ã–π –∫–∞–ø–∏—Ç–∞–ª: ${config['backtesting']['initial_capital']:,.2f}")
    logger.info(f"  –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–∞–ø–∏—Ç–∞–ª: ${backtest_results.get('final_balance', config['backtesting']['initial_capital']):,.2f}")
    logger.info(f"  –û–±—â–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {backtest_results.get('total_return', 0)*100:.2f}%")
    logger.info(f"  –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –®–∞—Ä–ø–∞: {backtest_results.get('sharpe_ratio', 0):.2f}")
    logger.info(f"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞: {backtest_results.get('max_drawdown', 0)*100:.2f}%")
    logger.info(f"  Win Rate: {backtest_results.get('win_rate', 0)*100:.2f}%")
    logger.info(f"  –í—Å–µ–≥–æ —Å–¥–µ–ª–æ–∫: {backtest_results.get('total_trades', 0)}")
    
    logger.end_stage("backtesting", 
                    total_return=backtest_results.get('total_return', 0)*100,
                    sharpe_ratio=backtest_results.get('sharpe_ratio', 0))
    
    return backtest_results

def analyze_results(config: dict, results: dict, logger):
    """–ê–Ω–∞–ª–∏–∑ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    logger.start_stage("results_analysis")
    
    logger.info("üìä –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    
    min_sharpe = config['validation']['min_sharpe_ratio']
    min_win_rate = config['validation']['min_win_rate']
    max_dd = config['validation']['max_drawdown']
    
    passed_validation = True
    
    if results['sharpe_ratio'] < min_sharpe:
        logger.warning(f"‚ö†Ô∏è Sharpe Ratio ({results['sharpe_ratio']:.2f}) –Ω–∏–∂–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ ({min_sharpe})")
        passed_validation = False
    
    if results['win_rate'] < min_win_rate:
        logger.warning(f"‚ö†Ô∏è Win Rate ({results['win_rate']:.2%}) –Ω–∏–∂–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ ({min_win_rate:.2%})")
        passed_validation = False
    
    if abs(results['max_drawdown']) > max_dd:
        logger.warning(f"‚ö†Ô∏è Max Drawdown ({results['max_drawdown']:.2%}) –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç ({max_dd:.2%})")
        passed_validation = False
    
    if passed_validation:
        logger.info("‚úÖ –í—Å–µ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã!")
    else:
        logger.warning("‚ùå –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –Ω–µ –ø—Ä–æ–π–¥–µ–Ω—ã")
    
    logger.end_stage("results_analysis", validation_passed=passed_validation)
    
    return passed_validation

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description='Crypto AI Trading System')
    parser.add_argument('--config', type=str, default='config/config.yaml',
                       help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏')
    parser.add_argument('--mode', type=str, default='full',
                       choices=['data', 'train', 'backtest', 'full', 'demo', 'interactive', 'production', 'inference', 'validate', 'monitor'],
                       help='–†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã')
    parser.add_argument('--model-path', type=str, default=None,
                       help='–ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (–¥–ª—è —Ä–µ–∂–∏–º–∞ backtest)')
    parser.add_argument('--use-improved-model', action='store_true',
                       help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É–ª—É—á—à–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –º–æ–¥–µ–ª–∏ —Å FeatureAttention')
    parser.add_argument('--validate-only', action='store_true',
                       help='–¢–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –±–µ–∑ –∑–∞–ø—É—Å–∫–∞')
    parser.add_argument('--prepare-data', action='store_true',
                       help='–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å prepare_trading_data.py –µ—Å–ª–∏ –Ω–µ—Ç –∫–µ—à–∞')
    
    # –ù–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    parser.add_argument('--target-focus', type=str, default='all',
                       choices=['all', 'returns', 'directions', 'long_profits', 'short_profits', 'risk_metrics'],
                       help='–§–æ–∫—É—Å –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≥—Ä—É–ø–ø–µ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö')
    parser.add_argument('--loss-type', type=str, default='unified',
                       choices=['unified', 'directional', 'profit_aware', 'ensemble'],
                       help='–¢–∏–ø loss —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏')
    parser.add_argument('--ensemble-count', type=int, default=1,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –≤ –∞–Ω—Å–∞–º–±–ª–µ (1 = –±–µ–∑ –∞–Ω—Å–∞–º–±–ª—è)')
    parser.add_argument('--direction-focus', action='store_true',
                       help='–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã')
    parser.add_argument('--large-movement-weight', type=float, default=1.0,
                       help='–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–µ—Å–∞ –¥–ª—è –∫—Ä—É–ø–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π —Ü–µ–Ω—ã (1.0 = –±–µ–∑ –≤–µ—Å–∞)')
    parser.add_argument('--min-movement-threshold', type=float, default=0.005,
                       help='–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–≤–∏–∂–µ–Ω–∏—è –¥–ª—è —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ (0.5%)')
    parser.add_argument('--production', action='store_true',
                       help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å production –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é (config_production.yaml)')
    parser.add_argument('--checkpoint', type=str, default=None,
                       help='–ü—É—Ç—å –∫ checkpoint –¥–ª—è fine-tuning (–Ω–∞–ø—Ä–∏–º–µ—Ä: models_saved/best_model_20250710_150018.pth)')
    
    args = parser.parse_args()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º production —Ä–µ–∂–∏–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    is_production_mode = args.production or args.mode in ['production', 'inference', 'validate']
    
    if is_production_mode:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º ProductionConfig –¥–ª—è production —Ä–µ–∂–∏–º–æ–≤
        config_manager = ProductionConfig(args.config, production_mode=True)
        config = config_manager.config
        logger_name = "CryptoAI-Production"
    else:
        # –û–±—ã—á–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config = load_config(args.config)
        logger_name = "CryptoAI"
    
    # –°–æ–∑–¥–∞–µ–º logger —Å—Ä–∞–∑—É
    logger = get_logger(logger_name)
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–ª–∞–≥ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    if args.use_improved_model:
        config['model']['use_improvements'] = True
        config['model']['feature_attention'] = True
        config['model']['multi_scale_patches'] = True
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è
    # –°–æ–∑–¥–∞–µ–º —Å–µ–∫—Ü–∏—é training –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
    if 'training' not in config:
        config['training'] = {}
    
    if args.target_focus != 'all':
        config['training']['target_focus'] = args.target_focus
        logger.info(f"üéØ –§–æ–∫—É—Å –Ω–∞ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö: {args.target_focus}")
    
    if args.loss_type != 'unified':
        config['training']['loss_type'] = args.loss_type
        logger.info(f"üîß –¢–∏–ø loss —Ñ—É–Ω–∫—Ü–∏–∏: {args.loss_type}")
    
    if args.ensemble_count > 1:
        config['training']['ensemble_count'] = args.ensemble_count
        config['model']['use_ensemble'] = True
        logger.info(f"üé≠ –ê–Ω—Å–∞–º–±–ª—å –∏–∑ {args.ensemble_count} –º–æ–¥–µ–ª–µ–π")
    
    if args.direction_focus:
        config['training']['direction_focus'] = True
        config['model']['task_type'] = 'direction_prediction'
        logger.info("üéØ –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è")
    
    if args.large_movement_weight != 1.0:
        config['training']['large_movement_weight'] = args.large_movement_weight
        logger.info(f"‚öñÔ∏è –í–µ—Å –∫—Ä—É–ø–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π: {args.large_movement_weight}")
    
    if args.min_movement_threshold != 0.005:
        config['training']['min_movement_threshold'] = args.min_movement_threshold
        logger.info(f"üìè –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–≤–∏–∂–µ–Ω–∏—è: {args.min_movement_threshold:.3f} ({args.min_movement_threshold*100:.1f}%)")
    
    logger.info("="*80)
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ Crypto AI Trading System")
    logger.info(f"üìã –†–µ–∂–∏–º: {args.mode}")
    logger.info(f"‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {args.config}")
    if args.production or args.mode == 'production':
        logger.info("üè≠ PRODUCTION MODE - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è")
        logger.info("üìä –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ production —Ä–µ–∂–∏–º–∞:")
        logger.info("   - –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π batch size (512) –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏")
        logger.info("   - –£—Å–∏–ª–µ–Ω–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (dropout=0.5, weight_decay=0.01)")
        logger.info("   - –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–æ—Ä—å–±—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º")
        logger.info("   - –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π –≤–µ—Å direction loss (15.0)")
        logger.info("   - Focal Loss —Å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏")
    if args.use_improved_model:
        logger.info("üî• –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å FeatureAttention")
    logger.info("="*80)
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    if args.validate_only:
        logger.info("üîç –†–µ–∂–∏–º –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏...")
        from utils.config_validator import validate_config
        is_valid = validate_config(config)
        if is_valid:
            logger.info("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤–∞–ª–∏–¥–Ω–∞!")
        else:
            logger.error("‚ùå –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—à–∏–±–∫–∏!")
        return
    
    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
    if args.mode == 'interactive':
        logger.info("üéÆ –ó–∞–ø—É—Å–∫ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞...")
        from run_interactive import run_interactive_mode
        run_interactive_mode(config)
        return
    
    try:
        # –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö —Ä–µ–∂–∏–º–æ–≤
        train_data, val_data, test_data, feature_cols, target_cols = None, None, None, None, None
        train_loader, val_loader, test_loader = None, None, None
        config_updated = config.copy()
        model = None
        model_path = None
        
        if args.mode in ['data', 'train', 'full', 'production', 'backtest']:
            # Production —Ä–µ–∂–∏–º —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–µ–Ω train —Å production –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
            if args.mode == 'production':
                logger.info("üè≠ Production —Ä–µ–∂–∏–º –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")
                
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            train_data, val_data, test_data, feature_cols, target_cols = load_cached_data_if_exists(logger)
            
            if train_data is not None:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                logger.info("üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö —Ä–µ–∂–∏–º–æ–≤")
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ –≤ –∫–æ–Ω—Ñ–∏–≥–µ
                max_symbols = config.get('data', {}).get('max_symbols', None)
                if max_symbols:
                    logger.info(f"üéØ –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–æ {max_symbols} —Å–∏–º–≤–æ–ª–æ–≤")
                    unique_symbols = train_data['symbol'].unique()[:max_symbols]
                    train_data = train_data[train_data['symbol'].isin(unique_symbols)]
                    val_data = val_data[val_data['symbol'].isin(unique_symbols)]
                    test_data = test_data[test_data['symbol'].isin(unique_symbols)]
                    logger.info(f"üìä –ü–æ—Å–ª–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è: train={len(train_data):,}, val={len(val_data):,}, test={len(test_data):,}")
                
                train_loader, val_loader, test_loader, config_updated = create_unified_data_loaders(
                    train_data, val_data, test_data, feature_cols, target_cols, config, logger
                )
            elif args.mode in ['data', 'full']:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –∏ —ç—Ç–æ —Ä–µ–∂–∏–º data/full
                logger.info("üîÑ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ...")
                train_loader, val_loader, test_loader = prepare_data(config, logger)
                config_updated = config  # –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            elif args.mode == 'backtest':
                # –î–ª—è backtest –ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
                logger.info("üîç –†–µ–∂–∏–º backtest - –∏—â–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ...")
                train_loader, val_loader, test_loader, config_updated = create_unified_data_loaders(
                    config, demo_mode=False
                )
            else:
                # –†–µ–∂–∏–º train –±–µ–∑ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                logger.error("‚ùå –†–µ–∂–∏–º train —Ç—Ä–µ–±—É–µ—Ç –Ω–∞–ª–∏—á–∏—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!")
                
                if args.prepare_data:
                    logger.info("üîÑ –ó–∞–ø—É—Å–∫–∞–µ–º prepare_trading_data.py –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–µ—à–∞...")
                    import subprocess
                    result = subprocess.run(
                        ["python", "prepare_trading_data.py", "--config", args.config],
                        capture_output=True,
                        text=True
                    )
                    
                    if result.returncode == 0:
                        logger.info("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã!")
                        # –ü–æ–≤—Ç–æ—Ä–Ω–æ –ø—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–µ—à
                        train_data, val_data, test_data, feature_cols, target_cols = load_cached_data_if_exists(logger)
                        if train_data is not None:
                            train_loader, val_loader, test_loader, config_updated = create_unified_data_loaders(
                                train_data, val_data, test_data, feature_cols, target_cols, config, logger
                            )
                        else:
                            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏")
                            return
                    else:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö: {result.stderr}")
                        return
                else:
                    logger.error("–ó–∞–ø—É—Å—Ç–∏—Ç–µ: python prepare_trading_data.py")
                    logger.error("–ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ–ª–∞–≥ --prepare-data –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–ø—É—Å–∫–∞")
                    return
        
        if args.mode in ['train', 'full', 'production']:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –¥–µ–ª–∞—Ç—å fine-tuning
            if config_updated.get('fine_tuning', {}).get('enabled', False) and args.checkpoint:
                logger.info("üéØ Fine-tuning —Ä–µ–∂–∏–º –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω")
                from training.fine_tuner import create_fine_tuner
                
                # –°–æ–∑–¥–∞–µ–º FineTuner —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º checkpoint
                fine_tuner = create_fine_tuner(config_updated, args.checkpoint)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º learning rate –¥–ª—è fine-tuning
                fine_tuning_lr = config_updated.get('fine_tuning', {}).get('learning_rate', 0.00002)
                for param_group in fine_tuner.optimizer.param_groups:
                    param_group['lr'] = fine_tuning_lr
                
                # –ó–∞–ø—É—Å–∫–∞–µ–º fine-tuning
                fine_tuning_epochs = config_updated.get('fine_tuning', {}).get('epochs', 30)
                best_val_loss = float('inf')
                
                for epoch in range(fine_tuning_epochs):
                    fine_tuner.current_epoch = epoch
                    
                    # Train
                    train_metrics = fine_tuner.train_epoch(train_loader)
                    
                    # Validate
                    val_metrics = fine_tuner.validate(val_loader)
                    
                    # Scheduler step
                    if fine_tuner.scheduler:
                        fine_tuner.scheduler.step(val_metrics['loss'])
                    
                    # Save best model
                    if val_metrics['loss'] < best_val_loss:
                        best_val_loss = val_metrics['loss']
                        model_path = fine_tuner.save_checkpoint(epoch, val_metrics, is_best=True)
                    
                    logger.info(f"Epoch {epoch+1}/{fine_tuning_epochs} - "
                              f"Train Loss: {train_metrics['loss']:.4f}, "
                              f"Val Loss: {val_metrics['loss']:.4f}, "
                              f"Direction Acc: {val_metrics.get('direction_accuracy', 0):.3f}")
                
                model = fine_tuner.model
                
            else:
                # –û–±—ã—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
                model, model_path, train_loader = train_model(config_updated, train_loader, val_loader, logger)
        
        if args.mode in ['backtest', 'full']:
            if args.mode == 'backtest':
                if not args.model_path:
                    logger.error("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å --model-path –¥–ª—è —Ä–µ–∂–∏–º–∞ backtest")
                    return
                
                logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {args.model_path}")
                
                # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
                checkpoint = torch.load(args.model_path, map_location='cuda' if torch.cuda.is_available() else 'cpu')
                
                # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
                from models.patchtst_unified import UnifiedPatchTSTForTrading
                model = UnifiedPatchTSTForTrading(config_updated)
                
                # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤
                if 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    model.load_state_dict(checkpoint)
                
                model.eval()
                logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                
            results = backtest_strategy(config, model, test_loader, train_loader, logger)
            
            validation_passed = analyze_results(config, results, logger)
        
        if args.mode == 'demo':
            logger.info("üéØ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ä–µ–∂–∏–º - —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î")
            from data.data_loader import CryptoDataLoader
            
            data_loader = CryptoDataLoader(config)
            available_symbols = data_loader.get_available_symbols()
            
            logger.info(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î —É—Å–ø–µ—à–Ω–æ")
            logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(available_symbols)} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"üîç –ü–µ—Ä–≤—ã–µ 10 —Å–∏–º–≤–æ–ª–æ–≤: {available_symbols[:10]}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–µ–±–æ–ª—å—à–æ–π –æ–±—Ä–∞–∑–µ—Ü –¥–∞–Ω–Ω—ã—Ö
            sample_data = data_loader.load_data(
                symbols=available_symbols[:2],
                start_date="2025-06-01",
                end_date="2025-06-16"
            )
            
            logger.info(f"üìà –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(sample_data)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏")
        
        # Production-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ä–µ–∂–∏–º—ã
        if args.mode == 'inference':
            # Production inference
            if not args.model_path:
                logger.error("‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å --model-path –¥–ª—è —Ä–µ–∂–∏–º–∞ inference")
                return
            
            logger.info("üîÆ –ó–∞–ø—É—Å–∫ production inference...")
            
            inference = ProductionInference(args.model_path, config, logger)
            
            # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∑–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            # –î–ª—è –ø—Ä–∏–º–µ—Ä–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            test_data = torch.randn(1, config['model']['context_window'], config['model']['input_size'])
            
            results = inference.predict(test_data)
            
            if 'error' not in results:
                logger.info("‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ:")
                logger.info(f"   Future Returns: {results['future_returns'].numpy()}")
                if 'direction_classes' in results:
                    classes = ['LONG', 'SHORT', 'FLAT']
                    for i, cls in enumerate(results['direction_classes'][0]):
                        logger.info(f"   Direction {i+1}: {classes[cls]}")
            else:
                logger.error("‚ùå –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
        
        if args.mode == 'validate':
            # –û—Ç–¥–µ–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏
            if not args.model_path:
                logger.error("‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å --model-path –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
                return
            
            logger.info("üîç –ó–∞–ø—É—Å–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            from models.patchtst_unified import create_unified_model
            model = create_unified_model(config)
            
            checkpoint = torch.load(args.model_path)
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            else:
                model.load_state_dict(checkpoint)
            
            model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            if val_loader is None:
                from data.precomputed_dataset import create_precomputed_loaders
                _, val_loader, _ = create_precomputed_loaders(config, logger)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            validator = ModelValidator(config, logger)
            if validator.validate_model(model, val_loader):
                logger.info("‚úÖ –ú–æ–¥–µ–ª—å –ø—Ä–æ—à–ª–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—é!")
            else:
                logger.error("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –ø—Ä–æ—à–ª–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—é!")
        
        if args.mode == 'monitor':
            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è
            logger.info("üìä –ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞...")
            
            import subprocess
            subprocess.run(['python', 'monitor_training.py'])
        
        # Production —Ä–µ–∂–∏–º —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
        if args.mode == 'production' and model is not None:
            logger.info("‚úÖ –ó–∞–ø—É—Å–∫ production –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è...")
            validator = ModelValidator(config, logger)
            
            if validator.validate_model(model, val_loader):
                logger.info("üéâ –ú–æ–¥–µ–ª—å –ø—Ä–æ—à–ª–∞ production –≤–∞–ª–∏–¥–∞—Ü–∏—é!")
                logger.info(f"üì¶ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é: {model_path}")
            else:
                logger.error("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –ø—Ä–æ—à–ª–∞ production –≤–∞–ª–∏–¥–∞—Ü–∏—é!")
                logger.error("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        
        logger.info("="*80)
        logger.info("‚úÖ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        logger.info("="*80)
        
    except Exception as e:
        logger.log_error(e, "main")
        logger.critical("‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞! –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ.")
        raise

if __name__ == "__main__":
    main()