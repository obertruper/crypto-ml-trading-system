#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import os
import json
import pickle
import time
from datetime import datetime
import psycopg2
import logging
import yaml
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.utils.class_weight import compute_class_weight
import warnings
from datetime import datetime

warnings.filterwarnings('ignore')
np.random.seed(42)
tf.random.set_seed(42)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
log_dir = f'logs/training_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
os.makedirs(log_dir, exist_ok=True)

# –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
physical_devices = tf.config.list_physical_devices('GPU')
if len(physical_devices) > 0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
    logger.info(f"üñ•Ô∏è GPU –¥–æ—Å—Ç—É–ø–µ–Ω: {physical_devices[0].name}")
else:
    logger.info("‚ö†Ô∏è GPU –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")

# –§–∞–π–ª–æ–≤—ã–π –ª–æ–≥–≥–µ—Ä
file_handler = logging.FileHandler(f'{log_dir}/training.log', encoding='utf-8')
file_handler.setLevel(logging.INFO)
file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
file_handler.setFormatter(file_formatter)

# –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π –ª–æ–≥–≥–µ—Ä
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
console_handler.setFormatter(console_formatter)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ª–æ–≥–≥–µ—Ä–∞
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
logger.addHandler(file_handler)
logger.addHandler(console_handler)


class DetailedLoggingCallback(keras.callbacks.Callback):
    """–ö–∞—Å—Ç–æ–º–Ω—ã–π callback –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    def __init__(self, log_dir, model_name):
        super().__init__()
        self.log_dir = log_dir
        self.model_name = model_name
        self.epoch_start_time = None
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        self.metrics_file = open(f'{log_dir}/{model_name}_metrics.csv', 'w')
        self.metrics_file.write('epoch,loss,val_loss,accuracy,val_accuracy,precision,val_precision,recall,val_recall,auc,val_auc,lr,time\n')
        
    def on_epoch_begin(self, epoch, logs=None):
        self.epoch_start_time = time.time()
        logger.info(f"\n{'='*60}")
        logger.info(f"üîÑ –≠–ø–æ—Ö–∞ {epoch + 1} –Ω–∞—á–∞–ª–∞—Å—å –¥–ª—è {self.model_name}")
        logger.info(f"   Learning rate: {self.model.optimizer.learning_rate.numpy():.6f}")
        
    def on_epoch_end(self, epoch, logs=None):
        epoch_time = time.time() - self.epoch_start_time
        
        # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        logger.info(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–ø–æ—Ö–∏ {epoch + 1} ({self.model_name}):")
        logger.info(f"   ‚è±Ô∏è  –í—Ä–µ–º—è: {epoch_time:.2f} —Å–µ–∫")
        logger.info(f"   üìâ Loss: {logs.get('loss', 0):.4f} (train) | {logs.get('val_loss', 0):.4f} (val)")
        logger.info(f"   üéØ Accuracy: {logs.get('accuracy', 0):.4f} (train) | {logs.get('val_accuracy', 0):.4f} (val)")
        logger.info(f"   üéØ Precision: {logs.get('precision', 0):.4f} (train) | {logs.get('val_precision', 0):.4f} (val)")
        logger.info(f"   üéØ Recall: {logs.get('recall', 0):.4f} (train) | {logs.get('val_recall', 0):.4f} (val)")
        logger.info(f"   üìà AUC: {logs.get('auc', 0):.4f} (train) | {logs.get('val_auc', 0):.4f} (val)")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
        self.metrics_file.write(f"{epoch + 1},{logs.get('loss', 0):.4f},{logs.get('val_loss', 0):.4f},"
                               f"{logs.get('accuracy', 0):.4f},{logs.get('val_accuracy', 0):.4f},"
                               f"{logs.get('precision', 0):.4f},{logs.get('val_precision', 0):.4f},"
                               f"{logs.get('recall', 0):.4f},{logs.get('val_recall', 0):.4f},"
                               f"{logs.get('auc', 0):.4f},{logs.get('val_auc', 0):.4f},"
                               f"{self.model.optimizer.learning_rate.numpy():.6f},{epoch_time:.2f}\n")
        self.metrics_file.flush()
        
    def on_train_batch_begin(self, batch, logs=None):
        if batch % 100 == 0:
            logger.debug(f"   –ë–∞—Ç—á {batch} –Ω–∞—á–∞–ª—Å—è...")
            
    def on_train_batch_end(self, batch, logs=None):
        if batch % 100 == 0 and logs:
            logger.debug(f"   –ë–∞—Ç—á {batch}: loss={logs.get('loss', 0):.4f}")
            
    def on_train_end(self, logs=None):
        self.metrics_file.close()
        logger.info(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ {self.model_name} –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        logger.info(f"üìä –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {self.log_dir}/{self.model_name}_metrics.csv")


class AdvancedTrainer:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏"""
    
    def __init__(self, config_path='config.yaml'):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ë–î
        self.db_config = self.config['database'].copy()
        if not self.db_config.get('password'):
            self.db_config.pop('password', None)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ)
        self.sequence_length = self.config['model']['sequence_length']
        self.prediction_horizon = self.config['model']['prediction_horizon']
        self.batch_size = self.config['model']['batch_size']
        self.epochs = 100  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
        self.learning_rate = 0.001  # –ù–∞—á–∞–ª—å–Ω—ã–π learning rate
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.use_attention = True
        self.use_residual = True
        self.dropout_rate = 0.2
        self.l2_regularization = 1e-4
        self.gradient_clip = 1.0
        
        self.scaler = RobustScaler()
        self.models = {}
        self.feature_columns = None
        self.log_dir = log_dir  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π log_dir
        
    def connect_db(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î"""
        self.conn = psycopg2.connect(**self.db_config)
        logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω –∫ PostgreSQL")
        
    def create_focal_loss(self, alpha=0.9, gamma=2.0):
        """
        Focal Loss –¥–ª—è –±–æ—Ä—å–±—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤
        alpha=0.9 –¥–∞—ë—Ç –±–æ–ª—å—à–∏–π –≤–µ—Å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º –ø—Ä–∏–º–µ—Ä–∞–º (minority class)
        gamma=2.0 —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö
        """
        def focal_loss(y_true, y_pred):
            epsilon = tf.keras.backend.epsilon()
            y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)
            
            # –í—ã—á–∏—Å–ª—è–µ–º focal loss
            p_t = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)
            alpha_factor = tf.ones_like(y_true) * alpha
            alpha_t = tf.where(tf.equal(y_true, 1), alpha_factor, 1 - alpha_factor)
            cross_entropy = -tf.math.log(p_t)
            weight = alpha_t * tf.pow((1 - p_t), gamma)
            loss = weight * cross_entropy
            
            return tf.reduce_mean(loss)
        
        return focal_loss
    
    def create_advanced_model(self, input_shape, name):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏"""
        
        inputs = layers.Input(shape=input_shape, name='input')
        
        # Feature extraction —Å residual connections
        x = layers.LSTM(256, return_sequences=True, 
                       kernel_regularizer=keras.regularizers.l2(self.l2_regularization),
                       recurrent_regularizer=keras.regularizers.l2(self.l2_regularization))(inputs)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(self.dropout_rate)(x)
        
        # Residual block 1
        residual = x
        x = layers.LSTM(128, return_sequences=True,
                       kernel_regularizer=keras.regularizers.l2(self.l2_regularization),
                       recurrent_regularizer=keras.regularizers.l2(self.l2_regularization))(x)
        x = layers.BatchNormalization()(x)
        
        if self.use_residual:
            # –ü—Ä–∏–≤–æ–¥–∏–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–ª—è residual connection
            residual = layers.Dense(128)(residual)
            x = layers.Add()([x, residual])
        
        x = layers.Dropout(self.dropout_rate)(x)
        
        # Attention mechanism
        if self.use_attention:
            attention = layers.MultiHeadAttention(
                num_heads=4, 
                key_dim=16,
                dropout=self.dropout_rate
            )(x, x)
            x = layers.Add()([x, attention])
            x = layers.LayerNormalization()(x)
        
        # Global features
        avg_pool = layers.GlobalAveragePooling1D()(x)
        max_pool = layers.GlobalMaxPooling1D()(x)
        
        # Combine features
        combined = layers.Concatenate()([avg_pool, max_pool])
        
        # Deep layers with batch norm
        x = layers.Dense(128, kernel_regularizer=keras.regularizers.l2(self.l2_regularization))(combined)
        x = layers.BatchNormalization()(x)
        x = layers.Activation('relu')(x)
        x = layers.Dropout(self.dropout_rate * 1.5)(x)
        
        x = layers.Dense(64, kernel_regularizer=keras.regularizers.l2(self.l2_regularization))(x)
        x = layers.BatchNormalization()(x)
        x = layers.Activation('relu')(x)
        x = layers.Dropout(self.dropout_rate)(x)
        
        x = layers.Dense(32, kernel_regularizer=keras.regularizers.l2(self.l2_regularization))(x)
        x = layers.BatchNormalization()(x)
        x = layers.Activation('relu')(x)
        x = layers.Dropout(self.dropout_rate * 0.5)(x)
        
        # Output
        outputs = layers.Dense(1, activation='sigmoid', name='output')(x)
        
        model = keras.Model(inputs=inputs, outputs=outputs, name=name)
        
        # Compile —Å advanced –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º
        optimizer = keras.optimizers.Adam(
            learning_rate=self.learning_rate,
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-7,
            clipnorm=self.gradient_clip
        )
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º focal loss –¥–ª—è –ª—É—á—à–µ–π —Ä–∞–±–æ—Ç—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º
        model.compile(
            optimizer=optimizer,
            loss=self.create_focal_loss(alpha=0.9, gamma=2.0),
            metrics=[
                'accuracy',
                keras.metrics.Precision(name='precision'),
                keras.metrics.Recall(name='recall'),
                keras.metrics.AUC(name='auc')
            ]
        )
        
        return model
    
    def load_and_prepare_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        logger.info("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL...")
        logger.info("üîç –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∑–∞–ø—Ä–æ—Å –∫ –ë–î –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ñ—å—é—á–µ—Ä—Å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        start_time = time.time()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Ñ—å—é—á–µ—Ä—Å–æ–≤
        query = """
        SELECT 
            p.symbol, p.timestamp, p.datetime,
            p.technical_indicators,
            p.buy_profit_target, p.buy_loss_target,
            p.sell_profit_target, p.sell_loss_target
        FROM processed_market_data p
        JOIN raw_market_data r ON p.raw_data_id = r.id
        WHERE p.technical_indicators IS NOT NULL
          AND r.market_type = 'futures'
        ORDER BY p.symbol, p.timestamp
        """
        
        df = pd.read_sql_query(query, self.conn)
        load_time = time.time() - start_time
        
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –∑–∞ {load_time:.2f} —Å–µ–∫")
        logger.info(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤: {df['symbol'].nunique()}")
        logger.info(f"üìÖ –ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö: {df['datetime'].min()} - {df['datetime'].max()}")
        
        if len(df) == 0:
            return None
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        if len(df) > 0:
            sample_indicators = df.iloc[0]['technical_indicators']
            self.feature_columns = list(sample_indicators.keys())
            
            features = []
            for _, row in df.iterrows():
                indicators = row['technical_indicators']
                feature_values = []
                for col in self.feature_columns:
                    val = indicators.get(col, 0)
                    if val is None or np.isnan(val) or np.isinf(val):
                        val = 0
                    feature_values.append(float(val))
                features.append(feature_values)
            
            X = np.array(features)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X = np.nan_to_num(X, nan=0.0, posinf=1e6, neginf=-1e6)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            X_scaled = self.scaler.fit_transform(X)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            scaler_stats = {
                'mean': self.scaler.center_.tolist() if hasattr(self.scaler, 'center_') else None,
                'scale': self.scaler.scale_.tolist() if hasattr(self.scaler, 'scale_') else None,
                'features': self.feature_columns
            }
            
            with open('trained_model/scaler_stats.json', 'w') as f:
                json.dump(scaler_stats, f, indent=2)
            
            return {
                'X': X_scaled,
                'y_buy_profit': df['buy_profit_target'].values,
                'y_buy_loss': df['buy_loss_target'].values,
                'y_sell_profit': df['sell_profit_target'].values,
                'y_sell_loss': df['sell_loss_target'].values,
                'symbols': df['symbol'].values,
                'timestamps': df['timestamp'].values
            }
        
        return None
    
    def create_sequences_advanced(self, X, y, symbols):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Å —É—á–µ—Ç–æ–º —Å–∏–º–≤–æ–ª–æ–≤"""
        sequences = []
        labels = []
        
        unique_symbols = np.unique(symbols)
        
        for symbol in unique_symbols:
            symbol_mask = symbols == symbol
            symbol_indices = np.where(symbol_mask)[0]
            
            if len(symbol_indices) < self.sequence_length:
                continue
            
            # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è —ç—Ç–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
            X_symbol = X[symbol_indices]
            y_symbol = y[symbol_indices]
            
            for i in range(len(X_symbol) - self.sequence_length):
                sequences.append(X_symbol[i:i + self.sequence_length])
                labels.append(y_symbol[i + self.sequence_length])
        
        return np.array(sequences), np.array(labels)
    
    def balance_dataset(self, X, y, strategy='oversample'):
        """–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —á–µ—Ä–µ–∑ –æ–≤–µ—Ä—Å—ç–º–ø–ª–∏–Ω–≥"""
        from sklearn.utils import resample
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –∫–ª–∞—Å—Å—ã
        pos_indices = np.where(y == 1)[0]
        neg_indices = np.where(y == 0)[0]
        
        logger.info(f"   –ò—Å—Ö–æ–¥–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {len(pos_indices)} pos / {len(neg_indices)} neg")
        
        if strategy == 'oversample':
            # –û–≤–µ—Ä—Å—ç–º–ø–ª–∏–Ω–≥ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
            n_samples = len(neg_indices)
            pos_indices_resampled = resample(pos_indices, 
                                            replace=True, 
                                            n_samples=n_samples,
                                            random_state=42)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å—ã
            all_indices = np.concatenate([neg_indices, pos_indices_resampled])
            np.random.shuffle(all_indices)
            
            X_balanced = X[all_indices]
            y_balanced = y[all_indices]
            
            logger.info(f"   –ü–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: {sum(y_balanced)} pos / {len(y_balanced) - sum(y_balanced)} neg")
            
            return X_balanced, y_balanced
        else:
            return X, y
    
    def train_with_callbacks(self, model, X_train, y_train, X_val, y_val, model_name):
        """–û–±—É—á–µ–Ω–∏–µ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ callbacks"""
        
        logger.info(f"\n{'='*70}")
        logger.info(f"üöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ò: {model_name}")
        logger.info(f"{'='*70}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        logger.info(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è:")
        logger.info(f"   –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {len(X_train)}")
        logger.info(f"   –†–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(X_val)}")
        logger.info(f"   –§–æ—Ä–º–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {X_train.shape}")
        logger.info(f"   –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ (train): {sum(y_train)} ({sum(y_train)/len(y_train)*100:.1f}%)")
        logger.info(f"   –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ (val): {sum(y_val)} ({sum(y_val)/len(y_val)*100:.1f}%)")
        
        # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏
        logger.info(f"\n‚öñÔ∏è –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏:")
        X_train_balanced, y_train_balanced = self.balance_dataset(X_train, y_train, strategy='oversample')
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ (—Ç–µ–ø–µ—Ä—å –æ–Ω–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –±–æ–ª–µ–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω—ã)
        class_weights = compute_class_weight(
            'balanced',
            classes=np.unique(y_train_balanced),
            y=y_train_balanced
        )
        class_weight_dict = dict(enumerate(class_weights))
        
        logger.info(f"\n‚öñÔ∏è –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏:")
        logger.info(f"   –í–µ—Å –∫–ª–∞—Å—Å–∞ 0 (–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π): {class_weight_dict.get(0, 1.0):.2f}")
        logger.info(f"   –í–µ—Å –∫–ª–∞—Å—Å–∞ 1 (–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π): {class_weight_dict.get(1, 1.0):.2f}")
        
        # Callbacks
        callbacks = [
            # –ù–∞—à –¥–µ—Ç–∞–ª—å–Ω—ã–π –ª–æ–≥–≥–µ—Ä
            DetailedLoggingCallback(self.log_dir, model_name),
            # Early stopping —Å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º –ª—É—á—à–∏—Ö –≤–µ—Å–æ–≤ (—Å–ª–µ–¥–∏–º –∑–∞ recall)
            keras.callbacks.EarlyStopping(
                monitor='val_recall',
                patience=15,
                restore_best_weights=True,
                verbose=1,
                mode='max'
            ),
            
            # –£–º–µ–Ω—å—à–µ–Ω–∏–µ learning rate –ø—Ä–∏ –∑–∞—Å—Ç–æ–µ
            keras.callbacks.ReduceLROnPlateau(
                monitor='val_recall',
                factor=0.5,
                patience=7,
                min_lr=1e-7,
                verbose=1,
                mode='max'
            ),
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ (–ø–æ recall)
            keras.callbacks.ModelCheckpoint(
                filepath=f'trained_model/{model_name}_best.h5',
                monitor='val_recall',
                mode='max',
                save_best_only=True,
                verbose=1
            ),
            
            # TensorBoard –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
            keras.callbacks.TensorBoard(
                log_dir=f'logs/{model_name}_{datetime.now().strftime("%Y%m%d-%H%M%S")}',
                histogram_freq=1,
                write_graph=True,
                update_freq='epoch'
            ),
            
            # –ö–∞—Å—Ç–æ–º–Ω—ã–π callback –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            keras.callbacks.LambdaCallback(
                on_epoch_end=lambda epoch, logs: logger.info(
                    f"Epoch {epoch + 1}: loss={logs['loss']:.4f}, "
                    f"val_loss={logs['val_loss']:.4f}, "
                    f"val_auc={logs['val_auc']:.4f}"
                )
            )
        ]
        
        # –û–±—É—á–µ–Ω–∏–µ —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        history = model.fit(
            X_train_balanced, y_train_balanced,
            validation_data=(X_val, y_val),
            epochs=self.epochs,
            batch_size=self.batch_size,
            class_weight=class_weight_dict,
            callbacks=callbacks,
            verbose=1
        )
        
        return history
    
    def find_optimal_threshold(self, y_true, y_pred_proba):
        """–ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞ –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ F1-score"""
        from sklearn.metrics import precision_recall_curve
        
        precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)
        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
        
        # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Ä–æ–≥ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º F1
        optimal_idx = np.argmax(f1_scores)
        optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5
        optimal_f1 = f1_scores[optimal_idx]
        
        logger.info(f"   üéØ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {optimal_threshold:.3f} (F1={optimal_f1:.3f})")
        
        return optimal_threshold
    
    def evaluate_model(self, model, X_test, y_test, model_name):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏"""
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred = model.predict(X_test)
        
        # –ù–∞—Ö–æ–¥–∏–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
        optimal_threshold = self.find_optimal_threshold(y_test, y_pred)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º
        y_pred_binary = (y_pred > optimal_threshold).astype(int).flatten()
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred_binary),
            'precision': precision_score(y_test, y_pred_binary, zero_division=0),
            'recall': recall_score(y_test, y_pred_binary, zero_division=0),
            'f1': f1_score(y_test, y_pred_binary, zero_division=0),
            'auc': roc_auc_score(y_test, y_pred) if len(np.unique(y_test)) > 1 else 0,
            'optimal_threshold': optimal_threshold
        }
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        tp = np.sum((y_test == 1) & (y_pred_binary == 1))
        tn = np.sum((y_test == 0) & (y_pred_binary == 0))
        fp = np.sum((y_test == 0) & (y_pred_binary == 1))
        fn = np.sum((y_test == 1) & (y_pred_binary == 0))
        
        metrics['true_positives'] = int(tp)
        metrics['true_negatives'] = int(tn)
        metrics['false_positives'] = int(fp)
        metrics['false_negatives'] = int(fn)
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç—å
        metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0
        
        logger.info(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ {model_name}:")
        logger.info(f"   Accuracy: {metrics['accuracy']:.4f}")
        logger.info(f"   Precision: {metrics['precision']:.4f}")
        logger.info(f"   Recall: {metrics['recall']:.4f}")
        logger.info(f"   F1 Score: {metrics['f1']:.4f}")
        logger.info(f"   AUC: {metrics['auc']:.4f}")
        logger.info(f"   Specificity: {metrics['specificity']:.4f}")
        logger.info(f"   –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {metrics['optimal_threshold']:.3f}")
        
        return metrics
    
    def train(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è"""
        logger.info("\n" + "="*80)
        logger.info("üöÄ –ù–ê–ß–ê–õ–û –ü–†–û–¶–ï–°–°–ê –û–ë–£–ß–ï–ù–ò–Ø ML –ú–û–î–ï–õ–ò –î–õ–Ø –ö–†–ò–ü–¢–û–¢–†–ï–ô–î–ò–ù–ì–ê")
        logger.info("="*80)
        logger.info(f"üìÖ –î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ª–æ–≥–æ–≤: {self.log_dir}")
        
        try:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            os.makedirs('trained_model', exist_ok=True)
            os.makedirs('logs', exist_ok=True)
            os.makedirs('plots', exist_ok=True)
            
            logger.info("\nüìä –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø:")
            logger.info(f"   Sequence length: {self.sequence_length}")
            logger.info(f"   Batch size: {self.batch_size}")
            logger.info(f"   Epochs: {self.epochs}")
            logger.info(f"   Learning rate: {self.learning_rate}")
            logger.info(f"   Use attention: {self.use_attention}")
            logger.info(f"   Use residual: {self.use_residual}")
            
            # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –ë–î
            logger.info("\nüîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL...")
            self.connect_db()
            logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            data = self.load_and_prepare_data()
            if not data:
                logger.error("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                return
            
            # –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            model_configs = [
                ('buy_profit_model', data['y_buy_profit']),
                ('sell_profit_model', data['y_sell_profit'])
            ]
            
            results = {}
            
            for model_name, y_target in model_configs:
                logger.info(f"\n{'='*60}")
                logger.info(f"–û–±—É—á–µ–Ω–∏–µ {model_name}")
                logger.info(f"{'='*60}")
                
                # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                logger.info(f"\nüîÑ –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è {model_name}...")
                seq_start_time = time.time()
                
                X_seq, y_seq = self.create_sequences_advanced(
                    data['X'], y_target, data['symbols']
                )
                
                seq_time = time.time() - seq_start_time
                
                if len(X_seq) == 0:
                    logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {model_name}")
                    continue
                
                logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(X_seq)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∑–∞ {seq_time:.2f} —Å–µ–∫")
                logger.info(f"   –§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {X_seq.shape}")
                logger.info(f"   –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {sum(y_seq)} ({sum(y_seq)/len(y_seq)*100:.1f}%)")
                logger.info(f"   –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(y_seq) - sum(y_seq)} ({(len(y_seq) - sum(y_seq))/len(y_seq)*100:.1f}%)")
                
                # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test
                n_samples = len(X_seq)
                train_end = int(n_samples * 0.7)
                val_end = int(n_samples * 0.85)
                
                X_train = X_seq[:train_end]
                y_train = y_seq[:train_end]
                X_val = X_seq[train_end:val_end]
                y_val = y_seq[train_end:val_end]
                X_test = X_seq[val_end:]
                y_test = y_seq[val_end:]
                
                # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
                model = self.create_advanced_model(X_seq.shape[1:], model_name)
                logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –º–æ–¥–µ–ª—å —Å {model.count_params():,} –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏")
                
                # –û–±—É—á–∞–µ–º
                history = self.train_with_callbacks(
                    model, X_train, y_train, X_val, y_val, model_name
                )
                
                # –û—Ü–µ–Ω–∏–≤–∞–µ–º
                test_metrics = self.evaluate_model(model, X_test, y_test, model_name)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                results[model_name] = {
                    'history': history.history,
                    'metrics': test_metrics,
                    'model': model
                }
                
                self.models[model_name] = model
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            logger.info("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
            self.save_all_models(results)
            
            # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            logger.info("\n" + "="*80)
            logger.info("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–£–ß–ï–ù–ò–Ø")
            logger.info("="*80)
            
            for model_name, result in results.items():
                metrics = result['metrics']
                logger.info(f"\nü§ñ {model_name}:")
                logger.info(f"   Test Accuracy: {metrics['accuracy']:.4f}")
                logger.info(f"   Test Precision: {metrics['precision']:.4f}")
                logger.info(f"   Test Recall: {metrics['recall']:.4f}")
                logger.info(f"   Test F1: {metrics['f1']:.4f}")
                logger.info(f"   Test AUC: {metrics['auc']:.4f}")
                logger.info(f"   –≠–ø–æ—Ö –æ–±—É—á–µ–Ω–æ: {len(result['history']['loss'])}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
            report_path = f'{self.log_dir}/training_report.txt'
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(f"–û–¢–ß–ï–¢ –û–ë –û–ë–£–ß–ï–ù–ò–ò\n")
                f.write(f"{'='*50}\n")
                f.write(f"–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:\n")
                f.write(f"  - Sequence length: {self.sequence_length}\n")
                f.write(f"  - Batch size: {self.batch_size}\n")
                f.write(f"  - Epochs: {self.epochs}\n")
                f.write(f"  - Learning rate: {self.learning_rate}\n")
                f.write(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:\n")
                for model_name, result in results.items():
                    f.write(f"\n{model_name}:\n")
                    for metric, value in result['metrics'].items():
                        f.write(f"  - {metric}: {value:.4f}\n")
            
            logger.info(f"\nüìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
            logger.info(f"üìÅ –í—Å–µ –ª–æ–≥–∏ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {self.log_dir}")
            
            logger.info("\n" + "="*80)
            logger.info("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!")
            logger.info("="*80)
            
        except Exception as e:
            logger.error(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
            logger.error(f"–¢—Ä–µ–π—Å–±–µ–∫:", exc_info=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—à–∏–±–∫—É –≤ —Ñ–∞–π–ª
            error_path = f'{self.log_dir}/error.log'
            with open(error_path, 'w', encoding='utf-8') as f:
                f.write(f"–û—à–∏–±–∫–∞: {e}\n")
                import traceback
                f.write(traceback.format_exc())
            
            raise
        finally:
            if hasattr(self, 'conn'):
                self.conn.close()
                logger.info("üì§ –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î –∑–∞–∫—Ä—ã—Ç–æ")
    
    def save_all_models(self, results):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        save_start_time = time.time()
        
        logger.info("\nüì¶ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è...")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
        for name, model in self.models.items():
            model_path = f'trained_model/{name}_advanced.h5'
            model.save(model_path)
            logger.info(f"   ‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path} ({os.path.getsize(model_path) / 1024 / 1024:.2f} MB)")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler
        with open('trained_model/scaler_advanced.pkl', 'wb') as f:
            pickle.dump(self.scaler, f)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            'type': 'advanced_lstm_attention',
            'sequence_length': self.sequence_length,
            'features': self.feature_columns,
            'use_attention': self.use_attention,
            'use_residual': self.use_residual,
            'created_at': datetime.now().isoformat(),
            'results': {
                name: {
                    'metrics': result['metrics'],
                    'epochs_trained': len(result['history']['loss']),
                    'optimal_threshold': result['metrics'].get('optimal_threshold', 0.5)
                }
                for name, result in results.items()
            }
        }
        
        with open('trained_model/metadata_advanced.json', 'w') as f:
            json.dump(metadata, f, indent=2)
        
        # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
        self.create_training_visualizations(results)
        
        save_time = time.time() - save_start_time
        logger.info(f"\n‚úÖ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –∑–∞ {save_time:.2f} —Å–µ–∫")
        logger.info(f"   üìÅ –ú–æ–¥–µ–ª–∏: trained_model/")
        logger.info(f"   üìä –ì—Ä–∞—Ñ–∏–∫–∏: plots/")
        logger.info(f"   üìù –õ–æ–≥–∏: {self.log_dir}/")
    
    def create_training_visualizations(self, results):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
        import matplotlib.pyplot as plt
        
        logger.info("\nüìà –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")
        
        # –°–æ–∑–¥–∞–µ–º –æ–±—â–∏–π –≥—Ä–∞—Ñ–∏–∫ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('–ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫—Ä–∏–ø—Ç–æ—Ç—Ä–µ–π–¥–∏–Ω–≥–∞', fontsize=16)
        
        colors = ['blue', 'green', 'red', 'orange']
        
        for idx, (model_name, result) in enumerate(results.items()):
            history = result['history']
            color = colors[idx % len(colors)]
            
            # Loss –≥—Ä–∞—Ñ–∏–∫
            axes[0, 0].plot(history['loss'], label=f'{model_name} (train)', 
                          color=color, alpha=0.8)
            axes[0, 0].plot(history['val_loss'], label=f'{model_name} (val)', 
                          color=color, linestyle='--', alpha=0.8)
            
            # Accuracy –≥—Ä–∞—Ñ–∏–∫
            axes[0, 1].plot(history['accuracy'], label=f'{model_name} (train)', 
                          color=color, alpha=0.8)
            axes[0, 1].plot(history['val_accuracy'], label=f'{model_name} (val)', 
                          color=color, linestyle='--', alpha=0.8)
            
            # AUC –≥—Ä–∞—Ñ–∏–∫
            axes[1, 0].plot(history['auc'], label=f'{model_name} (train)', 
                          color=color, alpha=0.8)
            axes[1, 0].plot(history['val_auc'], label=f'{model_name} (val)', 
                          color=color, linestyle='--', alpha=0.8)
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            metrics = result['metrics']
            axes[1, 1].bar(idx * 5 + np.arange(4), 
                         [metrics['accuracy'], metrics['precision'], 
                          metrics['recall'], metrics['f1']],
                         label=model_name, color=color, alpha=0.8)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥—Ä–∞—Ñ–∏–∫–æ–≤
        axes[0, 0].set_title('Loss –ø–æ —ç–ø–æ—Ö–∞–º')
        axes[0, 0].set_xlabel('–≠–ø–æ—Ö–∞')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend(loc='upper right', fontsize=8)
        axes[0, 0].grid(True, alpha=0.3)
        
        axes[0, 1].set_title('Accuracy –ø–æ —ç–ø–æ—Ö–∞–º')
        axes[0, 1].set_xlabel('–≠–ø–æ—Ö–∞')
        axes[0, 1].set_ylabel('Accuracy')
        axes[0, 1].legend(loc='lower right', fontsize=8)
        axes[0, 1].grid(True, alpha=0.3)
        
        axes[1, 0].set_title('AUC –ø–æ —ç–ø–æ—Ö–∞–º')
        axes[1, 0].set_xlabel('–≠–ø–æ—Ö–∞')
        axes[1, 0].set_ylabel('AUC')
        axes[1, 0].legend(loc='lower right', fontsize=8)
        axes[1, 0].grid(True, alpha=0.3)
        
        axes[1, 1].set_title('–§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (test)')
        axes[1, 1].set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ')
        axes[1, 1].set_xticks(np.arange(len(results)) * 5 + 1.5)
        axes[1, 1].set_xticklabels(['Acc', 'Prec', 'Rec', 'F1'] * len(results), 
                                   rotation=45, fontsize=8)
        axes[1, 1].legend(loc='upper right', fontsize=8)
        axes[1, 1].grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        plot_path = f'plots/training_summary_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"   ‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {plot_path}")


if __name__ == "__main__":
    trainer = AdvancedTrainer()
    trainer.train()