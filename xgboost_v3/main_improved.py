#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π –≥–ª–∞–≤–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è XGBoost v3.0
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —É–ø—Ä–æ—â–µ–Ω–Ω—ã–µ —Ç–∞—Ä–≥–µ—Ç—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import argparse
import logging
import time
from pathlib import Path
from typing import Dict, List, Tuple
import pandas as pd
import numpy as np
import gc

from config import Config
from data import DataLoader, DataPreprocessor, FeatureEngineer
from data.target_calculator import TargetCalculator
from data.cacher import CacheManager
from models import XGBoostTrainer, EnsembleModel, OptunaOptimizer, DataBalancer
from utils import LoggingManager, ReportGenerator
from utils.feature_selector import FeatureSelector

logger = logging.getLogger(__name__)


class ImprovedXGBoostPipeline:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Å —É–ø—Ä–æ—â–µ–Ω–Ω—ã–º–∏ —Ç–∞—Ä–≥–µ—Ç–∞–º–∏"""
    
    def __init__(self, config: Config):
        self.config = config
        self.log_manager = LoggingManager(config)
        self.cache_manager = CacheManager(config)
        self.report_generator = ReportGenerator(config)
        self.target_calculator = TargetCalculator(
            lookahead_bars=4,  # 1 —á–∞—Å –¥–ª—è 15-–º–∏–Ω –¥–∞–Ω–Ω—ã—Ö
            price_threshold=0.5  # –ø–æ—Ä–æ–≥ 0.5% –¥–ª—è threshold_binary
        )
        
    def run(self, target_type: str = "threshold_binary", optimize: bool = True, 
            ensemble_size: int = 5):
        """
        –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            target_type: –¢–∏–ø —Ç–∞—Ä–≥–µ—Ç–∞ - simple_binary, threshold_binary, 
                        direction_multiclass, simple_regression
            optimize: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Optuna –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            ensemble_size: –†–∞–∑–º–µ—Ä –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π
        """
        start_time = time.time()
        
        try:
            # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            logger.info("=" * 60)
            logger.info("üöÄ –ó–ê–ü–£–°–ö –£–õ–£–ß–®–ï–ù–ù–û–ì–û XGBOOST v3.0")
            logger.info(f"üìä –¢–∏–ø —Ç–∞—Ä–≥–µ—Ç–∞: {target_type}")
            logger.info("=" * 60)
            
            data_loader = DataLoader(self.config)
            df = data_loader.load_data()
            
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {len(df)} —Å—Ç—Ä–æ–∫")
            logger.info(f"üìà –°–∏–º–≤–æ–ª—ã: {df['symbol'].nunique()}")
            
            # 2. –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            logger.info("\nüîß –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
            feature_engineer = FeatureEngineer(self.config)
            df_features = feature_engineer.create_features(df)
            
            # 3. –†–∞—Å—á–µ—Ç —É–ø—Ä–æ—â–µ–Ω–Ω—ã—Ö —Ç–∞—Ä–≥–µ—Ç–æ–≤
            logger.info(f"\nüéØ –†–∞—Å—á–µ—Ç —Ç–∞—Ä–≥–µ—Ç–æ–≤ —Ç–∏–ø–∞: {target_type}")
            df_with_targets = self.target_calculator.calculate_all_targets(df_features)
            
            # –í—ã–±–∏—Ä–∞–µ–º –Ω—É–∂–Ω—ã–π —Ç–∞—Ä–≥–µ—Ç
            target_column = f"target_{target_type}"
            if target_column not in df_with_targets.columns:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —Ç–∞—Ä–≥–µ—Ç–∞: {target_type}")
            
            # –î–ª—è –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–∞ –∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥—Ä—É–≥—É—é –ª–æ–≥–∏–∫—É
            if target_type == "direction_multiclass":
                return self._train_multiclass(df_with_targets, target_column, 
                                            optimize, ensemble_size)
            elif target_type == "simple_regression":
                return self._train_regression(df_with_targets, target_column,
                                            optimize, ensemble_size)
            else:
                # –î–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                return self._train_binary(df_with_targets, target_column,
                                        optimize, ensemble_size)
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ: {e}", exc_info=True)
            raise
        finally:
            total_time = time.time() - start_time
            logger.info(f"\n‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {total_time/60:.1f} –º–∏–Ω—É—Ç")
            
    def _train_binary(self, df: pd.DataFrame, target_column: str,
                     optimize: bool, ensemble_size: int) -> Dict:
        """–û–±—É—á–µ–Ω–∏–µ –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        
        # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
        preprocessor = DataPreprocessor(self.config)
        X_train, X_test, y_train, y_test, feature_names = preprocessor.prepare_data(
            df, target_column=target_column
        )
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        logger.info("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö:")
        logger.info(f"   Train: {len(X_train)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        logger.info(f"   Test: {len(X_test)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        logger.info(f"   –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_names)}")
        logger.info(f"   –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –≤ train: {(y_train == 1).sum()} ({(y_train == 1).mean()*100:.1f}%)")
        logger.info(f"   –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –≤ test: {(y_test == 1).sum()} ({(y_test == 1).mean()*100:.1f}%)")
        
        # –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_selector = FeatureSelector(
            method=self.config.training.feature_selection_method,
            top_k=100  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ø-100 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        )
        
        X_train_selected, selected_features = feature_selector.select_features(
            X_train, y_train, feature_names
        )
        X_test_selected = X_test[selected_features]
        
        # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        balancer = DataBalancer(self.config)
        X_train_balanced, y_train_balanced = balancer.balance_data(
            X_train_selected, y_train
        )
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if optimize:
            optimizer = OptunaOptimizer(self.config)
            best_params = optimizer.optimize(
                X_train_balanced, y_train_balanced,
                n_trials=self.config.training.optuna_trials
            )
            logger.info(f"\nüéØ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {best_params}")
        else:
            best_params = self.config.model.to_dict()
            
        # –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è
        ensemble = EnsembleModel(
            base_params=best_params,
            ensemble_size=ensemble_size,
            config=self.config
        )
        
        logger.info(f"\nüöÄ –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –∏–∑ {ensemble_size} –º–æ–¥–µ–ª–µ–π...")
        ensemble.fit(X_train_balanced, y_train_balanced)
        
        # –û—Ü–µ–Ω–∫–∞
        metrics = ensemble.evaluate(X_test_selected, y_test)
        
        # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_importance = self._analyze_feature_importance(
            ensemble, selected_features
        )
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results = {
            'metrics': metrics,
            'feature_importance': feature_importance,
            'selected_features': selected_features,
            'best_params': best_params,
            'target_type': target_column,
            'ensemble': ensemble
        }
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
        self.report_generator.generate_final_report(results, self.log_manager.log_dir)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model_path = self.log_manager.log_dir / f"model_{target_column}.pkl"
        ensemble.save(model_path)
        logger.info(f"\nüíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
        
        return results
        
    def _train_multiclass(self, df: pd.DataFrame, target_column: str,
                         optimize: bool, ensemble_size: int) -> Dict:
        """–û–±—É—á–µ–Ω–∏–µ –¥–ª—è –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        logger.info("\nüéØ –ú—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (5 –∫–ª–∞—Å—Å–æ–≤)")
        
        # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
        preprocessor = DataPreprocessor(self.config)
        X_train, X_test, y_train, y_test, feature_names = preprocessor.prepare_data(
            df, target_column=target_column
        )
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º
        logger.info("\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
        for class_id in range(5):
            train_count = (y_train == class_id).sum()
            test_count = (y_test == class_id).sum()
            logger.info(f"   –ö–ª–∞—Å—Å {class_id}: Train {train_count} ({train_count/len(y_train)*100:.1f}%), "
                       f"Test {test_count} ({test_count/len(y_test)*100:.1f}%)")
        
        # –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_selector = FeatureSelector(
            method=self.config.training.feature_selection_method,
            top_k=100
        )
        
        X_train_selected, selected_features = feature_selector.select_features(
            X_train, y_train, feature_names
        )
        X_test_selected = X_test[selected_features]
        
        # –î–ª—è –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–∞ –∏–∑–º–µ–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        multiclass_params = self.config.model.to_dict()
        multiclass_params['objective'] = 'multi:softprob'
        multiclass_params['num_class'] = 5
        multiclass_params['eval_metric'] = 'mlogloss'
        
        # –û–±—É—á–µ–Ω–∏–µ
        trainer = XGBoostTrainer(self.config)
        trainer.params = multiclass_params
        
        model = trainer.train(
            X_train_selected, y_train,
            X_test_selected, y_test
        )
        
        # –û—Ü–µ–Ω–∫–∞
        from sklearn.metrics import classification_report, confusion_matrix
        y_pred = model.predict(X_test_selected)
        
        logger.info("\nüìä –û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
        logger.info(classification_report(y_test, y_pred))
        
        results = {
            'model': model,
            'classification_report': classification_report(y_test, y_pred, output_dict=True),
            'confusion_matrix': confusion_matrix(y_test, y_pred),
            'selected_features': selected_features,
            'target_type': target_column
        }
        
        return results
        
    def _train_regression(self, df: pd.DataFrame, target_column: str,
                         optimize: bool, ensemble_size: int) -> Dict:
        """–û–±—É—á–µ–Ω–∏–µ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏"""
        logger.info("\nüìà –†–µ–≥—Ä–µ—Å—Å–∏—è (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è)")
        
        # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
        preprocessor = DataPreprocessor(self.config)
        X_train, X_test, y_train, y_test, feature_names = preprocessor.prepare_data(
            df, target_column=target_column
        )
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–∞—Ä–≥–µ—Ç–æ–≤
        logger.info("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–∞—Ä–≥–µ—Ç–æ–≤:")
        logger.info(f"   Train: mean={y_train.mean():.4f}, std={y_train.std():.4f}")
        logger.info(f"   Test: mean={y_test.mean():.4f}, std={y_test.std():.4f}")
        
        # –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_selector = FeatureSelector(
            method=self.config.training.feature_selection_method,
            top_k=100
        )
        
        X_train_selected, selected_features = feature_selector.select_features(
            X_train, y_train, feature_names
        )
        X_test_selected = X_test[selected_features]
        
        # –î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏–∑–º–µ–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        regression_params = self.config.model.to_dict()
        regression_params['objective'] = 'reg:squarederror'
        regression_params['eval_metric'] = 'rmse'
        
        # –û–±—É—á–µ–Ω–∏–µ
        trainer = XGBoostTrainer(self.config)
        trainer.params = regression_params
        
        model = trainer.train(
            X_train_selected, y_train,
            X_test_selected, y_test
        )
        
        # –û—Ü–µ–Ω–∫–∞
        from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
        y_pred = model.predict(X_test_selected)
        
        metrics = {
            'mae': mean_absolute_error(y_test, y_pred),
            'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),
            'r2': r2_score(y_test, y_pred)
        }
        
        logger.info("\nüìä –ú–µ—Ç—Ä–∏–∫–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏:")
        logger.info(f"   MAE: {metrics['mae']:.6f}")
        logger.info(f"   RMSE: {metrics['rmse']:.6f}")
        logger.info(f"   R¬≤: {metrics['r2']:.4f}")
        
        results = {
            'model': model,
            'metrics': metrics,
            'selected_features': selected_features,
            'target_type': target_column,
            'predictions': {
                'y_true': y_test,
                'y_pred': y_pred
            }
        }
        
        return results
        
    def _analyze_feature_importance(self, ensemble: EnsembleModel, 
                                   feature_names: List[str]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–µ–π"""
        
        # –ü–æ–ª—É—á–∞–µ–º —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—É—é –≤–∞–∂–Ω–æ—Å—Ç—å
        importances = []
        for model in ensemble.models:
            if hasattr(model, 'feature_importances_'):
                importances.append(model.feature_importances_)
                
        avg_importance = np.mean(importances, axis=0)
        
        # –°–æ–∑–¥–∞–µ–º DataFrame
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': avg_importance
        }).sort_values('importance', ascending=False)
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è
        def get_category(feature):
            feature_lower = feature.lower()
            
            if any(p in feature_lower for p in ['rsi', 'macd', 'bb_', 'adx', 'atr', 
                                                'stoch', 'williams', 'mfi', 'cci']):
                return '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã'
            elif 'btc_' in feature_lower:
                return 'BTC –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏'
            elif any(p in feature_lower for p in ['hour', 'dow', 'day', 'week']):
                return '–í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏'
            elif 'volume' in feature_lower:
                return '–û–±—ä–µ–º–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏'
            else:
                return '–î—Ä—É–≥–∏–µ'
                
        importance_df['category'] = importance_df['feature'].apply(get_category)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        category_stats = importance_df.groupby('category')['importance'].agg([
            'sum', 'mean', 'count'
        ]).sort_values('sum', ascending=False)
        
        logger.info("\nüìä –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
        for cat, row in category_stats.iterrows():
            logger.info(f"   {cat}: {row['sum']*100:.1f}% "
                       f"(—Å—Ä–µ–¥–Ω–µ–µ: {row['mean']*100:.2f}%, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {row['count']})")
            
        logger.info("\nüîù –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
        for _, row in importance_df.head(10).iterrows():
            logger.info(f"   {row['feature']}: {row['importance']*100:.2f}%")
            
        return {
            'importance_df': importance_df,
            'category_stats': category_stats
        }


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
    parser = argparse.ArgumentParser(description='–£–ª—É—á—à–µ–Ω–Ω—ã–π XGBoost v3.0')
    
    parser.add_argument('--target-type', type=str, default='threshold_binary',
                       choices=['simple_binary', 'threshold_binary', 
                               'direction_multiclass', 'simple_regression'],
                       help='–¢–∏–ø —Ç–∞—Ä–≥–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è')
    parser.add_argument('--optimize', action='store_true',
                       help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Optuna –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏')
    parser.add_argument('--ensemble-size', type=int, default=5,
                       help='–†–∞–∑–º–µ—Ä –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π')
    parser.add_argument('--test-mode', action='store_true',
                       help='–¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º (—Ç–æ–ª—å–∫–æ BTC/ETH)')
    parser.add_argument('--gpu', action='store_true',
                       help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU')
    parser.add_argument('--config', type=str, default='config.yaml',
                       help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏')
    
    args = parser.parse_args()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    config_path = Path(args.config)
    if config_path.exists():
        config = Config.from_yaml(str(config_path))
    else:
        config = Config()
        
    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    config.training.test_mode = args.test_mode
    
    if args.gpu:
        config.model.tree_method = 'gpu_hist'
        config.model.predictor = 'gpu_predictor'
        config.model.gpu_id = 0
        
    # –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞
    pipeline = ImprovedXGBoostPipeline(config)
    results = pipeline.run(
        target_type=args.target_type,
        optimize=args.optimize,
        ensemble_size=args.ensemble_size
    )
    
    logger.info("\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
    

if __name__ == "__main__":
    main()