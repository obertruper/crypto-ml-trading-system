"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Ä–∞–±–æ—Ç—ã —Å —Ñ–∞–π–ª–∞–º–∏ –∏ –¥–∞–Ω–Ω—ã–º–∏
"""

import os
import pickle
import joblib
import pandas as pd
import numpy as np
import pyarrow as pa
import pyarrow.parquet as pq
from typing import Any, Dict, Optional, Union
import logging
from pathlib import Path
import gc
import concurrent.futures
from functools import partial

logger = logging.getLogger(__name__)


class FastIO:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å —Ñ–∞–π–ª–∞–º–∏"""
    
    @staticmethod
    def save_pickle(obj: Any, filepath: str, compress: int = 3) -> None:
        """
        –ë—ã—Å—Ç—Ä–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ –≤ pickle —Å —Å–∂–∞—Ç–∏–µ–º
        
        Args:
            obj: –û–±—ä–µ–∫—Ç –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            compress: –£—Ä–æ–≤–µ–Ω—å —Å–∂–∞—Ç–∏—è (0-9, –≥–¥–µ 3 - –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å)
        """
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º joblib –¥–ª—è –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–∞—Å—Å–∏–≤–æ–≤
            if isinstance(obj, (np.ndarray, pd.DataFrame)):
                joblib.dump(obj, filepath, compress=compress)
                logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ —Å joblib: {filepath}")
            else:
                # –î–ª—è –æ–±—ã—á–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º pickle —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
                with open(filepath, 'wb') as f:
                    pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)
                logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ —Å pickle: {filepath}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è {filepath}: {e}")
            raise
    
    @staticmethod
    def load_pickle(filepath: str) -> Any:
        """
        –ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –æ–±—ä–µ–∫—Ç–∞ –∏–∑ pickle
        
        Args:
            filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            
        Returns:
            –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª –ª–∏ —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω —á–µ—Ä–µ–∑ joblib
            try:
                obj = joblib.load(filepath)
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å joblib: {filepath}")
                return obj
            except:
                # –ï—Å–ª–∏ –Ω–µ joblib, –ø—Ä–æ–±—É–µ–º –æ–±—ã—á–Ω—ã–π pickle
                with open(filepath, 'rb') as f:
                    obj = pickle.load(f)
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å pickle: {filepath}")
                return obj
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {filepath}: {e}")
            raise
    
    @staticmethod
    def save_parquet_fast(df: pd.DataFrame, filepath: str, 
                         compression: str = 'snappy',
                         use_pyarrow: bool = True) -> None:
        """
        –ë—ã—Å—Ç—Ä–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ DataFrame –≤ parquet
        
        Args:
            df: DataFrame –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            compression: –¢–∏–ø —Å–∂–∞—Ç–∏—è ('snappy', 'gzip', 'brotli')
            use_pyarrow: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å pyarrow –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        """
        try:
            if use_pyarrow:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º pyarrow –Ω–∞–ø—Ä—è–º—É—é –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏
                table = pa.Table.from_pandas(df, preserve_index=False)
                pq.write_table(
                    table, 
                    filepath,
                    compression=compression,
                    use_dictionary=True,  # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                    write_statistics=True,  # –î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —á—Ç–µ–Ω–∏—è
                    row_group_size=50000  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≥—Ä—É–ø–ø—ã
                )
            else:
                # Fallback –Ω–∞ pandas
                df.to_parquet(
                    filepath, 
                    compression=compression,
                    engine='pyarrow',
                    index=False
                )
            
            size_mb = os.path.getsize(filepath) / (1024 * 1024)
            logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ parquet: {filepath} ({size_mb:.1f} MB)")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è parquet {filepath}: {e}")
            raise
    
    @staticmethod
    def load_parquet_fast(filepath: str, 
                         columns: Optional[list] = None,
                         use_pyarrow: bool = True) -> pd.DataFrame:
        """
        –ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ DataFrame –∏–∑ parquet
        
        Args:
            filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            columns: –°–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ (None = –≤—Å–µ)
            use_pyarrow: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å pyarrow –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
            
        Returns:
            –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π DataFrame
        """
        try:
            if use_pyarrow:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º pyarrow —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
                parquet_file = pq.ParquetFile(filepath)
                
                # –ï—Å–ª–∏ –Ω—É–∂–Ω—ã —Ç–æ–ª—å–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
                if columns:
                    df = parquet_file.read(columns=columns).to_pandas()
                else:
                    df = parquet_file.read().to_pandas()
            else:
                # Fallback –Ω–∞ pandas
                df = pd.read_parquet(filepath, columns=columns, engine='pyarrow')
            
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ parquet: {filepath} ({len(df):,} —Å—Ç—Ä–æ–∫)")
            return df
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ parquet {filepath}: {e}")
            raise
    
    @staticmethod
    def parallel_save_models(models: Dict[str, Any], 
                           base_path: str,
                           n_workers: int = 4) -> None:
        """
        –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π
        
        Args:
            models: –°–ª–æ–≤–∞—Ä—å {–∏–º—è: –º–æ–¥–µ–ª—å}
            base_path: –ë–∞–∑–æ–≤—ã–π –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            n_workers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ—Ä–∫–µ—Ä–æ–≤
        """
        os.makedirs(base_path, exist_ok=True)
        
        def save_model(item):
            name, model = item
            filepath = os.path.join(base_path, f"{name}.pkl")
            FastIO.save_pickle(model, filepath)
            return name
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=n_workers) as executor:
            futures = {executor.submit(save_model, item): item[0] 
                      for item in models.items()}
            
            for future in concurrent.futures.as_completed(futures):
                name = futures[future]
                try:
                    result = future.result()
                    logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å: {result}")
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ {name}: {e}")
    
    @staticmethod
    def optimize_memory(df: pd.DataFrame, 
                       verbose: bool = True) -> pd.DataFrame:
        """
        –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ DataFrame
        
        Args:
            df: DataFrame –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            verbose: –í—ã–≤–æ–¥–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            
        Returns:
            –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DataFrame
        """
        start_mem = df.memory_usage().sum() / 1024**2
        
        for col in df.columns:
            col_type = df[col].dtype
            
            if col_type != 'object':
                c_min = df[col].min()
                c_max = df[col].max()
                
                # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è int
                if str(col_type)[:3] == 'int':
                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                        df[col] = df[col].astype(np.int8)
                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                        df[col] = df[col].astype(np.int16)
                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                        df[col] = df[col].astype(np.int32)
                
                # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è float
                else:
                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                        df[col] = df[col].astype(np.float16)
                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                        df[col] = df[col].astype(np.float32)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        for col in df.select_dtypes(include=['object']).columns:
            num_unique_values = len(df[col].unique())
            num_total_values = len(df[col])
            if num_unique_values / num_total_values < 0.5:
                df[col] = df[col].astype('category')
        
        end_mem = df.memory_usage().sum() / 1024**2
        
        if verbose:
            logger.info(f'üéØ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏: {start_mem:.1f} MB ‚Üí {end_mem:.1f} MB '
                       f'({100 * (start_mem - end_mem) / start_mem:.1f}% —ç–∫–æ–Ω–æ–º–∏–∏)')
        
        return df
    
    @staticmethod
    def batch_process_files(file_paths: list,
                          process_func: callable,
                          n_workers: int = 4,
                          **kwargs) -> list:
        """
        –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤
        
        Args:
            file_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º
            process_func: –§—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            n_workers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ—Ä–∫–µ—Ä–æ–≤
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è process_func
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        process_func_with_args = partial(process_func, **kwargs)
        
        results = []
        with concurrent.futures.ProcessPoolExecutor(max_workers=n_workers) as executor:
            future_to_file = {executor.submit(process_func_with_args, fp): fp 
                            for fp in file_paths}
            
            for future in concurrent.futures.as_completed(future_to_file):
                filepath = future_to_file[future]
                try:
                    result = future.result()
                    results.append(result)
                    logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω: {filepath}")
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {filepath}: {e}")
                    results.append(None)
        
        return results


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
fast_io = FastIO()