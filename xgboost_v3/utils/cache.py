"""
–ú–µ–Ω–µ–¥–∂–µ—Ä –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
"""

import pickle
import hashlib
import json
import logging
from pathlib import Path
from typing import Any, Optional, Dict
import pandas as pd
import numpy as np

logger = logging.getLogger(__name__)


class CacheManager:
    """–ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, cache_dir: str = ".cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.metadata_file = self.cache_dir / "cache_metadata.json"
        self.metadata = self._load_metadata()
        
    def _load_metadata(self) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫—ç—à–∞"""
        if self.metadata_file.exists():
            with open(self.metadata_file, 'r') as f:
                return json.load(f)
        return {}
        
    def _save_metadata(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫—ç—à–∞"""
        with open(self.metadata_file, 'w') as f:
            json.dump(self.metadata, f, indent=2)
            
    def _generate_key(self, identifier: str, params: Optional[Dict] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∫–ª—é—á–∞ –¥–ª—è –∫—ç—à–∞"""
        key_data = {'identifier': identifier}
        if params:
            key_data.update(params)
            
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_str.encode()).hexdigest()
        
    def get(self, identifier: str, params: Optional[Dict] = None) -> Optional[Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫—ç—à–∞"""
        cache_key = self._generate_key(identifier, params)
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    data = pickle.load(f)
                    
                logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞: {identifier}")
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                if cache_key in self.metadata:
                    self.metadata[cache_key]['access_count'] += 1
                    self.metadata[cache_key]['last_access'] = pd.Timestamp.now().isoformat()
                    self._save_metadata()
                    
                return data
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–∑ –∫—ç—à–∞: {e}")
                # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                cache_file.unlink()
                
        return None
        
    def set(self, identifier: str, data: Any, params: Optional[Dict] = None):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –∫—ç—à"""
        cache_key = self._generate_key(identifier, params)
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
                
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            self.metadata[cache_key] = {
                'identifier': identifier,
                'params': params,
                'created': pd.Timestamp.now().isoformat(),
                'last_access': pd.Timestamp.now().isoformat(),
                'access_count': 0,
                'size_mb': cache_file.stat().st_size / 1024 / 1024
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞–Ω–Ω—ã—Ö
            if isinstance(data, pd.DataFrame):
                self.metadata[cache_key]['data_info'] = {
                    'type': 'DataFrame',
                    'shape': data.shape,
                    'columns': len(data.columns)
                }
            elif isinstance(data, dict):
                self.metadata[cache_key]['data_info'] = {
                    'type': 'dict',
                    'keys': list(data.keys())
                }
                
            self._save_metadata()
            
            logger.info(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –∫—ç—à: {identifier} ({cache_file.stat().st_size / 1024 / 1024:.1f} MB)")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ –∫—ç—à: {e}")
            if cache_file.exists():
                cache_file.unlink()
                
    def clear(self, identifier: Optional[str] = None):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞"""
        if identifier:
            # –û—á–∏—â–∞–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä
            keys_to_remove = []
            for cache_key, info in self.metadata.items():
                if info['identifier'] == identifier:
                    cache_file = self.cache_dir / f"{cache_key}.pkl"
                    if cache_file.exists():
                        cache_file.unlink()
                    keys_to_remove.append(cache_key)
                    
            for key in keys_to_remove:
                del self.metadata[key]
                
            self._save_metadata()
            logger.info(f"üóëÔ∏è –û—á–∏—â–µ–Ω –∫—ç—à –¥–ª—è: {identifier}")
            
        else:
            # –û—á–∏—â–∞–µ–º –≤–µ—Å—å –∫—ç—à
            for cache_file in self.cache_dir.glob("*.pkl"):
                cache_file.unlink()
                
            self.metadata = {}
            self._save_metadata()
            logger.info("üóëÔ∏è –í–µ—Å—å –∫—ç—à –æ—á–∏—â–µ–Ω")
            
    def get_cache_info(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫—ç—à–µ"""
        total_size = sum(
            (self.cache_dir / f"{key}.pkl").stat().st_size 
            for key in self.metadata.keys()
            if (self.cache_dir / f"{key}.pkl").exists()
        ) / 1024 / 1024  # MB
        
        info = {
            'total_items': len(self.metadata),
            'total_size_mb': total_size,
            'cache_dir': str(self.cache_dir),
            'items': []
        }
        
        for cache_key, metadata in self.metadata.items():
            info['items'].append({
                'identifier': metadata['identifier'],
                'created': metadata['created'],
                'last_access': metadata['last_access'],
                'access_count': metadata['access_count'],
                'size_mb': metadata['size_mb']
            })
            
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É –¥–æ—Å—Ç—É–ø—É
        info['items'].sort(key=lambda x: x['last_access'], reverse=True)
        
        return info
        
    def cleanup_old_cache(self, days: int = 7):
        """–£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–≥–æ –∫—ç—à–∞"""
        current_time = pd.Timestamp.now()
        keys_to_remove = []
        
        for cache_key, metadata in self.metadata.items():
            last_access = pd.Timestamp(metadata['last_access'])
            
            if (current_time - last_access).days > days:
                cache_file = self.cache_dir / f"{cache_key}.pkl"
                if cache_file.exists():
                    cache_file.unlink()
                keys_to_remove.append(cache_key)
                
        for key in keys_to_remove:
            del self.metadata[key]
            
        if keys_to_remove:
            self._save_metadata()
            logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ {len(keys_to_remove)} —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤ –∫—ç—à–∞")
            
    def cache_dataframe(self, df: pd.DataFrame, identifier: str, 
                       compression: bool = True) -> bool:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è DataFrame —Å –∫–æ–º–ø—Ä–µ—Å—Å–∏–µ–π"""
        if compression:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º parquet –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è
            cache_key = self._generate_key(identifier)
            cache_file = self.cache_dir / f"{cache_key}.parquet"
            
            try:
                df.to_parquet(cache_file, compression='snappy', index=False)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                self.metadata[cache_key] = {
                    'identifier': identifier,
                    'type': 'parquet',
                    'created': pd.Timestamp.now().isoformat(),
                    'last_access': pd.Timestamp.now().isoformat(),
                    'access_count': 0,
                    'size_mb': cache_file.stat().st_size / 1024 / 1024,
                    'shape': df.shape,
                    'columns': list(df.columns)
                }
                self._save_metadata()
                
                logger.info(f"üíæ DataFrame —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ parquet: {identifier} ({cache_file.stat().st_size / 1024 / 1024:.1f} MB)")
                return True
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ parquet: {e}")
                return False
        else:
            self.set(identifier, df)
            return True
            
    def load_dataframe(self, identifier: str) -> Optional[pd.DataFrame]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ DataFrame –∏–∑ –∫—ç—à–∞"""
        cache_key = self._generate_key(identifier)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º parquet —Ñ–∞–π–ª
        parquet_file = self.cache_dir / f"{cache_key}.parquet"
        if parquet_file.exists():
            try:
                df = pd.read_parquet(parquet_file)
                logger.info(f"‚úÖ DataFrame –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ parquet: {identifier}")
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                if cache_key in self.metadata:
                    self.metadata[cache_key]['access_count'] += 1
                    self.metadata[cache_key]['last_access'] = pd.Timestamp.now().isoformat()
                    self._save_metadata()
                    
                return df
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ parquet: {e}")
                
        # –ü—Ä–æ–±—É–µ–º –æ–±—ã—á–Ω—ã–π pickle
        return self.get(identifier)