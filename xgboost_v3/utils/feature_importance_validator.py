"""
–í–∞–ª–∏–¥–∞—Ç–æ—Ä –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è XGBoost v3.0
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —á—Ç–æ –º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Tuple, Any

try:
    from config.feature_mapping import get_feature_category, get_temporal_blacklist
    USE_FEATURE_MAPPING = True
except ImportError:
    USE_FEATURE_MAPPING = False

logger = logging.getLogger(__name__)


class FeatureImportanceValidator:
    """–ö–ª–∞—Å—Å –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, max_temporal_importance: float = 5.0):
        """
        Args:
            max_temporal_importance: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–æ–ø—É—Å—Ç–∏–º–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (%)
        """
        self.max_temporal_importance = max_temporal_importance
        self.validation_results = {}
        
    def validate_model_feature_importance(self, model: Any, feature_names: List[str], 
                                        model_name: str = "unknown") -> Dict:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏
        
        Args:
            model: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å XGBoost
            feature_names: –°–ø–∏—Å–æ–∫ –∏–º–µ–Ω –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        if not hasattr(model, 'feature_importances_'):
            logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –∏–º–µ–µ—Ç feature_importances_")
            return {'valid': False, 'reason': 'no_feature_importances'}
        
        # –ü–æ–ª—É—á–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç–∏
        importances = model.feature_importances_
        
        # –°–æ–∑–¥–∞–µ–º DataFrame
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importances
        }).sort_values('importance', ascending=False)
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        if USE_FEATURE_MAPPING:
            importance_df['category'] = importance_df['feature'].apply(get_feature_category)
        else:
            importance_df['category'] = importance_df['feature'].apply(self._categorize_feature_fallback)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        category_analysis = self._analyze_by_categories(importance_df)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
        validation_result = self._check_overfitting(category_analysis, model_name)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.validation_results[model_name] = {
            'category_analysis': category_analysis,
            'validation': validation_result,
            'top_features': importance_df.head(10)['feature'].tolist()
        }
        
        return validation_result
    
    def validate_ensemble_importance(self, models: Dict[str, Any], feature_names: List[str]) -> Dict:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π
        
        Args:
            models: –°–ª–æ–≤–∞—Ä—å –º–æ–¥–µ–ª–µ–π {direction: {'ensemble': ensemble_model}}
            feature_names: –°–ø–∏—Å–æ–∫ –∏–º–µ–Ω –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∞–Ω—Å–∞–º–±–ª—è
        """
        logger.info("üîç –í–ê–õ–ò–î–ê–¶–ò–Ø –í–ê–ñ–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í –ê–ù–°–ê–ú–ë–õ–Ø")
        
        all_importances = {}
        model_results = {}
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–æ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        for direction in ['buy', 'sell']:
            if direction not in models:
                continue
                
            ensemble = models[direction]['ensemble']
            
            for i, model in enumerate(ensemble.models):
                model_name = f"{direction}_model_{i}"
                result = self.validate_model_feature_importance(model, feature_names, model_name)
                model_results[model_name] = result
                
                # –°–æ–±–∏—Ä–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è
                if hasattr(model, 'feature_importances_'):
                    for feat, imp in zip(feature_names, model.feature_importances_):
                        if feat not in all_importances:
                            all_importances[feat] = []
                        all_importances[feat].append(imp)
        
        # –£—Å—Ä–µ–¥–Ω—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç–∏
        avg_importances = {feat: np.mean(imps) for feat, imps in all_importances.items()}
        
        # –°–æ–∑–¥–∞–µ–º –æ–±—â–∏–π –∞–Ω–∞–ª–∏–∑
        avg_importance_df = pd.DataFrame({
            'feature': list(avg_importances.keys()),
            'importance': list(avg_importances.values())
        }).sort_values('importance', ascending=False)
        
        if USE_FEATURE_MAPPING:
            avg_importance_df['category'] = avg_importance_df['feature'].apply(get_feature_category)
        else:
            avg_importance_df['category'] = avg_importance_df['feature'].apply(self._categorize_feature_fallback)
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        ensemble_category_analysis = self._analyze_by_categories(avg_importance_df)
        ensemble_validation = self._check_overfitting(ensemble_category_analysis, "ensemble")
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self._log_ensemble_results(ensemble_category_analysis, ensemble_validation, model_results)
        
        return {
            'ensemble_validation': ensemble_validation,
            'category_analysis': ensemble_category_analysis,
            'model_results': model_results,
            'top_features': avg_importance_df.head(15)['feature'].tolist()
        }
    
    def _analyze_by_categories(self, importance_df: pd.DataFrame) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
        total_importance = importance_df['importance'].sum()
        
        category_stats = {}
        for category in ['technical', 'temporal', 'btc_related', 'symbol', 'other']:
            cat_features = importance_df[importance_df['category'] == category]
            
            if len(cat_features) > 0:
                cat_importance = cat_features['importance'].sum()
                percentage = (cat_importance / total_importance) * 100 if total_importance > 0 else 0
                
                category_stats[category] = {
                    'count': len(cat_features),
                    'total_importance': cat_importance,
                    'percentage': percentage,
                    'top_features': cat_features.head(3)['feature'].tolist()
                }
            else:
                category_stats[category] = {
                    'count': 0,
                    'total_importance': 0,
                    'percentage': 0,
                    'top_features': []
                }
        
        return category_stats
    
    def _check_overfitting(self, category_analysis: Dict, model_name: str) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º"""
        temporal_percentage = category_analysis['temporal']['percentage']
        technical_percentage = category_analysis['technical']['percentage']
        
        issues = []
        severity = "ok"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: –°–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if temporal_percentage > self.max_temporal_importance:
            issues.append(f"Temporal –ø—Ä–∏–∑–Ω–∞–∫–∏: {temporal_percentage:.1f}% > {self.max_temporal_importance}%")
            severity = "critical"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–æ–ª–∂–Ω—ã –¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞—Ç—å
        if technical_percentage < 70:
            issues.append(f"–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–ª–∏—à–∫–æ–º —Å–ª–∞–±—ã–µ: {technical_percentage:.1f}% < 70%")
            if severity != "critical":
                severity = "warning"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 3: –í—Ä–µ–º–µ–Ω–Ω—ã–µ –≤–∞–∂–Ω–µ–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö (–∫—Ä–∏—Ç–∏—á–Ω–æ!)
        if temporal_percentage > technical_percentage:
            issues.append("–ö–†–ò–¢–ò–ß–ù–û: Temporal –≤–∞–∂–Ω–µ–µ Technical!")
            severity = "critical"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 4: –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —Ç–æ–ø–µ
        if USE_FEATURE_MAPPING:
            blacklist = get_temporal_blacklist()
            top_temporal = category_analysis['temporal']['top_features']
            problematic = [f for f in top_temporal if f in blacklist]
            if problematic:
                issues.append(f"–ü—Ä–æ–±–ª–µ–º–Ω—ã–µ temporal –≤ —Ç–æ–ø–µ: {problematic}")
                if severity != "critical":
                    severity = "warning"
        
        result = {
            'valid': len(issues) == 0,
            'severity': severity,
            'issues': issues,
            'temporal_percentage': temporal_percentage,
            'technical_percentage': technical_percentage
        }
        
        # –õ–æ–≥–∏—Ä—É–µ–º –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏
        if issues:
            logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã –≤ –º–æ–¥–µ–ª–∏ {model_name}:")
            for issue in issues:
                logger.warning(f"   - {issue}")
        else:
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name}: –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤–∞–ª–∏–¥–Ω—ã")
        
        return result
    
    def _log_ensemble_results(self, category_analysis: Dict, ensemble_validation: Dict, 
                            model_results: Dict):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω—Å–∞–º–±–ª—è"""
        logger.info("\n" + "="*60)
        logger.info("üìä –ò–¢–û–ì–û–í–´–ô –ê–ù–ê–õ–ò–ó –í–ê–ñ–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í")
        logger.info("="*60)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        logger.info("üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
        for category, stats in category_analysis.items():
            if stats['count'] > 0:
                logger.info(f"   {category}: {stats['count']} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, "
                          f"{stats['percentage']:.1f}% –≤–∞–∂–Ω–æ—Å—Ç–∏")
                if stats['top_features']:
                    logger.info(f"      –¢–æ–ø: {', '.join(stats['top_features'])}")
        
        # –û–±—â–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
        if ensemble_validation['valid']:
            logger.info("‚úÖ –í–ê–õ–ò–î–ê–¶–ò–Ø –ü–†–û–ô–î–ï–ù–ê: –ú–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö")
        else:
            logger.error("‚ùå –í–ê–õ–ò–î–ê–¶–ò–Ø –ù–ï –ü–†–û–ô–î–ï–ù–ê!")
            logger.error(f"   –°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å: {ensemble_validation['severity'].upper()}")
            for issue in ensemble_validation['issues']:
                logger.error(f"   - {issue}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º
        critical_models = [name for name, result in model_results.items() 
                          if result.get('severity') == 'critical']
        if critical_models:
            logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª–∏ —Å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø—Ä–æ–±–ª–µ–º–∞–º–∏: {', '.join(critical_models)}")
    
    def _categorize_feature_fallback(self, feature_name: str) -> str:
        """Fallback –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –µ—Å–ª–∏ feature_mapping –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
        feature_lower = feature_name.lower()
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ
        if any(t in feature_lower for t in ['hour', 'dow', 'weekend']):
            return 'temporal'
        
        # BTC related
        if 'btc_' in feature_lower:
            return 'btc_related'
        
        # Symbol
        if feature_lower.startswith('is_'):
            return 'symbol'
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏
        return 'technical'
    
    def get_recommendations(self) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –º–æ–¥–µ–ª–∏"""
        recommendations = []
        
        if not self.validation_results:
            return ["–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"]
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        critical_count = sum(1 for result in self.validation_results.values() 
                           if result['validation'].get('severity') == 'critical')
        
        if critical_count > 0:
            recommendations.append("–ö–†–ò–¢–ò–ß–ù–û: –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö")
            recommendations.append("1. –£–º–µ–Ω—å—à–∏—Ç—å –∫–≤–æ—Ç—É temporal –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–æ 1%")
            recommendations.append("2. –ò—Å–∫–ª—é—á–∏—Ç—å dow_sin, dow_cos, is_weekend –∏–∑ –æ–±—É—á–µ–Ω–∏—è")
            recommendations.append("3. –£–≤–µ–ª–∏—á–∏—Ç—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é (alpha, lambda)")
            recommendations.append("4. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö")
        
        return recommendations