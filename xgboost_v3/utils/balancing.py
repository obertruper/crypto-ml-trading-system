"""
–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤
"""

import numpy as np
import pandas as pd
import logging
from typing import Tuple, Optional
from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler
from imblearn.combine import SMOTETomek
from sklearn.utils.class_weight import compute_class_weight

from config import Config, FEATURE_CONFIG

logger = logging.getLogger(__name__)


class BalanceStrategy:
    """–ö–ª–∞—Å—Å –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤"""
    
    def __init__(self, config: Config):
        self.config = config
        self.binary_features = FEATURE_CONFIG['binary_thresholds'].keys()
        
    def balance_data(self, X: pd.DataFrame, y: pd.Series, 
                    model_name: str = "") -> Tuple[pd.DataFrame, pd.Series]:
        """
        –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ–≥–ª–∞—Å–Ω–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        
        Args:
            X: –ü—Ä–∏–∑–Ω–∞–∫–∏
            y: –ú–µ—Ç–∫–∏
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            
        Returns:
            –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ X, y
        """
        logger.info(f"\nüîÑ –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é {self.config.training.balance_method.upper()}...")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
        self._log_class_distribution(y, "–î–æ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏")
        
        if self.config.training.balance_method == "none":
            return X, y
        elif self.config.training.balance_method == "smote":
            X_balanced, y_balanced = self._apply_smote(X, y)
        elif self.config.training.balance_method == "adasyn":
            X_balanced, y_balanced = self._apply_adasyn(X, y)
        elif self.config.training.balance_method == "random_oversample":
            X_balanced, y_balanced = self._apply_random_oversampler(X, y)
        elif self.config.training.balance_method == "smote_tomek":
            X_balanced, y_balanced = self._apply_smote_tomek(X, y)
        else:
            logger.warning(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: {self.config.training.balance_method}")
            return X, y
            
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
        self._log_class_distribution(y_balanced, "–ü–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏")
        
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ—Å–ª–µ SMOTE
        if self.config.training.balance_method in ["smote", "adasyn", "smote_tomek"]:
            X_balanced = self._fix_binary_features_after_smote(X_balanced, model_name)
            
        return X_balanced, y_balanced
        
    def _apply_smote(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SMOTE –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏"""
        logger.info("üîÑ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SMOTE –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤...")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã SMOTE
        continuous_features, binary_features = self._separate_features(X)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º SMOTE —Ç–æ–ª—å–∫–æ –∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º
        smote = SMOTE(
            sampling_strategy='auto',
            k_neighbors=min(self.config.training.smote_k_neighbors, (y == 1).sum() - 1),
            random_state=42
        )
        
        try:
            if continuous_features:
                X_continuous = X[continuous_features]
                X_continuous_resampled, y_resampled = smote.fit_resample(X_continuous, y)
                
                # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                # –î–ª—è –Ω–æ–≤—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö
                n_synthetic = len(y_resampled) - len(y)
                
                if binary_features and n_synthetic > 0:
                    # –ë–µ—Ä–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–∞ 1
                    minority_indices = y[y == 1].index
                    random_indices = np.random.choice(minority_indices, n_synthetic, replace=True)
                    
                    # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
                    X_resampled = pd.DataFrame(X_continuous_resampled, columns=continuous_features)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                    for feature in binary_features:
                        original_values = X.loc[y.index, feature].values
                        synthetic_values = X.loc[random_indices, feature].values
                        X_resampled[feature] = np.concatenate([original_values, synthetic_values])
                else:
                    X_resampled = pd.DataFrame(X_continuous_resampled, columns=continuous_features)
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º RandomOverSampler
                logger.warning("–ù–µ—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è SMOTE, –∏—Å–ø–æ–ª—å–∑—É–µ–º RandomOverSampler")
                return self._apply_random_oversampler(X, y)
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ SMOTE: {e}")
            logger.warning("–í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
            return X, y
            
        logger.info(f"   –î–æ–±–∞–≤–ª–µ–Ω–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(y_resampled) - len(y)}")
        
        return X_resampled, y_resampled
        
    def _apply_adasyn(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ ADASYN –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏"""
        logger.info("üîÑ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ ADASYN –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏...")
        
        try:
            adasyn = ADASYN(
                sampling_strategy='auto',
                n_neighbors=min(5, (y == 1).sum() - 1),
                random_state=42
            )
            
            X_resampled, y_resampled = adasyn.fit_resample(X, y)
            X_resampled = pd.DataFrame(X_resampled, columns=X.columns)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ ADASYN: {e}")
            return self._apply_smote(X, y)
            
        return X_resampled, y_resampled
        
    def _apply_random_oversampler(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–ª—É—á–∞–π–Ω–æ–π –ø–µ—Ä–µ–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏"""
        logger.info("üîÑ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Random Oversampling...")
        
        ros = RandomOverSampler(sampling_strategy='auto', random_state=42)
        X_resampled, y_resampled = ros.fit_resample(X, y)
        
        X_resampled = pd.DataFrame(X_resampled, columns=X.columns)
        
        return X_resampled, y_resampled
        
    def _apply_smote_tomek(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ SMOTE –∏ Tomek links"""
        logger.info("üîÑ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SMOTE + Tomek links...")
        
        try:
            smt = SMOTETomek(random_state=42)
            X_resampled, y_resampled = smt.fit_resample(X, y)
            X_resampled = pd.DataFrame(X_resampled, columns=X.columns)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ SMOTE-Tomek: {e}")
            return self._apply_smote(X, y)
            
        return X_resampled, y_resampled
        
    def _separate_features(self, X: pd.DataFrame) -> Tuple[list, list]:
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –∏ –±–∏–Ω–∞—Ä–Ω—ã–µ"""
        binary_features = []
        continuous_features = []
        
        for col in X.columns:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø—Ä–∏–∑–Ω–∞–∫ –±–∏–Ω–∞—Ä–Ω—ã–º
            unique_values = X[col].unique()
            if len(unique_values) <= 2 and set(unique_values).issubset({0, 1}):
                binary_features.append(col)
            else:
                continuous_features.append(col)
                
        return continuous_features, binary_features
        
    def _fix_binary_features_after_smote(self, X: pd.DataFrame, model_name: str) -> pd.DataFrame:
        """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ SMOTE"""
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        X = self._clip_technical_indicators(X)
        
        # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        X = self._recreate_binary_features(X, model_name)
        
        return X
        
    def _clip_technical_indicators(self, X: pd.DataFrame) -> pd.DataFrame:
        """–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        logger.info("‚úÇÔ∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤...")
        
        from config.features_config import TECHNICAL_INDICATORS_BOUNDS
        
        for indicator, (min_val, max_val) in TECHNICAL_INDICATORS_BOUNDS.items():
            if indicator in X.columns:
                original_outliers = ((X[indicator] < min_val) | (X[indicator] > max_val)).sum()
                if original_outliers > 0:
                    X[indicator] = X[indicator].clip(min_val, max_val)
                    logger.info(f"   üìä {indicator}: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ {original_outliers} –∑–Ω–∞—á–µ–Ω–∏–π –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [{min_val}, {max_val}]")
                    
        logger.info("   ‚úÇÔ∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω—ã –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –ø–æ—Å–ª–µ SMOTE")
        
        return X
        
    def _recreate_binary_features(self, X: pd.DataFrame, model_name: str) -> pd.DataFrame:
        """–ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
        logger.info("üîÑ –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ SMOTE...")
        
        thresholds = FEATURE_CONFIG['binary_thresholds']
        recreated_features = []
        
        # RSI —É—Å–ª–æ–≤–∏—è
        if 'rsi_val' in X.columns and 'rsi_oversold' in X.columns:
            X['rsi_oversold'] = (X['rsi_val'] < thresholds['rsi_oversold']).astype(int)
            recreated_features.append('rsi_oversold')
            
        if 'rsi_val' in X.columns and 'rsi_overbought' in X.columns:
            X['rsi_overbought'] = (X['rsi_val'] > thresholds['rsi_overbought']).astype(int)
            recreated_features.append('rsi_overbought')
            
        # MACD —É—Å–ª–æ–≤–∏–µ
        if 'macd_hist' in X.columns and 'macd_bullish' in X.columns:
            X['macd_bullish'] = (X['macd_hist'] > thresholds['macd_bullish']).astype(int)
            recreated_features.append('macd_bullish')
            
        # –°–∏–ª—å–Ω—ã–π —Ç—Ä–µ–Ω–¥
        if 'adx_val' in X.columns and 'strong_trend' in X.columns:
            X['strong_trend'] = (X['adx_val'] > thresholds['strong_trend']).astype(int)
            recreated_features.append('strong_trend')
            
        # –í—Å–ø–ª–µ—Å–∫ –æ–±—ä–µ–º–∞
        if 'volume_ratio' in X.columns and 'volume_spike' in X.columns:
            X['volume_spike'] = (X['volume_ratio'] > thresholds['volume_spike']).astype(int)
            recreated_features.append('volume_spike')
            
        # –ü–æ–∑–∏—Ü–∏—è –≤ Bollinger Bands
        if 'bb_position' in X.columns:
            if 'bb_near_lower' in X.columns:
                X['bb_near_lower'] = (X['bb_position'] < thresholds['bb_near_lower']).astype(int)
                recreated_features.append('bb_near_lower')
                
            if 'bb_near_upper' in X.columns:
                X['bb_near_upper'] = (X['bb_position'] > thresholds['bb_near_upper']).astype(int)
                recreated_features.append('bb_near_upper')
                
        if recreated_features:
            logger.info(f"   ‚ôªÔ∏è –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω—ã –±–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ—Å–ª–µ SMOTE –¥–ª—è {model_name}: {recreated_features}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            stats = []
            for feature in recreated_features[:4]:  # –ü–µ—Ä–≤—ã–µ 4 –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
                if feature in X.columns:
                    pct = X[feature].mean() * 100
                    stats.append(f"{feature}: {pct:.1f}%")
                    
            if stats:
                logger.info(f"   üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ SMOTE: {', '.join(stats)}")
                
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self._validate_binary_features(X)
        
        return X
        
    def _validate_binary_features(self, X: pd.DataFrame):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        problems = []
        
        for col in X.columns:
            if col in self.binary_features or col.startswith(('is_', 'has_')):
                unique_vals = X[col].unique()
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω–æ—Å—Ç—å
                if len(unique_vals) == 1:
                    problems.append(f"{col}: –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ (–≤—Å–µ–≥–¥–∞ {unique_vals[0]})")
                    
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–∏–Ω–∞—Ä–Ω–æ—Å—Ç—å
                elif not set(unique_vals).issubset({0, 1, 0.0, 1.0}):
                    problems.append(f"{col}: –Ω–µ–±–∏–Ω–∞—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è {unique_vals}")
                    
        if problems:
            logger.warning("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã —Å –±–∏–Ω–∞—Ä–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏:")
            for problem in problems:
                logger.warning(f"   - {problem}")
                
    def _log_class_distribution(self, y: pd.Series, stage: str):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤"""
        class_counts = y.value_counts().sort_index()
        total = len(y)
        
        logger.info(f"   {stage}:")
        for class_val, count in class_counts.items():
            percentage = count / total * 100
            logger.info(f"      –ö–ª–∞—Å—Å {class_val}: {count:,} ({percentage:.1f}%)")
            
    def calculate_class_weights(self, y: pd.Series) -> dict:
        """–†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –≤–∑–≤–µ—à–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å"""
        classes = np.unique(y)
        weights = compute_class_weight('balanced', classes=classes, y=y)
        
        class_weight_dict = dict(zip(classes, weights))
        
        logger.info("üìä –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤:")
        for class_val, weight in class_weight_dict.items():
            logger.info(f"   –ö–ª–∞—Å—Å {class_val}: {weight:.3f}")
            
        return class_weight_dict