"""
–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
"""

import os
import hashlib
import json
import pickle
import logging
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
import pandas as pd
import numpy as np

logger = logging.getLogger(__name__)


class OptunaCache:
    """–ö—ç—à –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
    
    def __init__(self, cache_dir: str = "cache/optuna"):
        """
        Args:
            cache_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –∫—ç—à–∞
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
    def _get_cache_key(self, 
                      dataset_hash: str,
                      model_type: str,
                      task_type: str,
                      n_features: int,
                      n_samples: int) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∫–ª—é—á–∞ –¥–ª—è –∫—ç—à–∞"""
        key_data = {
            'dataset': dataset_hash,
            'model': model_type,
            'task': task_type,
            'features': n_features,
            'samples': n_samples
        }
        
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def _get_dataset_hash(self, X: pd.DataFrame, y: pd.Series) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —Ö—ç—à –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        # –ë–µ—Ä–µ–º —Å—ç–º–ø–ª –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è (–ø–µ—Ä–≤—ã–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏)
        sample_size = min(1000, len(X))
        X_sample = pd.concat([X.head(sample_size//2), X.tail(sample_size//2)])
        y_sample = pd.concat([y.head(sample_size//2), y.tail(sample_size//2)])
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä–æ–∫—É –¥–ª—è —Ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
        data_str = f"{X_sample.columns.tolist()}{X_sample.values.tobytes()}{y_sample.values.tobytes()}"
        return hashlib.md5(data_str.encode()).hexdigest()[:16]
    
    def check_cache(self, 
                   X: pd.DataFrame,
                   y: pd.Series,
                   model_type: str,
                   task_type: str) -> Optional[Dict[str, Any]]:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        Args:
            X: –ü—Ä–∏–∑–Ω–∞–∫–∏
            y: –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
            model_type: –¢–∏–ø –º–æ–¥–µ–ª–∏ (buy/sell)
            task_type: –¢–∏–ø –∑–∞–¥–∞—á–∏
            
        Returns:
            –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–ª–∏ None
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ö—ç—à –¥–∞–Ω–Ω—ã—Ö
            dataset_hash = self._get_dataset_hash(X, y)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–ª—é—á –∫—ç—à–∞
            cache_key = self._get_cache_key(
                dataset_hash=dataset_hash,
                model_type=model_type,
                task_type=task_type,
                n_features=X.shape[1],
                n_samples=X.shape[0]
            )
            
            # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫—ç—à–∞
            cache_file = self.cache_dir / f"{cache_key}.pkl"
            meta_file = self.cache_dir / f"{cache_key}_meta.json"
            
            if cache_file.exists() and meta_file.exists():
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                with open(meta_file, 'r') as f:
                    meta = json.load(f)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å
                if (meta.get('n_features') == X.shape[1] and 
                    abs(meta.get('n_samples', 0) - X.shape[0]) < X.shape[0] * 0.1):  # 10% –¥–æ–ø—É—Å–∫
                    
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                    with open(cache_file, 'rb') as f:
                        cached_data = pickle.load(f)
                    
                    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –∫—ç—à –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è {model_type}")
                    logger.info(f"   –õ—É—á—à–∏–π score: {cached_data['best_score']:.4f}")
                    logger.info(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ trials: {cached_data['n_trials']}")
                    
                    return cached_data
                else:
                    logger.info(f"‚ö†Ô∏è –ö—ç—à —É—Å—Ç–∞—Ä–µ–ª –¥–ª—è {model_type} (–∏–∑–º–µ–Ω–∏–ª—Å—è —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö)")
                    
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∫—ç—à–∞: {e}")
        
        return None
    
    def save_cache(self,
                  X: pd.DataFrame,
                  y: pd.Series,
                  model_type: str,
                  task_type: str,
                  best_params: Dict[str, Any],
                  best_score: float,
                  n_trials: int,
                  study: Optional[Any] = None) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∫—ç—à
        
        Args:
            X: –ü—Ä–∏–∑–Ω–∞–∫–∏
            y: –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
            model_type: –¢–∏–ø –º–æ–¥–µ–ª–∏
            task_type: –¢–∏–ø –∑–∞–¥–∞—á–∏
            best_params: –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            best_score: –õ—É—á—à–∏–π score
            n_trials: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
            study: –û–±—ä–µ–∫—Ç Optuna Study (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ö—ç—à –¥–∞–Ω–Ω—ã—Ö
            dataset_hash = self._get_dataset_hash(X, y)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–ª—é—á –∫—ç—à–∞
            cache_key = self._get_cache_key(
                dataset_hash=dataset_hash,
                model_type=model_type,
                task_type=task_type,
                n_features=X.shape[1],
                n_samples=X.shape[0]
            )
            
            # –î–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            cache_data = {
                'best_params': best_params,
                'best_score': best_score,
                'n_trials': n_trials,
                'model_type': model_type,
                'task_type': task_type
            }
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            meta_data = {
                'dataset_hash': dataset_hash,
                'n_features': X.shape[1],
                'n_samples': X.shape[0],
                'feature_names': X.columns.tolist(),
                'model_type': model_type,
                'task_type': task_type,
                'best_score': best_score,
                'n_trials': n_trials,
                'timestamp': pd.Timestamp.now().isoformat()
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            cache_file = self.cache_dir / f"{cache_key}.pkl"
            meta_file = self.cache_dir / f"{cache_key}_meta.json"
            
            with open(cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
            
            with open(meta_file, 'w') as f:
                json.dump(meta_data, f, indent=2)
            
            # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º study
            if study is not None:
                study_file = self.cache_dir / f"{cache_key}_study.pkl"
                with open(study_file, 'wb') as f:
                    pickle.dump(study, f)
            
            logger.info(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –∫—ç—à –¥–ª—è {model_type}")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—ç—à: {e}")
    
    def get_cache_info(self) -> pd.DataFrame:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Å–µ—Ö –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö"""
        cache_info = []
        
        for meta_file in self.cache_dir.glob("*_meta.json"):
            try:
                with open(meta_file, 'r') as f:
                    meta = json.load(f)
                
                cache_info.append({
                    'model_type': meta.get('model_type'),
                    'task_type': meta.get('task_type'),
                    'n_features': meta.get('n_features'),
                    'n_samples': meta.get('n_samples'),
                    'best_score': meta.get('best_score'),
                    'n_trials': meta.get('n_trials'),
                    'timestamp': meta.get('timestamp')
                })
            except:
                continue
        
        if cache_info:
            return pd.DataFrame(cache_info).sort_values('timestamp', ascending=False)
        else:
            return pd.DataFrame()
    
    def clear_old_cache(self, days: int = 7) -> int:
        """
        –£–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–π –∫—ç—à
        
        Args:
            days: –£–¥–∞–ª–∏—Ç—å –∫—ç—à —Å—Ç–∞—Ä—à–µ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–Ω–µ–π
            
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        """
        count = 0
        cutoff_date = pd.Timestamp.now() - pd.Timedelta(days=days)
        
        for meta_file in self.cache_dir.glob("*_meta.json"):
            try:
                with open(meta_file, 'r') as f:
                    meta = json.load(f)
                
                timestamp = pd.Timestamp(meta.get('timestamp', '2000-01-01'))
                
                if timestamp < cutoff_date:
                    # –£–¥–∞–ª—è–µ–º –≤—Å–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                    base_name = meta_file.stem.replace('_meta', '')
                    for pattern in [f"{base_name}.pkl", f"{base_name}_meta.json", f"{base_name}_study.pkl"]:
                        file_path = self.cache_dir / pattern
                        if file_path.exists():
                            file_path.unlink()
                            count += 1
                            
            except:
                continue
        
        if count > 0:
            logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ {count} —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤ –∫—ç—à–∞")
        
        return count