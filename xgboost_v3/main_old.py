#!/usr/bin/env python3
"""
XGBoost v3.0 - –ì–ª–∞–≤–Ω—ã–π –º–æ–¥—É–ª—å –∑–∞–ø—É—Å–∫–∞
–ß–∏—Å—Ç–∞—è –º–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è ML –∫—Ä–∏–ø—Ç–æ—Ç—Ä–µ–π–¥–∏–Ω–≥–∞
"""

import argparse
import logging
import sys
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—é –≤ sys.path
sys.path.insert(0, str(Path(__file__).parent))

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à–∏ –º–æ–¥—É–ª–∏
from config import Config
from data import DataLoader, DataPreprocessor, FeatureEngineer
from models import XGBoostTrainer, EnsembleModel, OptunaOptimizer
from utils import Visualizer, CacheManager

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
def setup_logging(log_dir: Path):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∏—Å—Ç–µ–º—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
    log_dir.mkdir(parents=True, exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_dir / 'training.log'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    return logging.getLogger(__name__)


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞"""
    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    parser = argparse.ArgumentParser(description='XGBoost v3.0 –¥–ª—è –∫—Ä–∏–ø—Ç–æ—Ç—Ä–µ–π–¥–∏–Ω–≥–∞')
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument('--task', type=str, default='classification_binary',
                       choices=['classification_binary', 'classification_multi', 'regression'],
                       help='–¢–∏–ø –∑–∞–¥–∞—á–∏')
    parser.add_argument('--config', type=str, default=None,
                       help='–ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É')
    
    # –†–µ–∂–∏–º—ã —Ä–∞–±–æ—Ç—ã
    parser.add_argument('--test-mode', action='store_true',
                       help='–¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º (2 —Å–∏–º–≤–æ–ª–∞)')
    parser.add_argument('--no-cache', action='store_true',
                       help='–ù–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à')
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    parser.add_argument('--ensemble-size', type=int, default=2,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –≤ –∞–Ω—Å–∞–º–±–ª–µ')
    parser.add_argument('--balance-method', type=str, default='smote',
                       choices=['none', 'smote', 'adasyn', 'class_weight'],
                       help='–ú–µ—Ç–æ–¥ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤')
    parser.add_argument('--optuna-trials', type=int, default=50,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ Optuna')
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument('--save-models', action='store_true', default=True,
                       help='–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏')
    parser.add_argument('--visualize', action='store_true', default=True,
                       help='–°–æ–∑–¥–∞—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏')
    
    args = parser.parse_args()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    if args.config:
        config = Config.from_yaml(args.config)
    else:
        config = Config()
        
    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    config.training.task_type = args.task
    config.training.test_mode = args.test_mode
    config.training.use_cache = not args.no_cache
    config.training.ensemble_size = args.ensemble_size
    config.training.balance_method = args.balance_method
    config.training.optuna_trials = args.optuna_trials
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    config.validate()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –ª–æ–≥–æ–≤
    log_dir = config.get_log_dir()
    logger = setup_logging(log_dir)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config.save(log_dir / 'config.yaml')
    
    logger.info("="*80)
    logger.info("üöÄ XGBoost v3.0 - –ù–∞—á–∞–ª–æ —Ä–∞–±–æ—Ç—ã")
    logger.info("="*80)
    logger.info(str(config))
    
    try:
        # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        logger.info("\nüì¶ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤...")
        
        data_loader = DataLoader(config)
        preprocessor = DataPreprocessor(config)
        feature_engineer = FeatureEngineer(config)
        cache_manager = CacheManager(cache_dir=".cache/xgboost_v3")
        visualizer = Visualizer(save_dir=log_dir / "plots")
        
        # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        logger.info("\nüìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = f"data_{config.training.test_mode}_{config.training.task_type}"
        df = None
        
        if config.training.use_cache:
            df = cache_manager.load_dataframe(cache_key)
            
        if df is None:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ –ë–î
            data_loader.connect()
            df = data_loader.load_data()
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
            if not data_loader.validate_data(df):
                raise ValueError("–î–∞–Ω–Ω—ã–µ –Ω–µ –ø—Ä–æ—à–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é")
                
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            if config.training.use_cache:
                cache_manager.cache_dataframe(df, cache_key)
                
            data_loader.disconnect()
        
        # 3. Feature Engineering
        logger.info("\nüîß –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        df = feature_engineer.create_features(df)
        
        # 4. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        logger.info("\nüìê –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        X, y_buy, y_sell = preprocessor.preprocess(df)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if config.training.task_type != "regression":
            y_buy, y_sell = preprocessor.transform_to_classification_labels(y_buy, y_sell)
        
        # 5. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        data_splits = preprocessor.split_data(X, y_buy, y_sell)
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç —Ö—Ä–∞–Ω–∏—Ç—å—Å—è –∑–¥–µ—Å—å
        results = {
            'buy': {},
            'sell': {}
        }
        
        # 6. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è Buy –∏ Sell
        for target_type in ['buy', 'sell']:
            logger.info(f"\n{'='*60}")
            logger.info(f"üéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è {target_type.upper()}")
            logger.info(f"{'='*60}")
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Ç–∏–ø–∞
            X_train = data_splits[target_type]['X_train']
            X_test = data_splits[target_type]['X_test']
            y_train = data_splits[target_type]['y_train']
            y_test = data_splits[target_type]['y_test']
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            X_train_scaled, X_test_scaled = preprocessor.normalize_features(X_train, X_test)
            
            # 7. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            if config.training.optuna_trials > 0:
                logger.info("\nüîç –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...")
                
                optimizer = OptunaOptimizer(config)
                best_params = optimizer.optimize(
                    X_train_scaled, y_train,
                    n_trials=config.training.optuna_trials,
                    model_type=target_type
                )
                
                # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
                if config.training.save_plots:
                    optimizer.plot_optimization_history(
                        save_path=log_dir / f"plots/{target_type}_optuna_history.png"
                    )
                    
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
                model_params = config.model.to_dict()
                model_params.update(best_params)
            else:
                model_params = None
                
            # 8. –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è
            if config.training.ensemble_size > 1:
                logger.info(f"\nüé≤ –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –∏–∑ {config.training.ensemble_size} –º–æ–¥–µ–ª–µ–π...")
                
                ensemble = EnsembleModel(config)
                models = ensemble.train_ensemble(
                    X_train_scaled, y_train,
                    X_test_scaled, y_test,
                    n_models=config.training.ensemble_size
                )
                
                # –û—Ü–µ–Ω–∫–∞ –∞–Ω—Å–∞–º–±–ª—è
                test_metrics = ensemble.evaluate(X_test_scaled, y_test, "Test")
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è
                if args.save_models:
                    ensemble.save_ensemble(log_dir / "models" / target_type)
                    
                results[target_type]['ensemble'] = ensemble
                results[target_type]['metrics'] = test_metrics
                
            else:
                # –û–±—É—á–µ–Ω–∏–µ –æ–¥–∏–Ω–æ—á–Ω–æ–π –º–æ–¥–µ–ª–∏
                logger.info("\nüéØ –û–±—É—á–µ–Ω–∏–µ –æ–¥–∏–Ω–æ—á–Ω–æ–π –º–æ–¥–µ–ª–∏...")
                
                trainer = XGBoostTrainer(config, model_name=f"{target_type}_model")
                model = trainer.train(
                    X_train_scaled, y_train,
                    X_test_scaled, y_test,
                    model_params=model_params
                )
                
                # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
                test_metrics = trainer.evaluate(X_test_scaled, y_test, "Test")
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
                if args.save_models:
                    trainer.save_model(log_dir / "models" / target_type)
                    
                results[target_type]['model'] = trainer
                results[target_type]['metrics'] = test_metrics
                
            # 9. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if args.visualize:
                logger.info("\nüìä –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")
                
                # Feature importance
                if hasattr(results[target_type].get('model', results[target_type].get('ensemble')), 'get_feature_importance'):
                    feature_importance = results[target_type]['model'].get_feature_importance()
                    if not feature_importance.empty:
                        visualizer.plot_feature_importance(
                            feature_importance,
                            model_name=f"{target_type}_model"
                        )
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
                if 'ensemble' in results[target_type]:
                    y_pred_proba = results[target_type]['ensemble'].predict(X_test_scaled, return_proba=True)
                else:
                    y_pred_proba = results[target_type]['model'].predict(X_test_scaled, return_proba=True)
                
                # ROC –∫—Ä–∏–≤–∞—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                if config.training.task_type != "regression":
                    visualizer.plot_roc_curve(y_test, y_pred_proba, model_name=f"{target_type}_model")
                    
                    # Confusion matrix
                    y_pred = (y_pred_proba > 0.5).astype(int)
                    visualizer.plot_confusion_matrix(y_test, y_pred, model_name=f"{target_type}_model")
                
                # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
                visualizer.plot_prediction_distribution(
                    y_test, y_pred_proba,
                    model_name=f"{target_type}_model",
                    task_type=config.training.task_type
                )
        
        # 10. –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
        logger.info("\n" + "="*80)
        logger.info("üìä –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´")
        logger.info("="*80)
        
        for target_type in ['buy', 'sell']:
            logger.info(f"\n{target_type.upper()} –º–æ–¥–µ–ª—å:")
            metrics = results[target_type]['metrics']
            
            if config.training.task_type == "regression":
                logger.info(f"  MAE: {metrics.get('mae', 0):.4f}")
                logger.info(f"  RMSE: {metrics.get('rmse', 0):.4f}")
                logger.info(f"  R¬≤: {metrics.get('r2', 0):.4f}")
                logger.info(f"  Direction Accuracy: {metrics.get('direction_accuracy', 0)*100:.1f}%")
            else:
                logger.info(f"  Accuracy: {metrics.get('accuracy', 0)*100:.1f}%")
                logger.info(f"  Precision: {metrics.get('precision', 0)*100:.1f}%")
                logger.info(f"  Recall: {metrics.get('recall', 0)*100:.1f}%")
                logger.info(f"  F1-Score: {metrics.get('f1', 0):.3f}")
                logger.info(f"  ROC-AUC: {metrics.get('roc_auc', 0):.3f}")
        
        logger.info("\n‚úÖ –û–±—É—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        logger.info(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {log_dir}")
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫—ç—à–µ
        cache_info = cache_manager.get_cache_info()
        logger.info(f"\nüíæ –ö—ç—à: {cache_info['total_items']} —Ñ–∞–π–ª–æ–≤, {cache_info['total_size_mb']:.1f} MB")
        
    except Exception as e:
        logger.error(f"\n‚ùå –û—à–∏–±–∫–∞: {e}", exc_info=True)
        raise
        
    finally:
        # –û—á–∏—Å—Ç–∫–∞
        if 'data_loader' in locals() and data_loader.connection:
            data_loader.disconnect()


if __name__ == "__main__":
    main()