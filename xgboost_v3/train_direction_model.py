#!/usr/bin/env python3
"""
–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ—Å—Ç—ã–µ –±–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏ –∏ walk-forward –∞–Ω–∞–ª–∏–∑.
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import argparse
import logging
import yaml
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
import psycopg2
import joblib
import json
from typing import Dict, List, Tuple, Optional

# –ú–æ–¥—É–ª–∏ –ø—Ä–æ–µ–∫—Ç–∞
from data.simple_targets import SimpleTargetSystem
from models.xgboost_trainer import XGBoostTrainer
from utils.metrics import MetricsCalculator
from utils.visualization import plot_walk_forward_results, plot_feature_importance
from config import Config

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class WalkForwardValidator:
    """
    –†–µ–∞–ª–∏–∑—É–µ—Ç walk-forward –∞–Ω–∞–ª–∏–∑ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤.
    –≠—Ç–æ –∑–æ–ª–æ—Ç–æ–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤ —Ç—Ä–µ–π–¥–∏–Ω–≥–µ.
    """
    
    def __init__(self,
                 train_window_days: int = 30,
                 test_window_days: int = 7,
                 n_splits: int = 10,
                 gap_hours: int = 1):
        """
        Args:
            train_window_days: –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –æ–±—É—á–µ–Ω–∏—è –≤ –¥–Ω—è—Ö
            test_window_days: –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –¥–Ω—è—Ö
            n_splits: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–±–∏–µ–Ω–∏–π
            gap_hours: –ó–∞–∑–æ—Ä –º–µ–∂–¥—É train –∏ test (–¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è —É—Ç–µ—á–∫–∏)
        """
        self.train_window_days = train_window_days
        self.test_window_days = test_window_days
        self.n_splits = n_splits
        self.gap_hours = gap_hours
        
    def split(self, df: pd.DataFrame) -> List[Tuple[pd.DataFrame, pd.DataFrame]]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ train/test —Å –ø–æ–º–æ—â—å—é walk-forward.
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (train_df, test_df)
        """
        df = df.sort_values('_timestamp').copy()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã
        start_time = df['_timestamp'].min()
        end_time = df['_timestamp'].max()
        
        # –í—ã—á–∏—Å–ª—è–µ–º —à–∞–≥ –¥–ª—è —Å–¥–≤–∏–≥–∞ –æ–∫–Ω–∞
        total_days = (end_time - start_time).days
        step_days = (total_days - self.train_window_days - self.test_window_days) // (self.n_splits - 1)
        
        splits = []
        
        for i in range(self.n_splits):
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã train
            train_start = start_time + timedelta(days=i * step_days)
            train_end = train_start + timedelta(days=self.train_window_days)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã test (—Å –∑–∞–∑–æ—Ä–æ–º)
            test_start = train_end + timedelta(hours=self.gap_hours)
            test_end = test_start + timedelta(days=self.test_window_days)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–µ –≤—ã—à–ª–∏ –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã
            if test_end > end_time:
                break
                
            # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
            train_df = df[(df['_timestamp'] >= train_start) & (df['_timestamp'] < train_end)]
            test_df = df[(df['_timestamp'] >= test_start) & (df['_timestamp'] < test_end)]
            
            if len(train_df) > 0 and len(test_df) > 0:
                splits.append((train_df, test_df))
                
                logger.info(f"Split {i+1}: Train {train_start.date()} to {train_end.date()} "
                          f"({len(train_df):,} samples), "
                          f"Test {test_start.date()} to {test_end.date()} "
                          f"({len(test_df):,} samples)")
        
        return splits


class DirectionPredictor:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è"""
    
    def __init__(self, config_path: str):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        with open(config_path, 'r') as f:
            self.config_dict = yaml.safe_load(f)
            
        self.db_config = {
            'host': self.config_dict['database']['host'],
            'port': self.config_dict['database']['port'],
            'database': self.config_dict['database']['database'],
            'user': self.config_dict['database']['user'],
            'password': self.config_dict['database']['password']
        }
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è XGBoost
        self.config = Config()
        self.config.training.task_type = 'classification_binary'
        
        # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        self.config.model.max_depth = 6
        self.config.model.learning_rate = 0.05
        self.config.model.n_estimators = 300
        self.config.model.subsample = 0.8
        self.config.model.colsample_bytree = 0.8
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        self.metrics_calculator = MetricsCalculator(self.config)
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.walk_forward_results = []
        
    def load_data(self, 
                  symbols: List[str],
                  target_type: str = 'buy_signal_threshold_1hour',
                  start_date: str = None,
                  end_date: str = None) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        
        import psycopg2
        import json
        
        conn = psycopg2.connect(**self.db_config)
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–º–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏
            query = """
            SELECT 
                t.timestamp,
                t.symbol,
                t.{} as target,
                p.open,
                p.high,
                p.low,
                p.close,
                p.volume,
                -- –†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
                (p.technical_indicators->>'rsi_val')::float as rsi_val,
                (p.technical_indicators->>'macd_val')::float as macd_val,
                (p.technical_indicators->>'macd_signal')::float as macd_signal,
                (p.technical_indicators->>'macd_diff')::float as macd_diff,
                (p.technical_indicators->>'bb_upper')::float as bb_upper,
                (p.technical_indicators->>'bb_middle')::float as bb_middle,
                (p.technical_indicators->>'bb_lower')::float as bb_lower,
                (p.technical_indicators->>'bb_width')::float as bb_width,
                (p.technical_indicators->>'bb_percent')::float as bb_percent,
                (p.technical_indicators->>'atr_val')::float as atr_val,
                (p.technical_indicators->>'adx_val')::float as adx_val,
                (p.technical_indicators->>'adx_plus_di')::float as adx_plus_di,
                (p.technical_indicators->>'adx_minus_di')::float as adx_minus_di,
                (p.technical_indicators->>'stoch_k')::float as stoch_k,
                (p.technical_indicators->>'stoch_d')::float as stoch_d,
                (p.technical_indicators->>'williams_r')::float as williams_r,
                (p.technical_indicators->>'cci_val')::float as cci_val,
                (p.technical_indicators->>'mfi_val')::float as mfi_val,
                (p.technical_indicators->>'obv_val')::float as obv_val,
                (p.technical_indicators->>'ema_9')::float as ema_9,
                (p.technical_indicators->>'ema_21')::float as ema_21,
                (p.technical_indicators->>'sma_50')::float as sma_50,
                (p.technical_indicators->>'sma_200')::float as sma_200,
                (p.technical_indicators->>'vwap')::float as vwap,
                (p.technical_indicators->>'pivot_point')::float as pivot_point,
                (p.technical_indicators->>'resistance_1')::float as resistance_1,
                (p.technical_indicators->>'support_1')::float as support_1,
                (p.technical_indicators->>'hour_sin')::float as hour_sin,
                (p.technical_indicators->>'hour_cos')::float as hour_cos,
                (p.technical_indicators->>'dow_sin')::float as dow_sin,
                (p.technical_indicators->>'dow_cos')::float as dow_cos,
                (p.technical_indicators->>'is_weekend')::float as is_weekend
            FROM simple_targets t
            JOIN processed_market_data p ON EXTRACT(EPOCH FROM t.timestamp) * 1000 = p.timestamp AND t.symbol = p.symbol
            WHERE t.{} IS NOT NULL
            """.format(target_type, target_type)
            
            conditions = []
            params = []
            
            if symbols:
                placeholders = ','.join(['%s'] * len(symbols))
                conditions.append(f"t.symbol IN ({placeholders})")
                params.extend(symbols)
                
            if start_date:
                conditions.append("t.timestamp >= %s")
                params.append(start_date)
                
            if end_date:
                conditions.append("t.timestamp <= %s")
                params.append(end_date)
                
            if conditions:
                query += " AND " + " AND ".join(conditions)
                
            query += " ORDER BY t.timestamp"
            
            df = pd.read_sql_query(query, conn, params=params)
            
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df):,} –∑–∞–ø–∏—Å–µ–π")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
            if 'target' in df.columns:
                class_counts = df['target'].value_counts()
                logger.info(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è {target_type}:")
                for class_val, count in class_counts.items():
                    percent = count / len(df) * 100
                    logger.info(f"  - –ö–ª–∞—Å—Å {class_val}: {count} ({percent:.1f}%)")
            
            return df
            
        finally:
            conn.close()
    
    def prepare_features(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é"""
        
        # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
        y = df['target'].astype(int)
        
        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        feature_cols = [col for col in df.columns if col not in [
            'timestamp', 'symbol', 'target', 'expected_return_buy', 'expected_return_sell'
        ]]
        
        X = df[feature_cols]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        X = X.copy()  # –ò–∑–±–µ–≥–∞–µ–º SettingWithCopyWarning
        X['_timestamp'] = df['timestamp'].values
        X['_symbol'] = df['symbol'].values
        
        logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {X.shape[1]-2} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        return X, y
    
    def select_top_features(self, X: pd.DataFrame, y: pd.Series, top_k: int = 50) -> List[str]:
        """–û—Ç–±–∏—Ä–∞–µ—Ç —Ç–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞–∂–Ω–æ—Å—Ç–∏"""
        
        from sklearn.ensemble import RandomForestClassifier
        
        # –£–¥–∞–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        feature_cols = [col for col in X.columns if not col.startswith('_')]
        X_train = X[feature_cols]
        
        # –ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏
        rf = RandomForestClassifier(
            n_estimators=100,
            max_depth=5,
            random_state=42,
            n_jobs=-1
        )
        
        rf.fit(X_train, y)
        
        # –ü–æ–ª—É—á–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç–∏
        importances = pd.DataFrame({
            'feature': feature_cols,
            'importance': rf.feature_importances_
        }).sort_values('importance', ascending=False)
        
        # –û—Ç–±–∏—Ä–∞–µ–º —Ç–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–∏
        top_features = importances.head(top_k)['feature'].tolist()
        
        logger.info(f"\n–¢–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
        for i, row in importances.head(10).iterrows():
            logger.info(f"  {row['feature']}: {row['importance']:.4f}")
            
        return top_features
    
    def train_walk_forward(self, 
                          df: pd.DataFrame,
                          target_direction: str = 'buy',
                          n_splits: int = 5):
        """–û–±—É—á–µ–Ω–∏–µ —Å walk-forward –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
        
        logger.info(f"\n{'='*60}")
        logger.info(f"Walk-Forward –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è {target_direction.upper()}")
        logger.info(f"{'='*60}")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        X, y = self.prepare_features(df)
        
        # Walk-forward –≤–∞–ª–∏–¥–∞—Ç–æ—Ä
        validator = WalkForwardValidator(
            train_window_days=30,
            test_window_days=7,
            n_splits=n_splits,
            gap_hours=1
        )
        
        # –°–æ–∑–¥–∞–µ–º DataFrame —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        data_with_meta = X.copy()
        data_with_meta['target'] = y
        
        splits = validator.split(data_with_meta)
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ split
        split_results = []
        all_predictions = []
        
        for i, (train_df, test_df) in enumerate(splits):
            logger.info(f"\n--- Split {i+1}/{len(splits)} ---")
            
            # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ X –∏ y
            y_train = train_df['target']
            y_test = test_df['target']
            
            X_train = train_df.drop(columns=['target'])
            X_test = test_df.drop(columns=['target'])
            
            # –û—Ç–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö
            if i == 0:  # –¢–æ–ª—å–∫–æ –≤ –ø–µ—Ä–≤—ã–π —Ä–∞–∑
                metadata_cols = [col for col in X_train.columns if col.startswith('_')]
                feature_cols = [col for col in X_train.columns if not col.startswith('_')]
                
                # –û—Ç–±–∏—Ä–∞–µ–º —Ç–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–∏
                selected_features = self.select_top_features(
                    X_train[feature_cols], 
                    y_train, 
                    top_k=50
                )
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            X_train_selected = X_train[selected_features]
            X_test_selected = X_test[selected_features]
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            trainer = XGBoostTrainer(self.config, model_name=f"{target_direction}_split_{i+1}")
            
            # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val –∏–∑ train –¥–∞–Ω–Ω—ã—Ö
            val_size = int(0.2 * len(X_train_selected))
            X_train_model = X_train_selected[:-val_size]
            y_train_model = y_train[:-val_size]
            X_val_model = X_train_selected[-val_size:]
            y_val_model = y_train[-val_size:]
            
            # –û–±—É—á–µ–Ω–∏–µ
            model = trainer.train(
                X_train_model, y_train_model,
                X_val_model, y_val_model
            )
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–µ
            y_pred_proba = trainer.predict(X_test_selected, return_proba=True)
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            metrics = self.metrics_calculator.calculate_classification_metrics(
                y_test.values, y_pred_proba
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            split_results.append({
                'split': i + 1,
                'train_start': train_df['_timestamp'].min(),
                'train_end': train_df['_timestamp'].max(),
                'test_start': test_df['_timestamp'].min(),
                'test_end': test_df['_timestamp'].max(),
                'train_size': len(train_df),
                'test_size': len(test_df),
                'metrics': metrics,
                'model': model
            })
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            predictions_df = test_df[['_timestamp', '_symbol']].copy()
            predictions_df['y_true'] = y_test
            predictions_df['y_pred_proba'] = y_pred_proba
            predictions_df['y_pred'] = (y_pred_proba > metrics['threshold']).astype(int)
            predictions_df['split'] = i + 1
            all_predictions.append(predictions_df)
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            logger.info(f"ROC-AUC: {metrics['roc_auc']:.4f}")
            logger.info(f"Accuracy: {metrics['accuracy']:.4f}")
            logger.info(f"Precision: {metrics['precision']:.4f}")
            logger.info(f"Recall: {metrics['recall']:.4f}")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        all_predictions_df = pd.concat(all_predictions, ignore_index=True)
        
        # –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        overall_metrics = self.metrics_calculator.calculate_classification_metrics(
            all_predictions_df['y_true'].values,
            all_predictions_df['y_pred_proba'].values
        )
        
        logger.info(f"\n{'='*60}")
        logger.info(f"–û–ë–©–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¥–ª—è {target_direction.upper()}")
        logger.info(f"{'='*60}")
        logger.info(f"ROC-AUC: {overall_metrics['roc_auc']:.4f}")
        logger.info(f"Accuracy: {overall_metrics['accuracy']:.4f}")
        logger.info(f"Precision: {overall_metrics['precision']:.4f}")
        logger.info(f"Recall: {overall_metrics['recall']:.4f}")
        logger.info(f"F1-Score: {overall_metrics['f1']:.4f}")
        
        return {
            'direction': target_direction,
            'splits': split_results,
            'predictions': all_predictions_df,
            'overall_metrics': overall_metrics,
            'selected_features': selected_features
        }
    
    def save_results(self, results: Dict, output_dir: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è"""
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics_data = {
            'overall': results['overall_metrics'],
            'splits': []
        }
        
        for split in results['splits']:
            metrics_data['splits'].append({
                'split': split['split'],
                'train_period': f"{split['train_start']} to {split['train_end']}",
                'test_period': f"{split['test_start']} to {split['test_end']}",
                'metrics': split['metrics']
            })
        
        with open(output_path / f"{results['direction']}_metrics.json", 'w') as f:
            json.dump(metrics_data, f, indent=2, default=str)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        results['predictions'].to_csv(
            output_path / f"{results['direction']}_predictions.csv",
            index=False
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        with open(output_path / f"{results['direction']}_features.txt", 'w') as f:
            for feat in results['selected_features']:
                f.write(f"{feat}\n")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –º–æ–¥–µ–ª—å
        last_model = results['splits'][-1]['model']
        joblib.dump(
            last_model,
            output_path / f"{results['direction']}_model_latest.pkl"
        )
        
        logger.info(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_path}")
    
    def generate_report(self, buy_results: Dict, sell_results: Dict, output_dir: str):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç"""
        
        output_path = Path(output_dir)
        
        report = f"""
============================================================
–û–¢–ß–ï–¢: –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã
============================================================

–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ú–û–î–ï–õ–ò BUY (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–æ—Å—Ç–∞ —Ü–µ–Ω—ã):
------------------------------------------------------------
ROC-AUC: {buy_results['overall_metrics']['roc_auc']:.4f}
Accuracy: {buy_results['overall_metrics']['accuracy']:.4f}
Precision: {buy_results['overall_metrics']['precision']:.4f}
Recall: {buy_results['overall_metrics']['recall']:.4f}
F1-Score: {buy_results['overall_metrics']['f1']:.4f}

–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ú–û–î–ï–õ–ò SELL (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–∞–¥–µ–Ω–∏—è —Ü–µ–Ω—ã):
------------------------------------------------------------
ROC-AUC: {sell_results['overall_metrics']['roc_auc']:.4f}
Accuracy: {sell_results['overall_metrics']['accuracy']:.4f}
Precision: {sell_results['overall_metrics']['precision']:.4f}
Recall: {sell_results['overall_metrics']['recall']:.4f}
F1-Score: {sell_results['overall_metrics']['f1']:.4f}

–î–ï–¢–ê–õ–ò WALK-FORWARD –ê–ù–ê–õ–ò–ó–ê:
------------------------------------------------------------
–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ splits: {len(buy_results['splits'])}
–†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –æ–±—É—á–µ–Ω–∏—è: 30 –¥–Ω–µ–π
–†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: 7 –¥–Ω–µ–π
–ó–∞–∑–æ—Ä –º–µ–∂–¥—É train/test: 1 —á–∞—Å

–¢–û–ü-10 –ü–†–ò–ó–ù–ê–ö–û–í (BUY):
------------------------------------------------------------
"""
        for i, feat in enumerate(buy_results['selected_features'][:10], 1):
            report += f"{i:2d}. {feat}\n"
            
        report += """
============================================================
"""
        
        with open(output_path / "final_report.txt", 'w') as f:
            f.write(report)
            
        logger.info(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path / 'final_report.txt'}")


def main():
    parser = argparse.ArgumentParser(description="–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è")
    
    parser.add_argument('--symbols', nargs='+', 
                       default=['BTCUSDT', 'ETHUSDT'],
                       help='–°–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è')
    
    parser.add_argument('--target-type', 
                       default='buy_signal_threshold_1hour',
                       help='–¢–∏–ø —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π')
    
    parser.add_argument('--n-splits', type=int, default=5,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ splits –¥–ª—è walk-forward')
    
    parser.add_argument('--output-dir', 
                       default='./direction_model_results',
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
    
    args = parser.parse_args()
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä
    predictor = DirectionPredictor('config.yaml')
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_dir = f"{args.output_dir}/{timestamp}"
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    df = predictor.load_data(
        symbols=args.symbols,
        target_type=args.target_type
    )
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–∫—É–ø–∫–∏
    buy_target = args.target_type.replace('sell', 'buy')
    df_buy = predictor.load_data(
        symbols=args.symbols,
        target_type=buy_target
    )
    
    buy_results = predictor.train_walk_forward(
        df_buy,
        target_direction='buy',
        n_splits=args.n_splits
    )
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–¥–∞–∂–∏
    sell_target = args.target_type.replace('buy', 'sell')
    df_sell = predictor.load_data(
        symbols=args.symbols,
        target_type=sell_target
    )
    
    sell_results = predictor.train_walk_forward(
        df_sell,
        target_direction='sell',
        n_splits=args.n_splits
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    predictor.save_results(buy_results, output_dir)
    predictor.save_results(sell_results, output_dir)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    predictor.generate_report(buy_results, sell_results, output_dir)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    try:
        # –ì—Ä–∞—Ñ–∏–∫ walk-forward —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        plot_walk_forward_results(
            buy_results['splits'],
            sell_results['splits'],
            output_dir
        )
        
        # –ì—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        plot_feature_importance(
            buy_results['selected_features'][:20],
            sell_results['selected_features'][:20],
            output_dir
        )
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏: {e}")
    
    logger.info(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ {output_dir}")


if __name__ == "__main__":
    main()