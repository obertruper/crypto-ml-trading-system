#!/usr/bin/env python3
"""
–û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π —Å confidence-based –ø–æ–¥—Ö–æ–¥–æ–º.

–ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
1. Confidence-based –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)
2. –ê–Ω—Å–∞–º–±–ª—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–π (trend, reversion, breakout, momentum)
3. –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
4. –£—á–µ—Ç —Ä—ã–Ω–æ—á–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞
5. Multi-task learning (–Ω–µ—Å–∫–æ–ª—å–∫–æ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ)
"""

import sys
import os
import logging
import yaml
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
import psycopg2
import joblib
import json
from typing import Dict, List, Tuple, Optional
import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import VotingClassifier, VotingRegressor
from sklearn.multioutput import MultiOutputClassifier, MultiOutputRegressor
from sklearn.metrics import roc_auc_score, accuracy_score, mean_squared_error
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ConfidenceModel:
    """
    –ú–æ–¥–µ–ª—å —Å confidence-based –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏.
    
    –ò–¥–µ—è: –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–µ —Ç–æ–ª—å–∫–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è,
    –Ω–æ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —ç—Ç–æ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏.
    """
    
    def __init__(self, name: str):
        self.name = name
        self.direction_model = None  # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
        self.confidence_model = None  # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        self.scaler = StandardScaler()
        
    def fit(self, X: pd.DataFrame, y: pd.Series, confidence_target: pd.Series):
        """
        –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        
        Args:
            X: –ü—Ä–∏–∑–Ω–∞–∫–∏
            y: –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è (–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ)
            confidence_target: –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        """
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        X_scaled = self.scaler.fit_transform(X)
        
        # –ú–æ–¥–µ–ª—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)
        self.direction_model = xgb.XGBClassifier(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            eval_metric='logloss'
        )
        
        self.direction_model.fit(X_scaled, y)
        
        # –ú–æ–¥–µ–ª—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (—Ä–µ–≥—Ä–µ—Å—Å–∏—è)
        self.confidence_model = xgb.XGBRegressor(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            eval_metric='rmse'
        )
        
        self.confidence_model.fit(X_scaled, confidence_target)
        
    def predict(self, X: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        
        Returns:
            direction_proba: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
            confidence: –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (0-1)
        """
        X_scaled = self.scaler.transform(X)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        direction_proba = self.direction_model.predict_proba(X_scaled)[:, 1]
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        confidence = self.confidence_model.predict(X_scaled)
        confidence = np.clip(confidence, 0, 1)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º [0, 1]
        
        return direction_proba, confidence
    
    def predict_with_confidence(self, X: pd.DataFrame, min_confidence: float = 0.6) -> Dict:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–æ–ª—å–∫–æ –¥–ª—è —Å–ª—É—á–∞–µ–≤ —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        """
        direction_proba, confidence = self.predict(X)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        high_confidence_mask = confidence >= min_confidence
        
        results = {
            'all_predictions': direction_proba,
            'all_confidence': confidence,
            'high_confidence_mask': high_confidence_mask,
            'high_confidence_predictions': direction_proba[high_confidence_mask],
            'high_confidence_count': high_confidence_mask.sum(),
            'coverage': high_confidence_mask.mean()
        }
        
        return results


class StrategyEnsemble:
    """
    –ê–Ω—Å–∞–º–±–ª—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä—ã–Ω–æ—á–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π
    """
    
    def __init__(self):
        self.models = {
            'trend_following': ConfidenceModel('trend_following'),
            'mean_reversion': ConfidenceModel('mean_reversion'),
            'breakout': ConfidenceModel('breakout'),
            'momentum': ConfidenceModel('momentum')
        }
        self.regime_model = None  # –ú–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ä–µ–∂–∏–º–∞ —Ä—ã–Ω–∫–∞
        
    def fit(self, X: pd.DataFrame, targets: Dict[str, pd.Series], 
            market_regime: pd.Series):
        """
        –û–±—É—á–∞–µ—Ç –∞–Ω—Å–∞–º–±–ª—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
        
        Args:
            X: –ü—Ä–∏–∑–Ω–∞–∫–∏
            targets: –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
            market_regime: –†—ã–Ω–æ—á–Ω—ã–π —Ä–µ–∂–∏–º
        """
        logger.info("–û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–π...")
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å —Ä–µ–∂–∏–º–∞ —Ä—ã–Ω–∫–∞
        self._fit_regime_model(X, market_regime)
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
        for strategy_name, model in self.models.items():
            if strategy_name in targets:
                logger.info(f"  –û–±—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: {strategy_name}")
                
                y = targets[strategy_name]['direction']
                confidence_target = targets[strategy_name]['confidence']
                
                model.fit(X, y, confidence_target)
    
    def _fit_regime_model(self, X: pd.DataFrame, market_regime: pd.Series):
        """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ä—ã–Ω–æ—á–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞"""
        from sklearn.preprocessing import LabelEncoder
        
        le = LabelEncoder()
        regime_encoded = le.fit_transform(market_regime)
        
        self.regime_model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=4,
            learning_rate=0.1,
            random_state=42
        )
        
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        self.regime_model.fit(X_scaled, regime_encoded)
        self.regime_encoder = le
        self.regime_scaler = scaler
    
    def predict(self, X: pd.DataFrame, min_confidence: float = 0.6) -> Dict:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞–Ω—Å–∞–º–±–ª—è —Å —É—á–µ—Ç–æ–º —Ä–µ–∂–∏–º–∞ —Ä—ã–Ω–∫–∞
        """
        # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∂–∏–º —Ä—ã–Ω–∫–∞
        X_regime_scaled = self.regime_scaler.transform(X)
        predicted_regime = self.regime_model.predict(X_regime_scaled)
        regime_proba = self.regime_model.predict_proba(X_regime_scaled)
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –≤—Å–µ—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
        strategy_predictions = {}
        for strategy_name, model in self.models.items():
            pred_result = model.predict_with_confidence(X, min_confidence)
            strategy_predictions[strategy_name] = pred_result
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∂–∏–º–∞ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        ensemble_prediction = self._combine_predictions(
            strategy_predictions, predicted_regime, regime_proba
        )
        
        return {
            'ensemble_prediction': ensemble_prediction,
            'strategy_predictions': strategy_predictions,
            'predicted_regime': predicted_regime,
            'regime_proba': regime_proba
        }
    
    def _combine_predictions(self, strategy_predictions: Dict, 
                           predicted_regime: np.ndarray,
                           regime_proba: np.ndarray) -> Dict:
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–π"""
        n_samples = len(predicted_regime)
        
        # –í–µ—Å–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞
        regime_weights = {
            0: {'trend_following': 0.4, 'momentum': 0.3, 'breakout': 0.2, 'mean_reversion': 0.1},
            1: {'mean_reversion': 0.4, 'trend_following': 0.3, 'breakout': 0.2, 'momentum': 0.1},
            2: {'breakout': 0.4, 'momentum': 0.3, 'trend_following': 0.2, 'mean_reversion': 0.1},
            3: {'mean_reversion': 0.5, 'breakout': 0.3, 'trend_following': 0.1, 'momentum': 0.1},
            4: {'trend_following': 0.5, 'momentum': 0.3, 'breakout': 0.1, 'mean_reversion': 0.1}
        }
        
        ensemble_proba = np.zeros(n_samples)
        ensemble_confidence = np.zeros(n_samples)
        
        for i in range(n_samples):
            regime = predicted_regime[i]
            weights = regime_weights.get(regime, regime_weights[0])
            
            weighted_proba = 0
            weighted_confidence = 0
            total_weight = 0
            
            for strategy, weight in weights.items():
                if strategy in strategy_predictions:
                    pred = strategy_predictions[strategy]
                    if i < len(pred['all_predictions']):
                        weighted_proba += pred['all_predictions'][i] * weight
                        weighted_confidence += pred['all_confidence'][i] * weight
                        total_weight += weight
            
            if total_weight > 0:
                ensemble_proba[i] = weighted_proba / total_weight
                ensemble_confidence[i] = weighted_confidence / total_weight
        
        return {
            'probabilities': ensemble_proba,
            'confidence': ensemble_confidence,
            'high_confidence_mask': ensemble_confidence >= 0.6,
            'coverage': (ensemble_confidence >= 0.6).mean()
        }


class AdvancedTrainingSystem:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config_dict = yaml.safe_load(f)
            
        self.db_config = {
            'host': self.config_dict['database']['host'],
            'port': self.config_dict['database']['port'],
            'database': self.config_dict['database']['database'],
            'user': self.config_dict['database']['user'],
            'password': self.config_dict['database']['password']
        }
        
        self.models = {}
        
    def load_advanced_data(self, symbols: List[str] = None, 
                          target_horizon: str = '1hour') -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        conn = psycopg2.connect(**self.db_config)
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º advanced targets + —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            query = f"""
            SELECT 
                a.timestamp,
                a.symbol,
                a.close_price,
                a.atr_14,
                a.volatility_percentile,
                a.market_regime,
                a.adaptive_threshold,
                a.buy_adaptive_{target_horizon} as target_direction,
                a.buy_strong_{target_horizon} as target_strong,
                a.return_normalized_{target_horizon} as target_return,
                a.risk_adjusted_return_{target_horizon} as target_risk_adj,
                a.trend_signal,
                a.reversion_signal,
                a.breakout_signal,
                a.momentum_signal,
                -- –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –∏–∑ processed_market_data
                (p.technical_indicators->>'rsi_val')::float as rsi_val,
                (p.technical_indicators->>'macd_val')::float as macd_val,
                (p.technical_indicators->>'macd_signal')::float as macd_signal,
                (p.technical_indicators->>'bb_upper')::float as bb_upper,
                (p.technical_indicators->>'bb_lower')::float as bb_lower,
                (p.technical_indicators->>'bb_percent')::float as bb_percent,
                (p.technical_indicators->>'atr_val')::float as atr_val,
                (p.technical_indicators->>'adx_val')::float as adx_val,
                (p.technical_indicators->>'stoch_k')::float as stoch_k,
                (p.technical_indicators->>'williams_r')::float as williams_r,
                (p.technical_indicators->>'cci_val')::float as cci_val,
                (p.technical_indicators->>'mfi_val')::float as mfi_val,
                (p.technical_indicators->>'obv_val')::float as obv_val,
                (p.technical_indicators->>'ema_9')::float as ema_9,
                (p.technical_indicators->>'ema_21')::float as ema_21,
                (p.technical_indicators->>'sma_50')::float as sma_50,
                (p.technical_indicators->>'vwap')::float as vwap
            FROM advanced_targets a
            JOIN processed_market_data p ON 
                EXTRACT(EPOCH FROM a.timestamp) * 1000 = p.timestamp 
                AND a.symbol = p.symbol
            WHERE a.buy_adaptive_{target_horizon} IS NOT NULL
            """
            
            params = []
            if symbols:
                placeholders = ','.join(['%s'] * len(symbols))
                query += f" AND a.symbol IN ({placeholders})"
                params.extend(symbols)
            
            query += " ORDER BY a.timestamp"
            
            df = pd.read_sql_query(query, conn, params=params)
            
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
            if 'target_direction' in df.columns:
                class_counts = df['target_direction'].value_counts()
                logger.info("\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
                for class_val, count in class_counts.items():
                    percent = count / len(df) * 100
                    logger.info(f"  - {class_val}: {count} ({percent:.1f}%)")
            
            return df
            
        finally:
            conn.close()
    
    def prepare_features_and_targets(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ"""
        
        # –ü—Ä–∏–∑–Ω–∞–∫–∏
        feature_cols = [
            'close_price', 'atr_14', 'volatility_percentile', 'adaptive_threshold',
            'rsi_val', 'macd_val', 'macd_signal', 'bb_upper', 'bb_lower', 'bb_percent',
            'atr_val', 'adx_val', 'stoch_k', 'williams_r', 'cci_val', 'mfi_val',
            'obv_val', 'ema_9', 'ema_21', 'sma_50', 'vwap'
        ]
        
        X = df[feature_cols].fillna(0)
        
        # –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
        targets = {
            'trend_following': {
                'direction': df['trend_signal'].fillna(0).astype(int),
                'confidence': self._calculate_confidence(df, 'trend_signal')
            },
            'mean_reversion': {
                'direction': df['reversion_signal'].fillna(0).astype(int),
                'confidence': self._calculate_confidence(df, 'reversion_signal')
            },
            'breakout': {
                'direction': df['breakout_signal'].fillna(0).astype(int),
                'confidence': self._calculate_confidence(df, 'breakout_signal')
            },
            'momentum': {
                'direction': df['momentum_signal'].fillna(0).astype(int),
                'confidence': self._calculate_confidence(df, 'momentum_signal')
            }
        }
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            'timestamp': df['timestamp'],
            'symbol': df['symbol'],
            'market_regime': df['market_regime'],
            'main_target': df['target_direction'].fillna(0).astype(int),
            'strong_target': df['target_strong'].fillna(0).astype(int)
        }
        
        return X, targets, metadata
    
    def _calculate_confidence(self, df: pd.DataFrame, signal_col: str) -> pd.Series:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Å–∏–≥–Ω–∞–ª–∞"""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ –∏ —Ä–µ–∂–∏–º–∞
        base_confidence = 0.5
        
        # –ë–æ–ª—å—à–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –Ω–∏–∑–∫–æ–π –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        vol_factor = 1 - df['volatility_percentile'].fillna(0.5)
        
        # –ë–æ–ª—å—à–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö —Ä–µ–∂–∏–º–æ–≤
        regime_factor = df['market_regime'].apply(
            lambda x: 0.8 if 'trending' in str(x).lower() else 0.6
        ).fillna(0.6)
        
        confidence = base_confidence + (vol_factor * 0.3) + (regime_factor * 0.2)
        confidence = np.clip(confidence, 0.2, 0.95)
        
        return confidence
    
    def train_with_time_series_cv(self, df: pd.DataFrame, n_splits: int = 5):
        """–û–±—É—á–µ–Ω–∏–µ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
        X, targets, metadata = self.prepare_features_and_targets(df)
        
        logger.info("\nüöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π")
        
        # –í—Ä–µ–º–µ–Ω–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        tscv = TimeSeriesSplit(n_splits=n_splits)
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        cv_results = []
        
        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
            logger.info(f"\n--- Fold {fold + 1}/{n_splits} ---")
            
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            
            # –ê–Ω—Å–∞–º–±–ª—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
            ensemble = StrategyEnsemble()
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            train_targets = {}
            for strategy in targets:
                train_targets[strategy] = {
                    'direction': targets[strategy]['direction'].iloc[train_idx],
                    'confidence': targets[strategy]['confidence'].iloc[train_idx]
                }
            
            # –û–±—É—á–∞–µ–º –∞–Ω—Å–∞–º–±–ª—å
            ensemble.fit(
                X_train, 
                train_targets, 
                metadata['market_regime'].iloc[train_idx]
            )
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            val_results = ensemble.predict(X_val, min_confidence=0.6)
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            val_target = metadata['main_target'].iloc[val_idx]
            
            # Overall accuracy
            ensemble_pred = (val_results['ensemble_prediction']['probabilities'] > 0.5).astype(int)
            accuracy = accuracy_score(val_target, ensemble_pred)
            
            # High-confidence accuracy
            high_conf_mask = val_results['ensemble_prediction']['high_confidence_mask']
            if high_conf_mask.sum() > 0:
                high_conf_accuracy = accuracy_score(
                    val_target[high_conf_mask],
                    ensemble_pred[high_conf_mask]
                )
                coverage = high_conf_mask.mean()
            else:
                high_conf_accuracy = 0
                coverage = 0
            
            # ROC-AUC
            try:
                roc_auc = roc_auc_score(val_target, val_results['ensemble_prediction']['probabilities'])
            except:
                roc_auc = 0.5
            
            fold_results = {
                'fold': fold + 1,
                'accuracy': accuracy,
                'high_confidence_accuracy': high_conf_accuracy,
                'coverage': coverage,
                'roc_auc': roc_auc,
                'train_size': len(train_idx),
                'val_size': len(val_idx)
            }
            
            cv_results.append(fold_results)
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            logger.info(f"Accuracy: {accuracy:.3f}")
            logger.info(f"High-conf accuracy: {high_conf_accuracy:.3f}")
            logger.info(f"Coverage: {coverage:.3f}")
            logger.info(f"ROC-AUC: {roc_auc:.3f}")
        
        # –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self._log_cv_summary(cv_results)
        
        # –û–±—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
        logger.info("\nüéØ –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö...")
        final_ensemble = StrategyEnsemble()
        final_ensemble.fit(X, targets, metadata['market_regime'])
        
        return final_ensemble, cv_results
    
    def _log_cv_summary(self, cv_results: List[Dict]):
        """–õ–æ–≥–∏—Ä—É–µ—Ç —Å–≤–æ–¥–∫—É –ø–æ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        df_results = pd.DataFrame(cv_results)
        
        logger.info("\nüìä –°–í–û–î–ö–ê –ü–û –ö–†–û–°–°-–í–ê–õ–ò–î–ê–¶–ò–ò:")
        logger.info("-" * 50)
        
        for metric in ['accuracy', 'high_confidence_accuracy', 'coverage', 'roc_auc']:
            mean_val = df_results[metric].mean()
            std_val = df_results[metric].std()
            logger.info(f"{metric}: {mean_val:.3f} ¬± {std_val:.3f}")
        
        logger.info(f"\n–°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {df_results['val_size'].mean():.0f}")
        logger.info(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ fold: {len(cv_results)}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse
    
    parser = argparse.ArgumentParser(description="–û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π")
    parser.add_argument('--symbols', nargs='+', default=['BTCUSDT'],
                       help='–°–∏–º–≤–æ–ª—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è')
    parser.add_argument('--horizon', default='1hour',
                       choices=['15min', '1hour', '4hour', '16hour'],
                       help='–í—Ä–µ–º–µ–Ω–Ω–æ–π –≥–æ—Ä–∏–∑–æ–Ω—Ç')
    parser.add_argument('--cv-splits', type=int, default=5,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ fold –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏')
    
    args = parser.parse_args()
    
    logger.info("""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë         –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö ML –º–æ–¥–µ–ª–µ–π             ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    # –°–æ–∑–¥–∞–µ–º —Å–∏—Å—Ç–µ–º—É –æ–±—É—á–µ–Ω–∏—è
    training_system = AdvancedTrainingSystem('config.yaml')
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    df = training_system.load_advanced_data(
        symbols=args.symbols,
        target_horizon=args.horizon
    )
    
    if len(df) == 0:
        logger.error("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
        logger.info("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python advanced_trading_system.py --test")
        return
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏
    final_model, cv_results = training_system.train_with_time_series_cv(
        df, n_splits=args.cv_splits
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    model_dir = Path(f'advanced_models_{timestamp}')
    model_dir.mkdir(exist_ok=True)
    
    joblib.dump(final_model, model_dir / 'ensemble_model.pkl')
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã CV
    pd.DataFrame(cv_results).to_csv(model_dir / 'cv_results.csv', index=False)
    
    logger.info(f"\n‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_dir}")
    logger.info("\nüìù –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
    logger.info("1. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏")
    logger.info("2. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –º–æ–¥–µ–ª—å –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    logger.info("3. –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–π—Ç–µ –≤ —Ç–æ—Ä–≥–æ–≤—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é")


if __name__ == "__main__":
    main()