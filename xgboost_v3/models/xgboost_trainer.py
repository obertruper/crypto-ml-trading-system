"""
–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è XGBoost –º–æ–¥–µ–ª–µ–π
"""

import xgboost as xgb
import numpy as np
import pandas as pd
import logging
from typing import Dict, Tuple, Optional, Union, List
from pathlib import Path
import joblib
import json

from config import Config
from config.constants import PREDICTION_PARAMS
from utils.metrics import MetricsCalculator
from models.data_balancer import DataBalancer

logger = logging.getLogger(__name__)


class XGBoostTrainer:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è XGBoost –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self, config: Config, model_name: str = "xgboost_model"):
        self.config = config
        self.model_name = model_name
        self.model = None
        self.metrics_calculator = MetricsCalculator(config)
        self.data_balancer = DataBalancer(config)
        self.feature_importance = None
        self.optimal_threshold = None  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
        self.training_history = {
            'train': {'loss': [], 'metric': []},
            'val': {'loss': [], 'metric': []}
        }
        
    def train(self, 
             X_train: pd.DataFrame, 
             y_train: pd.Series,
             X_val: pd.DataFrame,
             y_val: pd.Series,
             model_params: Optional[Dict] = None) -> xgb.Booster:
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ XGBoost
        
        Args:
            X_train: –û–±—É—á–∞—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            y_train: –û–±—É—á–∞—é—â–∏–µ –º–µ—Ç–∫–∏
            X_val: –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            y_val: –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
            model_params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
            
        Returns:
            –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å XGBoost
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: {self.model_name}")
        logger.info(f"{'='*60}")
        
        # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if self.config.training.task_type != "regression" and self.config.training.balance_method != "none":
            X_train, y_train = self.data_balancer.balance_data(X_train, y_train, is_classification=True)
            
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        if model_params is None:
            model_params = self.config.model.to_dict()
            
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ scale_pos_weight –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
        if self.config.training.task_type == "classification_binary" and model_params.get('scale_pos_weight') is None:
            scale_pos_weight = self._calculate_scale_pos_weight(y_train)
            model_params['scale_pos_weight'] = scale_pos_weight
            
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        if self._check_gpu_available():
            model_params['tree_method'] = 'gpu_hist'
            model_params['predictor'] = 'gpu_predictor'
            logger.info("üñ•Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        else:
            model_params['tree_method'] = 'hist'
            model_params['predictor'] = 'cpu_predictor'
            
        # –°–æ–∑–¥–∞–Ω–∏–µ DMatrix
        dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=list(X_train.columns))
        dval = xgb.DMatrix(X_val, label=y_val, feature_names=list(X_val.columns))
        
        # Callback –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏
        evals_result = {}
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        logger.info("üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
        logger.info(f"   –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {X_train.shape}")
        logger.info(f"   –†–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: {X_val.shape}")
        
        self.model = xgb.train(
            params=model_params,
            dtrain=dtrain,
            num_boost_round=self.config.model.n_estimators,
            evals=[(dtrain, 'train'), (dval, 'val')],
            early_stopping_rounds=self.config.model.early_stopping_rounds,
            verbose_eval=100,
            evals_result=evals_result
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è
        self._save_training_history(evals_result)
        
        # –í—ã—á–∏—Å–ª—è–µ–º feature importance
        self._calculate_feature_importance()
        
        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        val_metrics = self.evaluate(X_val, y_val, dataset_name="Validation")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        if self.config.training.task_type == "classification_binary":
            val_proba = self.predict(X_val, return_proba=True)
            logger.info(f"\nüìä –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:")
            logger.info(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π: min={val_proba.min():.3f}, max={val_proba.max():.3f}, mean={val_proba.mean():.3f}")
            logger.info(f"   –ö–≤–∞–Ω—Ç–∏–ª–∏: 25%={np.percentile(val_proba, 25):.3f}, 50%={np.percentile(val_proba, 50):.3f}, 75%={np.percentile(val_proba, 75):.3f}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏
            for thr in [0.3, 0.4, 0.5, 0.6, 0.7]:
                n_pos = (val_proba > thr).sum()
                logger.info(f"   –ü–æ—Ä–æ–≥ {thr:.1f}: {n_pos} –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö ({n_pos/len(val_proba)*100:.1f}%)")
        
        logger.info(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. Best iteration: {self.model.best_iteration}")
        
        return self.model
        
    def predict(self, X: pd.DataFrame, return_proba: bool = False) -> np.ndarray:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        
        Args:
            X: –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            return_proba: –í–µ—Ä–Ω—É—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (–¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)
            
        Returns:
            –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
        """
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ train()")
            
        dmatrix = xgb.DMatrix(X, feature_names=list(X.columns))
        predictions = self.model.predict(dmatrix)
        
        if self.config.training.task_type == "classification_binary" and not return_proba:
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –µ—Å–ª–∏ –æ–Ω –±—ã–ª –Ω–∞–π–¥–µ–Ω, –∏–Ω–∞—á–µ - –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π
            threshold = self.optimal_threshold if self.optimal_threshold is not None else PREDICTION_PARAMS['probability_threshold']
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ (–¥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è)
            if logger.isEnabledFor(logging.INFO):
                logger.info(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏:")
                logger.info(f"   Min: {predictions.min():.3f}, Max: {predictions.max():.3f}, Mean: {predictions.mean():.3f}")
                n_positive = (predictions > threshold).sum()
                logger.info(f"   –ü–æ—Ä–æ–≥: {threshold:.3f} {'(–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π)' if self.optimal_threshold is not None else '(–¥–µ—Ñ–æ–ª—Ç–Ω—ã–π)'}")
                logger.info(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ –∫–ª–∞—Å—Å 1: {n_positive} –∏–∑ {len(predictions)} ({n_positive/len(predictions)*100:.1f}%)")
            
            predictions = (predictions > threshold).astype(int)
            
        return predictions
        
    def evaluate(self, X: pd.DataFrame, y: pd.Series, dataset_name: str = "Test") -> Dict[str, float]:
        """
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            X: –ü—Ä–∏–∑–Ω–∞–∫–∏
            y: –ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
            dataset_name: –ù–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        predictions = self.predict(X, return_proba=True)
        
        if self.config.training.task_type == "regression":
            metrics = self.metrics_calculator.calculate_regression_metrics(y, predictions)
        else:
            # –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
            metrics = self.metrics_calculator.calculate_classification_metrics(y, predictions)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –µ—Å–ª–∏ —ç—Ç–æ –≤–∞–ª–∏–¥–∞—Ü–∏—è
            if dataset_name == "Validation" and 'threshold' in metrics:
                self.optimal_threshold = metrics['threshold']
                logger.info(f"üí° –°–æ—Ö—Ä–∞–Ω–µ–Ω –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {self.optimal_threshold:.4f}")
            
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        logger.info(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ {dataset_name}:")
        for metric_name, value in metrics.items():
            if isinstance(value, float):
                logger.info(f"   {metric_name}: {value:.4f}")
                
        return metrics
        
    def save_model(self, save_dir: Path):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        save_dir = Path(save_dir)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        model_path = save_dir / f"{self.model_name}.pkl"
        joblib.dump(self.model, model_path)
        logger.info(f"   ‚úÖ {model_path.name}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            'model_name': self.model_name,
            'task_type': self.config.training.task_type,
            'best_iteration': self.model.best_iteration if self.model else None,
            'optimal_threshold': self.optimal_threshold,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
            'feature_importance': self.feature_importance,
            'training_history': self.training_history,
            'config': {
                'model': self.config.model.__dict__,
                'training': self.config.training.__dict__
            }
        }
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º numpy —Ç–∏–ø—ã –≤ –æ–±—ã—á–Ω—ã–µ Python —Ç–∏–ø—ã –¥–ª—è JSON
        def convert_to_json_serializable(obj):
            if isinstance(obj, np.float32) or isinstance(obj, np.float64):
                return float(obj)
            elif isinstance(obj, np.int32) or isinstance(obj, np.int64):
                return int(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert_to_json_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_to_json_serializable(v) for v in obj]
            return obj
        
        metadata = convert_to_json_serializable(metadata)
        
        metadata_path = save_dir / f"{self.model_name}_metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
            
    def load_model(self, model_path: Path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        self.model = joblib.load(model_path)
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {model_path}")
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata_path = model_path.parent / f"{model_path.stem}_metadata.json"
        if metadata_path.exists():
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
                self.optimal_threshold = metadata.get('optimal_threshold')
                if self.optimal_threshold:
                    logger.info(f"   üí° –ó–∞–≥—Ä—É–∂–µ–Ω –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {self.optimal_threshold:.4f}")
        
    def _calculate_scale_pos_weight(self, y_train: pd.Series) -> float:
        """–†–∞—Å—á–µ—Ç scale_pos_weight –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤"""
        n_positive = (y_train == 1).sum()
        n_negative = (y_train == 0).sum()
        
        if n_positive == 0:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ!")
            return 1.0
            
        scale_pos_weight = n_negative / n_positive
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
        max_scale = min(3.0, n_negative / n_positive * 0.7)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 3 –∏ –±–µ—Ä–µ–º 70% –æ—Ç —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è
        if scale_pos_weight > max_scale:
            logger.warning(f"‚ö†Ô∏è –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π scale_pos_weight: {scale_pos_weight:.2f}, –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ {max_scale:.2f}")
            scale_pos_weight = max_scale
        
        logger.info(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
        logger.info(f"   –ö–ª–∞—Å—Å 0 (–Ω–µ –≤—Ö–æ–¥–∏—Ç—å): {n_negative:,} ({n_negative/(n_negative+n_positive)*100:.1f}%)")
        logger.info(f"   –ö–ª–∞—Å—Å 1 (–≤—Ö–æ–¥–∏—Ç—å): {n_positive:,} ({n_positive/(n_negative+n_positive)*100:.1f}%)")
        logger.info(f"   scale_pos_weight = {scale_pos_weight:.2f}")
        
        return scale_pos_weight
        
    def _check_gpu_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU"""
        try:
            import GPUtil
            gpus = GPUtil.getGPUs()
            return len(gpus) > 0
        except:
            return False
            
    def _save_training_history(self, evals_result: Dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è"""
        # XGBoost –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –≤ —Ñ–æ—Ä–º–∞—Ç–µ {dataset: {metric: [values]}}
        for dataset in evals_result:
            for metric in evals_result[dataset]:
                if dataset == 'train':
                    self.training_history['train']['metric'] = evals_result[dataset][metric]
                elif dataset == 'val':
                    self.training_history['val']['metric'] = evals_result[dataset][metric]
                    
    def _calculate_feature_importance(self):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        if self.model is None:
            return
            
        try:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ–ª—É—á–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏
            importance = self.model.get_score(importance_type='gain')
            
            if not importance:
                # –ï—Å–ª–∏ gain –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –ø—Ä–æ–±—É–µ–º weight
                importance = self.model.get_score(importance_type='weight')
                
            if not importance:
                # –ï—Å–ª–∏ –∏ weight –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –ø—Ä–æ–±—É–µ–º cover
                importance = self.model.get_score(importance_type='cover')
                
            if not importance:
                # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º get_fscore
                importance = self.model.get_fscore()
                
            if not importance:
                logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å feature importance –∏–∑ –º–æ–¥–µ–ª–∏")
                self.feature_importance = {}
                return
                
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
            self.feature_importance = dict(sorted(importance.items(), 
                                                key=lambda x: x[1], 
                                                reverse=True))
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            logger.info("\nüìä –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
            for i, (feature, score) in enumerate(list(self.feature_importance.items())[:10]):
                logger.info(f"   {i+1}. {feature}: {score:.2f}")
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ feature importance: {e}")
            self.feature_importance = {}
            
    def get_feature_importance(self) -> pd.DataFrame:
        """–ü–æ–ª—É—á–∏—Ç—å feature importance –∫–∞–∫ DataFrame"""
        if self.feature_importance is None:
            return pd.DataFrame()
            
        df = pd.DataFrame(list(self.feature_importance.items()), 
                         columns=['feature', 'importance'])
        df = df.sort_values('importance', ascending=False).reset_index(drop=True)
        
        return df