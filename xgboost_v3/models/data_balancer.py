"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤—â–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è XGBoost v3.0
"""

import numpy as np
import pandas as pd
from sklearn.utils import class_weight
from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler
import logging
from typing import Tuple, Optional, Dict

from config import Config

logger = logging.getLogger(__name__)


class DataBalancer:
    """–ö–ª–∞—Å—Å –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, config: Config):
        self.config = config
        self.method = config.training.balance_method
        
    def balance_data(self, X: pd.DataFrame, y: pd.Series, 
                    is_classification: bool = True) -> Tuple[pd.DataFrame, pd.Series]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        """
        if not is_classification or self.method == "none":
            return X, y
            
        logger.info(f"üîÑ –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–º: {self.method}")
        
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∏–Ω–¥–µ–∫—Å—ã —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω—ã
        X = X.reset_index(drop=True)
        y = y.reset_index(drop=True)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
        unique, counts = np.unique(y, return_counts=True)
        class_dist = dict(zip(unique, counts))
        logger.info(f"   –î–æ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: {class_dist}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤
        min_class_count = min(counts)
        if min_class_count < 6:
            logger.warning(f"‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –º–µ–Ω—å—à–µ–º –∫–ª–∞—Å—Å–µ ({min_class_count}), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É")
            return X, y
        
        # –í—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
        if self.method == "smote":
            X_balanced, y_balanced = self._apply_smote(X, y)
        elif self.method == "adasyn":
            X_balanced, y_balanced = self._apply_adasyn(X, y)
        elif self.method == "class_weight":
            # –î–ª—è class_weight –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å–∞, –Ω–µ –º–µ–Ω—è—è –¥–∞–Ω–Ω—ã–µ
            weights = self._calculate_class_weights(y)
            logger.info(f"   –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤: {weights}")
            return X, y
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: {self.method}")
            
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
        unique, counts = np.unique(y_balanced, return_counts=True)
        class_dist = dict(zip(unique, counts))
        logger.info(f"   –ü–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: {class_dist}")
            
        return pd.DataFrame(X_balanced, columns=X.columns), pd.Series(y_balanced)
        
    def _apply_smote(self, X: pd.DataFrame, y: pd.Series) -> Tuple[np.ndarray, np.ndarray]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SMOTE –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—ç–º–ø–ª–æ–≤ –≤ –º–µ–Ω—å—à–µ–º –∫–ª–∞—Å—Å–µ
            unique, counts = np.unique(y, return_counts=True)
            min_samples = min(counts)
            
            # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º k_neighbors –ø–æ–¥ —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
            k_neighbors = min(self.config.training.smote_k_neighbors, min_samples - 1)
            
            if k_neighbors < 1:
                logger.warning("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è SMOTE, –∏—Å–ø–æ–ª—å–∑—É–µ–º RandomOverSampler")
                ros = RandomOverSampler(sampling_strategy='auto', random_state=42)
                return ros.fit_resample(X, y)
                
            # –ò–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            binary_cols = self._identify_binary_columns(X)
            continuous_cols = [col for col in X.columns if col not in binary_cols]
            
            if len(continuous_cols) < 2:
                logger.warning("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è SMOTE, –∏—Å–ø–æ–ª—å–∑—É–µ–º RandomOverSampler")
                ros = RandomOverSampler(sampling_strategy='auto', random_state=42)
                return ros.fit_resample(X, y)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º SMOTE —Ç–æ–ª—å–∫–æ –∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º
            smote = SMOTE(
                sampling_strategy='auto',
                k_neighbors=k_neighbors,
                random_state=42
            )
            
            if binary_cols:
                # –ü—Ä–∏–º–µ–Ω—è–µ–º SMOTE —Ç–æ–ª—å–∫–æ –∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º
                X_continuous = X[continuous_cols]
                X_cont_resampled, y_resampled = smote.fit_resample(X_continuous, y)
                
                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
                n_synthetic = len(y_resampled) - len(y)
                
                if n_synthetic > 0:
                    # –î–ª—è –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É –∏–∑ minority –∫–ª–∞—Å—Å–∞
                    minority_mask = y == 1
                    minority_binary = X.loc[minority_mask, binary_cols]
                    
                    # –°–ª—É—á–∞–π–Ω–æ –≤—ã–±–∏—Ä–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ minority –∫–ª–∞—Å—Å–∞
                    synthetic_indices = np.random.choice(
                        len(minority_binary), 
                        size=n_synthetic, 
                        replace=True
                    )
                    synthetic_binary = minority_binary.iloc[synthetic_indices].values
                    
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    X_resampled = np.column_stack([
                        X_cont_resampled,
                        np.vstack([X[binary_cols].values, synthetic_binary])
                    ])
                    
                    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫
                    col_order = continuous_cols + binary_cols
                    X_resampled_df = pd.DataFrame(X_resampled, columns=col_order)
                    X_resampled = X_resampled_df[X.columns].values
                else:
                    X_resampled = X_cont_resampled
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –ø—Ä–∏–º–µ–Ω—è–µ–º SMOTE –∫–æ –≤—Å–µ–º
                X_resampled, y_resampled = smote.fit_resample(X, y)
            
            logger.info(f"   ‚úÖ SMOTE –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ (k_neighbors={k_neighbors})")
            
            return X_resampled, y_resampled
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ SMOTE: {e}")
            logger.warning("   –ò—Å–ø–æ–ª—å–∑—É–µ–º RandomOverSampler –∫–∞–∫ fallback")
            ros = RandomOverSampler(sampling_strategy='auto', random_state=42)
            return ros.fit_resample(X, y)
            
    def _apply_adasyn(self, X: pd.DataFrame, y: pd.Series) -> Tuple[np.ndarray, np.ndarray]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ ADASYN –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è —Å–µ—Ä–≤–µ—Ä–æ–≤"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—ç–º–ø–ª–æ–≤
            unique, counts = np.unique(y, return_counts=True)
            min_samples = min(counts)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–µ—Ä–≤–µ—Ä–∞
            import multiprocessing
            cpu_count = multiprocessing.cpu_count()
            
            if cpu_count > 64:
                # –î–ª—è –º–æ—â–Ω—ã—Ö —Å–µ—Ä–≤–µ—Ä–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª—å—à–µ —Å–æ—Å–µ–¥–µ–π –∏ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º
                n_neighbors = min(15, min_samples - 1, len(y) // 1000)
                n_jobs = min(32, cpu_count // 4)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º 1/4 —è–¥–µ—Ä
                logger.info(f"   üöÄ –ú–æ—â–Ω—ã–π —Å–µ—Ä–≤–µ—Ä: n_neighbors={n_neighbors}, n_jobs={n_jobs}")
            else:
                # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º n_neighbors –¥–ª—è –æ–±—ã—á–Ω—ã—Ö —Å–∏—Å—Ç–µ–º
                n_neighbors = min(5, min_samples - 1)
                n_jobs = 1
            
            if n_neighbors < 1:
                logger.warning("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ADASYN, –∏—Å–ø–æ–ª—å–∑—É–µ–º RandomOverSampler")
                ros = RandomOverSampler(sampling_strategy='auto', random_state=42)
                return ros.fit_resample(X, y)
                
            adasyn = ADASYN(
                sampling_strategy='auto',
                n_neighbors=n_neighbors,
                random_state=42,
                n_jobs=n_jobs
            )
            
            X_resampled, y_resampled = adasyn.fit_resample(X, y)
            logger.info(f"   ‚úÖ ADASYN –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ (n_neighbors={n_neighbors})")
            
            return X_resampled, y_resampled
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ ADASYN: {e}")
            logger.warning("   –ò—Å–ø–æ–ª—å–∑—É–µ–º RandomOverSampler –∫–∞–∫ fallback")
            ros = RandomOverSampler(sampling_strategy='auto', random_state=42)
            return ros.fit_resample(X, y)
            
    def _identify_binary_columns(self, X: pd.DataFrame) -> list:
        """–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –±–∏–Ω–∞—Ä–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫"""
        binary_cols = []
        
        for col in X.columns:
            unique_vals = X[col].dropna().unique()
            # –°—á–∏—Ç–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–º–∏ —Ç–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–∫–∏ —Å–æ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ 0 –∏ 1
            if len(unique_vals) <= 2 and all(val in [0, 1, 0.0, 1.0] for val in unique_vals):
                binary_cols.append(col)
            # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏ –±–∏–Ω–∞—Ä–Ω—ã—Ö –∏–º–µ–Ω
            elif any(pattern in col.lower() for pattern in ['is_', '_oversold', '_overbought', 
                                                            'bullish', 'bearish', 'spike',
                                                            'strong_trend', 'bb_near_']):
                # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
                X[col] = (X[col] != 0).astype(int)
                binary_cols.append(col)
                
        if binary_cols:
            logger.info(f"   üìä –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(binary_cols)} –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            
        return binary_cols
            
    def _calculate_class_weights(self, y: pd.Series) -> dict:
        """–†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏"""
        weights = class_weight.compute_class_weight(
            'balanced',
            classes=np.unique(y),
            y=y
        )
        
        return dict(zip(np.unique(y), weights))
        
    def get_scale_pos_weight(self, y: pd.Series) -> Optional[float]:
        """
        –†–∞—Å—á–µ—Ç scale_pos_weight –¥–ª—è XGBoost
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        if len(np.unique(y)) != 2:
            return None
            
        neg_count = (y == 0).sum()
        pos_count = (y == 1).sum()
        
        if pos_count == 0:
            return None
            
        scale_pos_weight = neg_count / pos_count
        
        logger.info(f"   scale_pos_weight = {scale_pos_weight:.2f} (neg: {neg_count}, pos: {pos_count})")
        
        return scale_pos_weight