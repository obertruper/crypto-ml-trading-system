"""
Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ XGBoost
"""

import optuna
from optuna.pruners import MedianPruner
from optuna.samplers import TPESampler
import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import StratifiedKFold, KFold
from sklearn.metrics import mean_absolute_error, roc_auc_score
import logging
from typing import Dict, Tuple, Optional, Callable

from config import Config
from config.constants import OPTUNA_PARAMS

logger = logging.getLogger(__name__)


class OptunaOptimizer:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é Optuna"""
    
    def __init__(self, config: Config):
        self.config = config
        self.best_params = None
        self.best_score = None
        self.study = None
        
    def optimize(self,
                X_train: pd.DataFrame,
                y_train: pd.Series,
                n_trials: Optional[int] = None,
                direction: Optional[str] = None,
                model_type: str = "buy") -> Dict:
        """
        –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        
        Args:
            X_train: –û–±—É—á–∞—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            y_train: –û–±—É—á–∞—é—â–∏–µ –º–µ—Ç–∫–∏
            n_trials: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            direction: –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ ("minimize" –∏–ª–∏ "maximize")
            model_type: –¢–∏–ø –º–æ–¥–µ–ª–∏ (buy/sell)
            
        Returns:
            –õ—É—á—à–∏–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        """
        if n_trials is None:
            n_trials = self.config.training.optuna_trials
            
        if direction is None:
            direction = "minimize" if self.config.training.task_type == "regression" else "maximize"
            
        logger.info(f"\nüîß –ó–∞–ø—É—Å–∫ Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...")
        logger.info(f"üîç –ù–∞—á–∞–ª–æ Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è {model_type}...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Å–µ—Ä–≤–µ—Ä–∞
        import multiprocessing
        import time
        cpu_count = multiprocessing.cpu_count()
        
        # –°–æ–∑–¥–∞–µ–º study —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –¥–ª—è –º–æ—â–Ω—ã—Ö —Å–µ—Ä–≤–µ—Ä–æ–≤
        if cpu_count > 64:
            logger.info(f"üöÄ –ú–æ—â–Ω—ã–π —Å–µ—Ä–≤–µ—Ä ({cpu_count} CPU) - –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–µ–∑ –º–µ–∂–ø—Ä–æ—Ü–µ—Å—Å–Ω–æ–π —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏")
            # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º storage –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
            # XGBoost —Å–∞–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–µ —è–¥—Ä–∞
            
            self.study = optuna.create_study(
                direction=direction,
                sampler=TPESampler(seed=OPTUNA_PARAMS['random_state']),
                pruner=MedianPruner(n_warmup_steps=OPTUNA_PARAMS['pruner_warmup_steps'])
            )
            
            # –û—Ç–∫–ª—é—á–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º Optuna, —Ç–∞–∫ –∫–∞–∫ XGBoost —Å–∞–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏—Ç—Å—è
            logger.info(f"   XGBoost –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ {cpu_count} —è–¥–µ—Ä")
            
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ Optuna
            self.study.optimize(
                lambda trial: self._objective(trial, X_train, y_train),
                n_trials=n_trials,
                n_jobs=1,  # –û–¥–Ω–æ–ø–æ—Ç–æ—á–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è, XGBoost —Å–∞–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏—Ç—Å—è
                show_progress_bar=True
            )
        else:
            # –û–±—ã—á–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö —Å–∏—Å—Ç–µ–º
            self.study = optuna.create_study(
                direction=direction,
                sampler=TPESampler(seed=OPTUNA_PARAMS['random_state']),
                pruner=MedianPruner(n_warmup_steps=OPTUNA_PARAMS['pruner_warmup_steps'])
            )
            
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
            self.study.optimize(
                lambda trial: self._objective(trial, X_train, y_train),
                n_trials=n_trials,
                show_progress_bar=True
            )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.best_params = self.study.best_params
        self.best_score = self.study.best_value
        
        logger.info(f"\n‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        logger.info(f"üèÜ –õ—É—á—à–∏–π score: {self.best_score:.5f}")
        logger.info(f"üéØ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
        for param, value in self.best_params.items():
            logger.info(f"   {param}: {value}")
            
        return self.best_params
        
    def _objective(self, trial: optuna.Trial, X: pd.DataFrame, y: pd.Series) -> float:
        """–¶–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        
        # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        params = {
            'objective': self.config.model.objective,
            'eval_metric': self.config.model.eval_metric,
            'max_depth': trial.suggest_int('max_depth', 
                                         OPTUNA_PARAMS['max_depth']['min'], 
                                         OPTUNA_PARAMS['max_depth']['max']),
            'learning_rate': trial.suggest_float('learning_rate', 
                                               OPTUNA_PARAMS['learning_rate']['min'], 
                                               OPTUNA_PARAMS['learning_rate']['max'], 
                                               log=OPTUNA_PARAMS['learning_rate']['log']),
            'subsample': trial.suggest_float('subsample', 
                                           OPTUNA_PARAMS['subsample']['min'], 
                                           OPTUNA_PARAMS['subsample']['max']),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 
                                                   OPTUNA_PARAMS['colsample_bytree']['min'], 
                                                   OPTUNA_PARAMS['colsample_bytree']['max']),
            'colsample_bylevel': trial.suggest_float('colsample_bylevel', 
                                                    OPTUNA_PARAMS['colsample_bylevel']['min'], 
                                                    OPTUNA_PARAMS['colsample_bylevel']['max']),
            'min_child_weight': trial.suggest_int('min_child_weight', 
                                                OPTUNA_PARAMS['min_child_weight']['min'], 
                                                OPTUNA_PARAMS['min_child_weight']['max']),
            'gamma': trial.suggest_float('gamma', 
                                       OPTUNA_PARAMS['gamma']['min'], 
                                       OPTUNA_PARAMS['gamma']['max']),
            'reg_alpha': trial.suggest_float('reg_alpha', 
                                           OPTUNA_PARAMS['reg_alpha']['min'], 
                                           OPTUNA_PARAMS['reg_alpha']['max']),
            'reg_lambda': trial.suggest_float('reg_lambda', 
                                            OPTUNA_PARAMS['reg_lambda']['min'], 
                                            OPTUNA_PARAMS['reg_lambda']['max']),
            'n_jobs': -1,
            'random_state': OPTUNA_PARAMS['random_state'],
            'verbosity': 0
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º scale_pos_weight –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        if self.config.training.task_type == "classification_binary":
            n_positive = (y == 1).sum()
            n_negative = (y == 0).sum()
            if n_positive > 0:
                scale_pos_weight = n_negative / n_positive
                # –ü–æ–∑–≤–æ–ª—è–µ–º Optuna –Ω–µ–º–Ω–æ–≥–æ –≤–∞—Ä—å–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä
                params['scale_pos_weight'] = trial.suggest_float(
                    'scale_pos_weight', 
                    scale_pos_weight * OPTUNA_PARAMS['scale_pos_weight_factor']['min'], 
                    scale_pos_weight * OPTUNA_PARAMS['scale_pos_weight_factor']['max']
                )
                
        # GPU –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if self._check_gpu_available():
            params['tree_method'] = 'gpu_hist'
            params['predictor'] = 'gpu_predictor'
        else:
            params['tree_method'] = 'hist'
            
        # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        scores = self._cross_validate(X, y, params)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        return np.mean(scores)
        
    def _cross_validate(self, X: pd.DataFrame, y: pd.Series, params: Dict) -> np.ndarray:
        """–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        n_folds = self.config.training.optuna_cv_folds
        
        # –í—ã–±–∏—Ä–∞–µ–º —Ç–∏–ø —Ä–∞–∑–±–∏–µ–Ω–∏—è
        if self.config.training.task_type == "regression":
            kf = KFold(n_splits=n_folds, shuffle=True, random_state=OPTUNA_PARAMS['random_state'])
            splits = kf.split(X)
        else:
            kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=OPTUNA_PARAMS['random_state'])
            splits = kf.split(X, y)
            
        scores = []
        
        for fold, (train_idx, val_idx) in enumerate(splits):
            X_fold_train = X.iloc[train_idx]
            y_fold_train = y.iloc[train_idx]
            X_fold_val = X.iloc[val_idx]
            y_fold_val = y.iloc[val_idx]
            
            # –°–æ–∑–¥–∞–µ–º DMatrix
            dtrain = xgb.DMatrix(X_fold_train, label=y_fold_train)
            dval = xgb.DMatrix(X_fold_val, label=y_fold_val)
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            model = xgb.train(
                params=params,
                dtrain=dtrain,
                num_boost_round=OPTUNA_PARAMS['cv_n_estimators'],
                evals=[(dval, 'val')],
                early_stopping_rounds=OPTUNA_PARAMS['cv_early_stopping_rounds'],
                verbose_eval=False
            )
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º
            predictions = model.predict(dval)
            
            if self.config.training.task_type == "regression":
                score = mean_absolute_error(y_fold_val, predictions)
            else:
                # –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º ROC-AUC
                try:
                    score = roc_auc_score(y_fold_val, predictions)
                except:
                    score = 0.5
                    
            scores.append(score)
            
        return np.array(scores)
        
    def _check_gpu_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU"""
        try:
            import GPUtil
            gpus = GPUtil.getGPUs()
            return len(gpus) > 0
        except:
            return False
            
    def get_optimization_history(self) -> pd.DataFrame:
        """–ü–æ–ª—É—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–∞–∫ DataFrame"""
        if self.study is None:
            return pd.DataFrame()
            
        history = []
        
        for trial in self.study.trials:
            trial_data = {
                'number': trial.number,
                'value': trial.value,
                'state': trial.state.name,
                **trial.params
            }
            history.append(trial_data)
            
        df = pd.DataFrame(history)
        return df.sort_values('value', ascending=(self.study.direction.name == 'MINIMIZE'))
        
    def plot_optimization_history(self, save_path: Optional[str] = None):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        if self.study is None:
            return
            
        import matplotlib.pyplot as plt
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Optuna Optimization History', fontsize=16)
        
        # 1. –ò—Å—Ç–æ—Ä–∏—è –∑–Ω–∞—á–µ–Ω–∏–π
        ax = axes[0, 0]
        trials_df = self.get_optimization_history()
        completed_trials = trials_df[trials_df['state'] == 'COMPLETE']
        
        ax.plot(completed_trials['number'], completed_trials['value'], 'b-', alpha=0.5)
        ax.scatter(completed_trials['number'], completed_trials['value'], c='blue', alpha=0.5)
        
        # –õ—É—á—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        best_idx = completed_trials['value'].idxmin() if self.study.direction.name == 'MINIMIZE' else completed_trials['value'].idxmax()
        best_trial = completed_trials.loc[best_idx]
        ax.scatter(best_trial['number'], best_trial['value'], c='red', s=100, label=f'Best: {best_trial["value"]:.5f}')
        
        ax.set_xlabel('Trial')
        ax.set_ylabel('Objective Value')
        ax.set_title('Optimization History')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 2. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è –≤–∞–∂–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        ax = axes[0, 1]
        important_params = ['max_depth', 'learning_rate', 'subsample', 'colsample_bytree']
        
        if len(completed_trials) > 0:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
            param_data = completed_trials[important_params].values
            param_data_norm = (param_data - param_data.min(axis=0)) / (param_data.max(axis=0) - param_data.min(axis=0) + 1e-8)
            
            for i in range(len(param_data_norm)):
                color = plt.cm.viridis(completed_trials.iloc[i]['value'] / completed_trials['value'].max())
                ax.plot(range(len(important_params)), param_data_norm[i], alpha=0.3, color=color)
                
            ax.set_xticks(range(len(important_params)))
            ax.set_xticklabels(important_params, rotation=45)
            ax.set_ylabel('Normalized Value')
            ax.set_title('Parameter Relationships')
            
        # 3. –í–∞–∂–Ω–æ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        ax = axes[1, 0]
        importances = optuna.importance.get_param_importances(self.study)
        
        if importances:
            params = list(importances.keys())
            values = list(importances.values())
            
            y_pos = np.arange(len(params))
            ax.barh(y_pos, values)
            ax.set_yticks(y_pos)
            ax.set_yticklabels(params)
            ax.set_xlabel('Importance')
            ax.set_title('Parameter Importance')
            
        # 4. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        ax = axes[1, 1]
        if len(completed_trials) > 10:
            # –ë–µ—Ä–µ–º —Ç–æ–ø 10% –ª—É—á—à–∏—Ö
            n_best = max(1, len(completed_trials) // 10)
            best_trials = completed_trials.nsmallest(n_best, 'value') if self.study.direction.name == 'MINIMIZE' else completed_trials.nlargest(n_best, 'value')
            
            text = "Best Parameters Distribution:\n\n"
            for param in ['max_depth', 'learning_rate', 'subsample']:
                if param in best_trials.columns:
                    mean_val = best_trials[param].mean()
                    std_val = best_trials[param].std()
                    text += f"{param}: {mean_val:.3f} ¬± {std_val:.3f}\n"
                    
            ax.text(0.1, 0.5, text, transform=ax.transAxes, fontsize=12, verticalalignment='center')
            ax.axis('off')
            ax.set_title('Best Trials Statistics')
            
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"üìä –ì—Ä–∞—Ñ–∏–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")
            
        plt.close()
        
    def suggest_next_params(self) -> Dict:
        """–ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ä—É—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"""
        if self.best_params is None:
            return {}
            
        # –°–æ–∑–¥–∞–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        variations = []
        
        for param, value in self.best_params.items():
            if isinstance(value, int):
                variations.append({
                    param: value + np.random.choice([-1, 0, 1])
                })
            elif isinstance(value, float):
                variations.append({
                    param: value * np.random.uniform(0.8, 1.2)
                })
                
        return variations