"""
–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è XGBoost v3.0
"""

import pandas as pd
import numpy as np
import logging
from typing import Tuple, List, Dict, Optional
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import train_test_split

from config import Config, FEATURE_GROUPS, EXCLUDE_COLUMNS, TECHNICAL_INDICATORS_BOUNDS
from config.constants import FILLNA_STRATEGIES, VALIDATION_PARAMS, DATA_LEAKAGE_PARAMS

logger = logging.getLogger(__name__)


class DataPreprocessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, config: Config):
        self.config = config
        self.scaler = None
        self.feature_names = None
        self.binary_features = FEATURE_GROUPS['binary_features']
        
        # –ì—Ä—É–ø–ø—ã –º–æ–Ω–µ—Ç –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.coin_groups = {
            'majors': ['BTC', 'ETH'],
            'top_alts': ['BNB', 'SOL', 'XRP', 'ADA', 'AVAX', 'DOT', 'MATIC', 'LINK'],
            'memecoins': ['DOGE', 'SHIB', 'PEPE', 'BONK', 'WIF', 'FLOKI'],
            'defi': ['UNI', 'AAVE', 'MKR', 'COMP', 'CRV', 'SUSHI', 'LDO']
        }
        
    def preprocess(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series, pd.Series]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        
        Returns:
            X: –ø—Ä–∏–∑–Ω–∞–∫–∏
            y_buy: —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è buy
            y_sell: —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è sell
        """
        logger.info("üîß –ù–∞—á–∞–ª–æ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö...")
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏
        df = self._optimize_memory(df)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        df = self._handle_missing_values(df)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤
        df = self._handle_outliers(df)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        X, y_buy, y_sell = self._prepare_features_and_targets(df)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        self._validate_preprocessed_data(X, y_buy, y_sell)
        
        logger.info(f"‚úÖ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –†–∞–∑–º–µ—Ä: {X.shape}")
        
        return X, y_buy, y_sell
        
    def split_data(self, X: pd.DataFrame, y_buy: pd.Series, y_sell: pd.Series) -> Dict:
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ train/test —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –í–†–ï–ú–ï–ù–ù–û–ì–û —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è"""
        logger.info(f"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (test_size={self.config.training.validation_split})...")
        logger.info("‚è∞ –ò–°–ü–û–õ–¨–ó–£–ï–ú –í–†–ï–ú–ï–ù–ù–û–ï –†–ê–ó–î–ï–õ–ï–ù–ò–ï –¥–ª—è –±–æ—Ä—å–±—ã —Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º!")
        
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∏–Ω–¥–µ–∫—Å—ã —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω—ã
        X = X.reset_index(drop=True)
        y_buy = y_buy.reset_index(drop=True)
        y_sell = y_sell.reset_index(drop=True)
        
        # –í–†–ï–ú–ï–ù–ù–û–ï –†–ê–ó–î–ï–õ–ï–ù–ò–ï - –ø–æ—Å–ª–µ–¥–Ω–∏–µ 20% –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∞
        n_samples = len(X)
        test_size = self.config.training.validation_split
        train_size = int(n_samples * (1 - test_size))
        
        logger.info(f"   –í—Å–µ–≥–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {n_samples:,}")
        logger.info(f"   Train: –ø–µ—Ä–≤—ã–µ {train_size:,} –æ–±—Ä–∞–∑—Ü–æ–≤ ({(1-test_size)*100:.0f}%)")
        logger.info(f"   Test: –ø–æ—Å–ª–µ–¥–Ω–∏–µ {n_samples - train_size:,} –æ–±—Ä–∞–∑—Ü–æ–≤ ({test_size*100:.0f}%)")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        X_train = X.iloc[:train_size].copy()
        X_test = X.iloc[train_size:].copy()
        
        y_train_buy = y_buy.iloc[:train_size].copy()
        y_test_buy = y_buy.iloc[train_size:].copy()
        
        y_train_sell = y_sell.iloc[:train_size].copy()
        y_test_sell = y_sell.iloc[train_size:].copy()
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        logger.info("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –†–ê–ó–î–ï–õ–ï–ù–ò–Ø:")
        
        if self.config.training.task_type != "regression":
            # –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
            y_buy_binary_train = (y_train_buy > self.config.training.classification_threshold).astype(int)
            y_buy_binary_test = (y_test_buy > self.config.training.classification_threshold).astype(int)
            
            logger.info(f"   Buy Train: {y_buy_binary_train.sum():,} –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö ({y_buy_binary_train.mean()*100:.1f}%)")
            logger.info(f"   Buy Test: {y_buy_binary_test.sum():,} –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö ({y_buy_binary_test.mean()*100:.1f}%)")
        else:
            # –î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
            logger.info(f"   Buy Train: mean={y_train_buy.mean():.2f}, std={y_train_buy.std():.2f}")
            logger.info(f"   Buy Test: mean={y_test_buy.mean():.2f}, std={y_test_buy.std():.2f}")
            
        return {
            'buy': {
                'X_train': X_train,
                'X_test': X_test,
                'y_train': y_train_buy,
                'y_test': y_test_buy
            },
            'sell': {
                'X_train': X_train,
                'X_test': X_test,
                'y_train': y_train_sell,
                'y_test': y_test_sell
            }
        }
        
    def normalize_features(self, X_train: pd.DataFrame, X_test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        logger.info("üîÑ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        binary_cols = [col for col in self.binary_features if col in X_train.columns]
        continuous_cols = [col for col in X_train.columns if col not in binary_cols]
        
        logger.info(f"   üìä –ë–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(binary_cols)}")
        logger.info(f"   üìä –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(continuous_cols)}")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if continuous_cols:
            self.scaler = RobustScaler()
            X_train_scaled = X_train.copy()
            X_test_scaled = X_test.copy()
            
            X_train_scaled[continuous_cols] = self.scaler.fit_transform(X_train[continuous_cols])
            X_test_scaled[continuous_cols] = self.scaler.transform(X_test[continuous_cols])
            
            return X_train_scaled, X_test_scaled
        else:
            return X_train, X_test
            
    def _optimize_memory(self, df: pd.DataFrame) -> pd.DataFrame:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
        logger.info("üíæ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏...")
        
        start_mem = df.memory_usage().sum() / 1024**2
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö —Ç–∏–ø–æ–≤
        for col in df.select_dtypes(include=['float']).columns:
            df[col] = pd.to_numeric(df[col], downcast='float')
            
        for col in df.select_dtypes(include=['int']).columns:
            df[col] = pd.to_numeric(df[col], downcast='integer')
            
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        if 'symbol' in df.columns:
            df['symbol'] = df['symbol'].astype('category')
            
        end_mem = df.memory_usage().sum() / 1024**2
        
        logger.info(f"   –ü–∞–º—è—Ç—å —É–º–µ–Ω—å—à–µ–Ω–∞ —Å {start_mem:.1f} MB –¥–æ {end_mem:.1f} MB ({(start_mem-end_mem)/start_mem*100:.1f}% —ç–∫–æ–Ω–æ–º–∏–∏)")
        
        return df
        
    def _handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
        missing_counts = df.isnull().sum()
        
        if missing_counts.sum() > 0:
            logger.info(f"üîß –û–±—Ä–∞–±–æ—Ç–∫–∞ {missing_counts.sum()} –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –∏–∑ –∫–æ–Ω—Å—Ç–∞–Ω—Ç
            fill_strategies = FILLNA_STRATEGIES
            
            # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –≤ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞—Ö
            for col in df.select_dtypes(include=[np.number]).columns:
                if df[col].isnull().any():
                    if col in fill_strategies:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∏–∑ –∫–æ–Ω—Å—Ç–∞–Ω—Ç
                        df[col] = df[col].fillna(fill_strategies[col])
                    else:
                        # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö - –º–µ–¥–∏–∞–Ω–∞ –∏–ª–∏ 0
                        median_val = df[col].median()
                        if pd.notna(median_val):
                            df[col] = df[col].fillna(median_val)
                        else:
                            df[col] = df[col].fillna(0)
                            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å–ª–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è
            remaining_nans = df.isnull().sum().sum()
            if remaining_nans > 0:
                logger.warning(f"‚ö†Ô∏è –û—Å—Ç–∞–ª–∏—Å—å NaN –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {remaining_nans}")
                # –§–∏–Ω–∞–ª—å–Ω–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ - —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤
                for col in df.columns:
                    if df[col].isnull().any():
                        if df[col].dtype == 'category':
                            # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö - –∑–∞–ø–æ–ª–Ω—è–µ–º –º–æ–¥–æ–π –∏–ª–∏ 'unknown'
                            mode = df[col].mode()
                            if len(mode) > 0:
                                df[col] = df[col].fillna(mode[0])
                            else:
                                df[col] = df[col].cat.add_categories(['unknown']).fillna('unknown')
                        elif df[col].dtype in ['object']:
                            # –î–ª—è —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö - –∑–∞–ø–æ–ª–Ω—è–µ–º –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π
                            df[col] = df[col].fillna('')
                        else:
                            # –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö - –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
                            df[col] = df[col].fillna(0)
                        
        return df
        
    def _handle_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤"""
        logger.info("üìä –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤...")
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –∏—Ö –¥–æ–ø—É—Å—Ç–∏–º—ã–º–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º–∏
        for indicator, (min_val, max_val) in TECHNICAL_INDICATORS_BOUNDS.items():
            if indicator in df.columns:
                original_outliers = ((df[indicator] < min_val) | (df[indicator] > max_val)).sum()
                if original_outliers > 0:
                    df[indicator] = df[indicator].clip(min_val, max_val)
                    logger.info(f"   üìä {indicator}: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ {original_outliers} –∑–Ω–∞—á–µ–Ω–∏–π –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [{min_val}, {max_val}]")
                    
        return df
        
    def _prepare_features_and_targets(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series, pd.Series]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
        logger.info("üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö...")
        
        # –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        if 'buy_expected_return' not in df.columns or 'sell_expected_return' not in df.columns:
            raise ValueError("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ")
            
        y_buy = df['buy_expected_return'].copy()
        y_sell = df['sell_expected_return'].copy()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        logger.info("üìä –¶–ï–õ–ï–í–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï:")
        logger.info(f"   buy_expected_return: min={y_buy.min():.2f}, max={y_buy.max():.2f}, mean={y_buy.mean():.2f}, std={y_buy.std():.2f}")
        logger.info(f"   sell_expected_return: min={y_sell.min():.2f}, max={y_sell.max():.2f}, mean={y_sell.mean():.2f}, std={y_sell.std():.2f}")
        
        # –ü—Ä–∏–∑–Ω–∞–∫–∏
        feature_columns = [col for col in df.columns if col not in EXCLUDE_COLUMNS]
        X = df[feature_columns].copy()
        
        # –£–¥–∞–ª—è–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        constant_features = []
        for col in X.columns:
            if X[col].nunique() <= 1:
                constant_features.append(col)
                
        if constant_features:
            logger.warning(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(constant_features)} –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —É–¥–∞–ª—è–µ–º: {constant_features[:5]}...")
            X = X.drop(columns=constant_features)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–º–µ–Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.feature_names = list(X.columns)
        
        logger.info(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(X.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        return X, y_buy, y_sell
        
    def _validate_preprocessed_data(self, X: pd.DataFrame, y_buy: pd.Series, y_sell: pd.Series):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("üîç –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π
        assert len(X) == len(y_buy) == len(y_sell), "–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN —Å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π
        nan_cols = X.columns[X.isnull().any()].tolist()
        if nan_cols:
            logger.error(f"‚ùå –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö: {nan_cols}")
            for col in nan_cols[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
                nan_count = X[col].isnull().sum()
                logger.error(f"   {col}: {nan_count} NaN –∑–Ω–∞—á–µ–Ω–∏–π")
        
        assert not X.isnull().any().any(), "–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö"
        assert not y_buy.isnull().any(), "–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –≤ y_buy"
        assert not y_sell.isnull().any(), "–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –≤ y_sell"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç–∏
        assert not np.isinf(X.select_dtypes(include=[np.number])).any().any(), "–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç–∏ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É—Ç–µ—á–∫—É –¥–∞–Ω–Ω—ã—Ö
        self._check_data_leakage(X, y_buy, y_sell)
        
        logger.info("‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–æ–π–¥–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        
    def _check_data_leakage(self, X: pd.DataFrame, y_buy: pd.Series, y_sell: pd.Series):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É—Ç–µ—á–∫—É –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("üîç –ü–†–û–í–ï–†–ö–ê –ù–ê –£–¢–ï–ß–ö–£ –î–ê–ù–ù–´–•:")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —Ü–µ–ª–µ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
        high_corr_features = []
        max_correlation = VALIDATION_PARAMS['max_feature_target_correlation']
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        numeric_columns = X.select_dtypes(include=[np.number]).columns
        
        if DATA_LEAKAGE_PARAMS['check_all_features']:
            columns_to_check = numeric_columns
        else:
            n_features = min(DATA_LEAKAGE_PARAMS['check_n_features'], len(numeric_columns))
            if DATA_LEAKAGE_PARAMS['random_sample'] and len(numeric_columns) > n_features:
                # –°–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞
                np.random.seed(42)
                columns_to_check = np.random.choice(numeric_columns, n_features, replace=False)
            else:
                # –ü–µ—Ä–≤—ã–µ n –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                columns_to_check = numeric_columns[:n_features]
        
        logger.info(f"   üìã –ü—Ä–æ–≤–µ—Ä—è–µ–º {len(columns_to_check)} –∏–∑ {len(numeric_columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        for col in columns_to_check:
            corr_buy = X[col].corr(y_buy)
            corr_sell = X[col].corr(y_sell)
            
            if abs(corr_buy) > max_correlation or abs(corr_sell) > max_correlation:
                high_corr_features.append(col)
                logger.warning(f"   ‚ö†Ô∏è –í—ã—Å–æ–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –¥–ª—è {col}: buy={corr_buy:.3f}, sell={corr_sell:.3f}")
            else:
                logger.info(f"   ‚úÖ –ü—Ä–∏–∑–Ω–∞–∫ {col}: corr_buy={corr_buy:.3f}, corr_sell={corr_sell:.3f}")
                
        if high_corr_features:
            logger.warning(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –≤—ã—Å–æ–∫–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π: {high_corr_features}")
            
    def transform_to_classification_labels(self, y_buy: pd.Series, y_sell: pd.Series) -> Tuple[pd.Series, pd.Series]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –º–µ—Ç–∫–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        if self.config.training.task_type == "classification_binary":
            threshold = self.config.training.classification_threshold
            logger.info(f"üîÑ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–ø–æ—Ä–æ–≥ > {threshold}%)...")
            
            y_buy_binary = (y_buy > threshold).astype(int)
            y_sell_binary = (y_sell > threshold).astype(int)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∏–Ω–∞—Ä–Ω—ã—Ö –º–µ—Ç–æ–∫ (–ø–æ—Ä–æ–≥ > {threshold}%):")
            logger.info(f"   Buy - –ö–ª–∞—Å—Å 1 (–≤—Ö–æ–¥–∏—Ç—å): {y_buy_binary.mean()*100:.1f}%")
            logger.info(f"   Sell - –ö–ª–∞—Å—Å 1 (–≤—Ö–æ–¥–∏—Ç—å): {y_sell_binary.mean()*100:.1f}%")
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            logger.info(f"\nüìà –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ expected_return:")
            logger.info(f"   Buy > 0%: {(y_buy > 0).mean()*100:.1f}%, Buy > 0.5%: {(y_buy > 0.5).mean()*100:.1f}%, Buy > 1%: {(y_buy > 1).mean()*100:.1f}%")
            logger.info(f"   Sell > 0%: {(y_sell > 0).mean()*100:.1f}%, Sell > 0.5%: {(y_sell > 0.5).mean()*100:.1f}%, Sell > 1%: {(y_sell > 1).mean()*100:.1f}%")
            
            return y_buy_binary, y_sell_binary
            
        elif self.config.training.task_type == "classification_multi":
            # –ú—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
            thresholds = self.config.training.multiclass_thresholds
            logger.info(f"üîÑ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤—ã–µ –º–µ—Ç–∫–∏ (–ø–æ—Ä–æ–≥–∏: {thresholds})...")
            
            y_buy_multi = pd.cut(y_buy, bins=[-np.inf] + thresholds + [np.inf], labels=False)
            y_sell_multi = pd.cut(y_sell, bins=[-np.inf] + thresholds + [np.inf], labels=False)
            
            return y_buy_multi, y_sell_multi
            
        else:
            # –†–µ–≥—Ä–µ—Å—Å–∏—è - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
            return y_buy, y_sell
    
    def group_symbols(self, df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        """
        –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ —Ç–∏–ø–∞–º –º–æ–Ω–µ—Ç –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å DataFrame –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
        """
        grouped_data = {}
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        all_symbols = df['symbol'].unique() if 'symbol' in df.columns else []
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –≥—Ä—É–ø–ø—É 'other' –æ—Å—Ç–∞–ª—å–Ω—ã–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏
        used_symbols = set()
        for group_symbols in self.coin_groups.values():
            used_symbols.update(group_symbols)
        
        self.coin_groups['other'] = [s for s in all_symbols if s not in used_symbols]
        
        # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
        for group_name, symbols in self.coin_groups.items():
            if symbols:  # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–∏–º–≤–æ–ª—ã –≤ –≥—Ä—É–ø–ø–µ
                group_df = df[df['symbol'].isin(symbols)] if 'symbol' in df.columns else pd.DataFrame()
                if not group_df.empty:
                    grouped_data[group_name] = group_df
                    logger.info(f"   –ì—Ä—É–ø–ø–∞ '{group_name}': {len(symbols)} –º–æ–Ω–µ—Ç, {len(group_df)} –∑–∞–ø–∏—Å–µ–π")
        
        return grouped_data
    
    def get_group_weights(self, group_name: str) -> Dict[str, float]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Å–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≥—Ä—É–ø–ø—ã –º–æ–Ω–µ—Ç
        """
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –≥—Ä—É–ø–ø
        group_weights = {
            'majors': {
                'technical': 0.70,    # –ë–æ–ª—å—à–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö
                'temporal': 0.15,
                'btc_related': 0.05,  # –ú–µ–Ω—å—à–µ BTC –¥–ª—è —Å–∞–º–æ–≥–æ BTC
                'other': 0.10
            },
            'top_alts': {
                'technical': 0.60,    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
                'temporal': 0.20,
                'btc_related': 0.10,
                'other': 0.10
            },
            'memecoins': {
                'technical': 0.50,    # –ú–µ–Ω—å—à–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö
                'temporal': 0.20,
                'btc_related': 0.20,  # –ë–æ–ª—å—à–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å BTC
                'other': 0.10
            },
            'defi': {
                'technical': 0.65,    # –ë–æ–ª—å—à–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö
                'temporal': 0.15,
                'btc_related': 0.10,
                'other': 0.10
            },
            'other': {
                'technical': 0.60,    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
                'temporal': 0.20,
                'btc_related': 0.10,
                'other': 0.10
            }
        }
        
        return group_weights.get(group_name, group_weights['other'])