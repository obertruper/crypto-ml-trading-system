#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π XGBoost v3
"""

import os
import sys
import json
import pickle
import pandas as pd
import numpy as np
import logging
from pathlib import Path
import glob

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from utils.feature_importance_validator import FeatureImportanceValidator
from config.feature_mapping import get_feature_category, get_temporal_blacklist

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def find_latest_models(base_dir="logs"):
    """–ù–∞–π—Ç–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏"""
    model_files = glob.glob(f"{base_dir}/**/classification_binary_model_*.pkl", recursive=True)
    
    if not model_files:
        logger.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –ø–∞–ø–∫–µ logs/")
        return None, None
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è–º
    model_dirs = {}
    for model_file in model_files:
        dir_name = os.path.dirname(model_file)
        if dir_name not in model_dirs:
            model_dirs[dir_name] = []
        model_dirs[dir_name].append(model_file)
    
    # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω—é—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
    latest_dir = max(model_dirs.keys(), key=lambda d: os.path.getmtime(d))
    
    logger.info(f"üìÅ –ù–∞–π–¥–µ–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è –º–æ–¥–µ–ª—å: {os.path.basename(os.path.dirname(latest_dir))}")
    
    return latest_dir, model_dirs[latest_dir]


def load_models_and_metadata(model_dir):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"""
    models = {'buy': [], 'sell': []}
    metadata = {'buy': [], 'sell': []}
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –¥–ª—è buy
    buy_dir = os.path.join(model_dir, "buy_models")
    if os.path.exists(buy_dir):
        for i in range(10):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ 10 –º–æ–¥–µ–ª–µ–π
            model_path = os.path.join(buy_dir, f"classification_binary_model_{i}.pkl")
            meta_path = os.path.join(buy_dir, f"classification_binary_model_{i}_metadata.json")
            
            if os.path.exists(model_path) and os.path.exists(meta_path):
                with open(model_path, 'rb') as f:
                    models['buy'].append(pickle.load(f))
                with open(meta_path, 'r') as f:
                    metadata['buy'].append(json.load(f))
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –¥–ª—è sell
    sell_dir = os.path.join(model_dir, "sell_models")
    if os.path.exists(sell_dir):
        for i in range(10):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ 10 –º–æ–¥–µ–ª–µ–π
            model_path = os.path.join(sell_dir, f"classification_binary_model_{i}.pkl")
            meta_path = os.path.join(sell_dir, f"classification_binary_model_{i}_metadata.json")
            
            if os.path.exists(model_path) and os.path.exists(meta_path):
                with open(model_path, 'rb') as f:
                    models['sell'].append(pickle.load(f))
                with open(meta_path, 'r') as f:
                    metadata['sell'].append(json.load(f))
    
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: Buy={len(models['buy'])}, Sell={len(models['sell'])}")
    
    return models, metadata


def analyze_feature_importance(models, metadata):
    """–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    print("\n" + "="*80)
    print("üìä –ê–ù–ê–õ–ò–ó –í–ê–ñ–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í")
    print("="*80)
    
    temporal_blacklist = get_temporal_blacklist()
    
    for direction in ['buy', 'sell']:
        if not models[direction]:
            continue
            
        print(f"\nüéØ –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: {direction.upper()}")
        print("-"*40)
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–æ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        all_importances = {}
        feature_names = metadata[direction][0]['feature_names'] if metadata[direction] else []
        
        for i, (model, meta) in enumerate(zip(models[direction], metadata[direction])):
            if hasattr(model, 'feature_importances_'):
                for feat, imp in zip(feature_names, model.feature_importances_):
                    if feat not in all_importances:
                        all_importances[feat] = []
                    all_importances[feat].append(imp)
        
        # –£—Å—Ä–µ–¥–Ω—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç–∏
        avg_importances = [(feat, np.mean(imps)) for feat, imps in all_importances.items()]
        avg_importances.sort(key=lambda x: x[1], reverse=True)
        
        # –¢–æ–ø-20 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        print("\nüìà –¢–æ–ø-20 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
        for i, (feat, imp) in enumerate(avg_importances[:20]):
            category = get_feature_category(feat)
            emoji = "üî¥" if category == "temporal" else "üü¢" if category == "technical" else "üîµ"
            warning = " ‚ö†Ô∏è BLACKLISTED!" if feat in temporal_blacklist else ""
            print(f"{i+1:2d}. {emoji} {feat:40s} {imp:.4f} ({category}){warning}")
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        category_stats = {}
        total_importance = sum(imp for _, imp in avg_importances)
        
        for feat, imp in avg_importances:
            cat = get_feature_category(feat)
            if cat not in category_stats:
                category_stats[cat] = {'count': 0, 'importance': 0}
            category_stats[cat]['count'] += 1
            category_stats[cat]['importance'] += imp
        
        print(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
        for cat, stats in sorted(category_stats.items()):
            percentage = (stats['importance'] / total_importance * 100) if total_importance > 0 else 0
            status = "‚úÖ" if cat != "temporal" or percentage <= 3 else "‚ùå –ü–†–û–ë–õ–ï–ú–ê!"
            print(f"   {cat:15s}: {stats['count']:3d} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, {percentage:5.1f}% –≤–∞–∂–Ω–æ—Å—Ç–∏ {status}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ temporal –≤ —Ç–æ–ø–µ
        top_10_temporal = [f for f, _ in avg_importances[:10] if get_feature_category(f) == "temporal"]
        if top_10_temporal:
            print(f"\n‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: Temporal –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —Ç–æ–ø-10: {', '.join(top_10_temporal)}")
            
        # Blacklisted –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —Ç–æ–ø–µ
        blacklisted_in_top = [f for f, _ in avg_importances[:20] if f in temporal_blacklist]
        if blacklisted_in_top:
            print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ù–û: Blacklisted –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —Ç–æ–ø-20: {', '.join(blacklisted_in_top)}")


def run_validation(models, metadata):
    """–ó–∞–ø—É—Å–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ FeatureImportanceValidator"""
    print("\n" + "="*80)
    print("üîç –í–ê–õ–ò–î–ê–¶–ò–Ø –í–ê–ñ–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í")
    print("="*80)
    
    validator = FeatureImportanceValidator(max_temporal_importance=3.0)
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–∞
    models_dict = {}
    for direction in ['buy', 'sell']:
        if models[direction]:
            # –°–æ–∑–¥–∞–µ–º mock –∞–Ω—Å–∞–º–±–ª—å
            class MockEnsemble:
                def __init__(self, models_list):
                    self.models = models_list
            
            models_dict[direction] = {
                'ensemble': MockEnsemble(models[direction])
            }
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–º–µ–Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_names = []
    if metadata['buy']:
        feature_names = metadata['buy'][0]['feature_names']
    elif metadata['sell']:
        feature_names = metadata['sell'][0]['feature_names']
    
    if models_dict and feature_names:
        validation_results = validator.validate_ensemble_importance(models_dict, feature_names)
        
        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = validator.get_recommendations()
        if recommendations:
            print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
    else:
        print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏")


def analyze_training_config(model_dir):
    """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è"""
    config_path = os.path.join(model_dir, "config.yaml")
    
    if os.path.exists(config_path):
        print("\n" + "="*80)
        print("‚öôÔ∏è –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –û–ë–£–ß–ï–ù–ò–Ø")
        print("="*80)
        
        import yaml
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        # –í—ã–≤–æ–¥–∏–º –∫–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        training = config.get('training', {})
        model = config.get('model', {})
        
        print(f"üìã –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
        print(f"   –ó–∞–¥–∞—á–∞: {training.get('task_type', '–Ω/–¥')}")
        print(f"   –ü–æ—Ä–æ–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {training.get('classification_threshold', '–Ω/–¥')}%")
        print(f"   –ú–µ—Ç–æ–¥ –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {training.get('feature_selection_method', '–Ω/–¥')}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {training.get('feature_selection_top_k', '–Ω/–¥')}")
        print(f"   –†–∞–∑–º–µ—Ä –∞–Ω—Å–∞–º–±–ª—è: {training.get('ensemble_size', '–Ω/–¥')}")
        print(f"   –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞: {training.get('balance_method', '–Ω/–¥')}")
        
        print(f"\nü§ñ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏:")
        print(f"   max_depth: {model.get('max_depth', '–Ω/–¥')}")
        print(f"   learning_rate: {model.get('learning_rate', '–Ω/–¥')}")
        print(f"   n_estimators: {model.get('n_estimators', '–Ω/–¥')}")
        print(f"   GPU: {'–î–∞' if model.get('tree_method') == 'gpu_hist' else '–ù–µ—Ç'}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("""
üîç –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ XGBoost v3.0
=========================================
""")
    
    # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model_dir, model_files = find_latest_models()
    
    if not model_dir:
        return
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    models, metadata = load_models_and_metadata(model_dir)
    
    if not any(models.values()):
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏")
        return
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    analyze_training_config(model_dir)
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    analyze_feature_importance(models, metadata)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏—é
    run_validation(models, metadata)
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞
    print("\n" + "="*80)
    print("üìù –ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê")
    print("="*80)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ final_report.txt
    report_path = os.path.join(model_dir, "final_report.txt")
    if os.path.exists(report_path):
        print("\nüìÑ –ú–µ—Ç—Ä–∏–∫–∏ –∏–∑ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞:")
        with open(report_path, 'r') as f:
            lines = f.readlines()
            for line in lines:
                if any(metric in line for metric in ['ROC-AUC:', 'Accuracy:', 'Precision:', 'Recall:']):
                    print(f"   {line.strip()}")
    
    print("\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print("\nüí° –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
    print("   1. –ï—Å–ª–∏ temporal > 3% - –∑–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ —Å –Ω–æ–≤—ã–º–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏")
    print("   2. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ python run_xgboost_v3.py –∏ –≤—ã–±–µ—Ä–∏—Ç–µ –≤–∞—Ä–∏–∞–Ω—Ç 1 –∏–ª–∏ 2")
    print("   3. –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è —Å–Ω–æ–≤–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏")


if __name__ == "__main__":
    main()