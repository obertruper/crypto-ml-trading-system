#!/usr/bin/env python3
"""
–û–±—É—á–µ–Ω–∏–µ XGBoost –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç–∏ —Å–¥–µ–ª–æ–∫
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–±–∏–Ω–∞—Ä–Ω–æ–π/–º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤–æ–π)
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ PostgreSQL
- –°–æ–≤–º–µ—Å—Ç–∏–º —Å —Ç–µ–∫—É—â–µ–π —Å–∏—Å—Ç–µ–º–æ–π
"""

import os
import sys
import psycopg2
import pandas as pd
import numpy as np
import logging
import joblib
import json
import yaml
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import xgboost as xgb
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import (mean_absolute_error, mean_squared_error, r2_score,
                           accuracy_score, precision_score, recall_score, f1_score,
                           confusion_matrix, classification_report, roc_auc_score,
                           roc_curve)
import argparse
import warnings
warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
log_dir = f'logs/xgboost_training_{current_time}'
os.makedirs(log_dir, exist_ok=True)
os.makedirs(f'{log_dir}/plots', exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'{log_dir}/training.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
MODEL_DIR = 'trained_model/xgboost'
os.makedirs(MODEL_DIR, exist_ok=True)


class XGBoostTrainer:
    def __init__(self, config_path='config.yaml', mode='binary'):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞ XGBoost –º–æ–¥–µ–ª–µ–π
        
        Args:
            config_path: –ø—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            mode: —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è ('regression', 'binary', 'multiclass')
        """
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.mode = mode
        logger.info(f"üéØ –†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è: {mode}")
        
        self.db_config = self.config['database'].copy()
        if not self.db_config.get('password'):
            self.db_config.pop('password', None)
        
        # –°–ø–∏—Å–æ–∫ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ (–∫–∞–∫ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ)
        self.TECHNICAL_INDICATORS = [
            # –¢—Ä–µ–Ω–¥–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            'ema_15', 'adx_val', 'adx_plus_di', 'adx_minus_di',
            'macd_val', 'macd_signal', 'macd_hist', 'sar',
            'ichimoku_conv', 'ichimoku_base', 'aroon_up', 'aroon_down',
            'dpo',
            
            # –û—Å—Ü–∏–ª–ª—è—Ç–æ—Ä—ã
            'rsi_val', 'stoch_k', 'stoch_d', 'cci_val', 'williams_r',
            'roc', 'ult_osc', 'mfi',
            
            # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
            'atr_val', 'bb_upper', 'bb_lower', 'bb_basis',
            'donchian_upper', 'donchian_lower',
            
            # –û–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            'obv', 'cmf', 'volume_sma', 'volume_ratio',
            
            # Vortex –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            'vortex_vip', 'vortex_vin',
            
            # –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            'macd_signal_ratio', 'adx_diff', 'bb_position',
            'rsi_dist_from_mid', 'stoch_diff', 'vortex_ratio',
            'ichimoku_diff', 'atr_norm',
            
            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            'hour', 'day_of_week', 'is_weekend',
            
            # –¶–µ–Ω–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            'price_change_1', 'price_change_4', 'price_change_16',
            'volatility_4', 'volatility_16'
        ]
        
        self.scaler = RobustScaler()
        self.models = {}
        self.feature_importance = {}
        
    def prepare_binary_labels(self, returns, threshold=0.3):
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        return (returns > threshold).astype(int)
    
    def prepare_multiclass_labels(self, returns):
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤—ã–µ –º–µ—Ç–∫–∏"""
        labels = np.zeros(len(returns))
        labels[returns < -0.5] = 0  # –£–±—ã—Ç–æ—á–Ω—ã–µ
        labels[(returns >= -0.5) & (returns < 0.5)] = 1  # –û–∫–æ–ª–æ –Ω—É–ª—è
        labels[(returns >= 0.5) & (returns < 1.5)] = 2  # –ú–∞–ª–æ–ø—Ä–∏–±—ã–ª—å–Ω—ã–µ
        labels[(returns >= 1.5) & (returns < 3)] = 3  # –ü—Ä–∏–±—ã–ª—å–Ω—ã–µ
        labels[returns >= 3] = 4  # –í—ã—Å–æ–∫–æ–ø—Ä–∏–±—ã–ª—å–Ω—ã–µ
        return labels.astype(int)
        
    def connect_db(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL"""
        try:
            conn = psycopg2.connect(**self.db_config)
            logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
            return conn
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ PostgreSQL: {e}")
            raise
    
    def load_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL"""
        logger.info("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL...")
        
        conn = self.connect_db()
        
        query = """
        SELECT 
            p.symbol, p.timestamp, p.datetime,
            p.technical_indicators,
            p.buy_expected_return,
            p.sell_expected_return,
            p.open, p.high, p.low, p.close, p.volume
        FROM processed_market_data p
        JOIN raw_market_data r ON p.raw_data_id = r.id
        WHERE p.technical_indicators IS NOT NULL
          AND r.market_type = 'futures'
          AND p.buy_expected_return IS NOT NULL
          AND p.sell_expected_return IS NOT NULL
        ORDER BY p.symbol, p.timestamp
        """
        
        df = pd.read_sql_query(query, conn)
        conn.close()
        
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        symbol_counts = df['symbol'].value_counts()
        logger.info("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–∏–º–≤–æ–ª–∞–º:")
        for symbol, count in symbol_counts.head(10).items():
            logger.info(f"   {symbol}: {count:,} –∑–∞–ø–∏—Å–µ–π")
        
        return df
    
    def prepare_features(self, df):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ technical_indicators"""
        logger.info("üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ JSON
        features = []
        for _, row in df.iterrows():
            feature_values = []
            indicators = row['technical_indicators']
            
            for indicator in self.TECHNICAL_INDICATORS:
                value = indicators.get(indicator, 0.0)
                if value is None or pd.isna(value):
                    value = 0.0
                feature_values.append(float(value))
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            rsi = indicators.get('rsi_val', 50.0)
            feature_values.append(1.0 if rsi is not None and rsi < 30 else 0.0)  # RSI oversold
            feature_values.append(1.0 if rsi is not None and rsi > 70 else 0.0)  # RSI overbought
            
            macd = indicators.get('macd_val', 0.0)
            macd_signal = indicators.get('macd_signal', 0.0)
            feature_values.append(1.0 if macd is not None and macd_signal is not None and macd > macd_signal else 0.0)
            
            bb_position = indicators.get('bb_position', 0.5)
            feature_values.append(1.0 if bb_position is not None and bb_position < 0.2 else 0.0)
            feature_values.append(1.0 if bb_position is not None and bb_position > 0.8 else 0.0)
            
            adx = indicators.get('adx_val', 0.0)
            feature_values.append(1.0 if adx is not None and adx > 25 else 0.0)
            
            volume_ratio = indicators.get('volume_ratio', 1.0)
            feature_values.append(1.0 if volume_ratio is not None and volume_ratio > 2.0 else 0.0)
            
            features.append(feature_values)
        
        # –°–æ–∑–¥–∞–µ–º DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        feature_names = self.TECHNICAL_INDICATORS + [
            'rsi_oversold', 'rsi_overbought', 'macd_bullish',
            'bb_near_lower', 'bb_near_upper', 'strong_trend', 'high_volume'
        ]
        
        X = pd.DataFrame(features, columns=feature_names)
        
        # –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        y_buy = df['buy_expected_return'].values
        y_sell = df['sell_expected_return'].values
        
        logger.info(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(X)} –ø—Ä–∏–º–µ—Ä–æ–≤ —Å {len(feature_names)} –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")
        
        return X, y_buy, y_sell, df['symbol'].values, df['timestamp'].values
    
    def train_model(self, X_train, y_train, X_val, y_val, model_name):
        """–û–±—É—á–µ–Ω–∏–µ XGBoost –º–æ–¥–µ–ª–∏"""
        logger.info(f"\n{'='*60}")
        logger.info(f"üöÄ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_name}")
        logger.info(f"{'='*60}")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã XGBoost
        if self.mode == 'regression':
            params = {
                'objective': 'reg:squarederror',
                'max_depth': 8,
                'learning_rate': 0.05,
                'n_estimators': 1000,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'min_child_weight': 5,
                'gamma': 0.1,
                'reg_alpha': 0.1,
                'reg_lambda': 1.0,
                'random_state': 42,
                'n_jobs': -1,
                'early_stopping_rounds': 50,
                'eval_metric': ['mae', 'rmse']
            }
            model = xgb.XGBRegressor(**params)
        
        elif self.mode == 'binary':
            # –ü–æ–¥—Å—á–µ—Ç –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
            pos_ratio = np.sum(y_train == 0) / np.sum(y_train == 1)
            logger.info(f"üìä –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ - 0: {np.sum(y_train == 0)}, 1: {np.sum(y_train == 1)}")
            logger.info(f"‚öñÔ∏è Scale pos weight: {pos_ratio:.2f}")
            
            params = {
                'objective': 'binary:logistic',
                'max_depth': 8,
                'learning_rate': 0.05,
                'n_estimators': 1000,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'min_child_weight': 5,
                'gamma': 0.1,
                'reg_alpha': 0.1,
                'reg_lambda': 1.0,
                'scale_pos_weight': pos_ratio,
                'random_state': 42,
                'n_jobs': -1,
                'early_stopping_rounds': 50,
                'eval_metric': ['auc', 'error']
            }
            model = xgb.XGBClassifier(**params)
            
        else:  # multiclass
            num_classes = len(np.unique(y_train))
            logger.info(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {num_classes}")
            
            params = {
                'objective': 'multi:softprob',
                'num_class': num_classes,
                'max_depth': 8,
                'learning_rate': 0.05,
                'n_estimators': 1000,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'min_child_weight': 5,
                'gamma': 0.1,
                'reg_alpha': 0.1,
                'reg_lambda': 1.0,
                'random_state': 42,
                'n_jobs': -1,
                'early_stopping_rounds': 50,
                'eval_metric': ['mlogloss', 'merror']
            }
            model = xgb.XGBClassifier(**params)
        
        # –û–±—É—á–µ–Ω–∏–µ —Å early stopping
        eval_set = [(X_train, y_train), (X_val, y_val)]
        model.fit(
            X_train, y_train,
            eval_set=eval_set,
            verbose=100
        )
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        if self.mode == 'regression':
            y_pred = model.predict(X_val)
            
            # –ú–µ—Ç—Ä–∏–∫–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
            mae = mean_absolute_error(y_val, y_pred)
            mse = mean_squared_error(y_val, y_pred)
            rmse = np.sqrt(mse)
            r2 = r2_score(y_val, y_pred)
            direction_accuracy = np.mean((y_pred > 0) == (y_val > 0))
            
            logger.info(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è {model_name}:")
            logger.info(f"   MAE: {mae:.4f}%")
            logger.info(f"   RMSE: {rmse:.4f}%")
            logger.info(f"   R¬≤: {r2:.4f}")
            logger.info(f"   Direction Accuracy: {direction_accuracy:.2%}")
            
            metrics = {
                'mae': mae,
                'rmse': rmse,
                'r2': r2,
                'direction_accuracy': direction_accuracy
            }
            
        elif self.mode == 'binary':
            y_pred = model.predict(X_val)
            y_pred_proba = model.predict_proba(X_val)[:, 1]
            
            # –ú–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            accuracy = accuracy_score(y_val, y_pred)
            precision = precision_score(y_val, y_pred, zero_division=0)
            recall = recall_score(y_val, y_pred, zero_division=0)
            f1 = f1_score(y_val, y_pred, zero_division=0)
            auc = roc_auc_score(y_val, y_pred_proba)
            
            logger.info(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è {model_name}:")
            logger.info(f"   Accuracy: {accuracy:.2%}")
            logger.info(f"   Precision: {precision:.2%}")
            logger.info(f"   Recall: {recall:.2%}")
            logger.info(f"   F1-Score: {f1:.4f}")
            logger.info(f"   ROC-AUC: {auc:.4f}")
            
            # Confusion Matrix
            cm = confusion_matrix(y_val, y_pred)
            logger.info(f"   Confusion Matrix:")
            logger.info(f"   TN: {cm[0,0]:,}  FP: {cm[0,1]:,}")
            logger.info(f"   FN: {cm[1,0]:,}  TP: {cm[1,1]:,}")
            
            metrics = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'auc': auc,
                'confusion_matrix': cm,
                'y_pred_proba': y_pred_proba
            }
            
        else:  # multiclass
            y_pred = model.predict(X_val)
            y_pred_proba = model.predict_proba(X_val)
            
            # –ú–µ—Ç—Ä–∏–∫–∏ –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            accuracy = accuracy_score(y_val, y_pred)
            precision = precision_score(y_val, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y_val, y_pred, average='weighted', zero_division=0)
            f1 = f1_score(y_val, y_pred, average='weighted', zero_division=0)
            
            logger.info(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è {model_name}:")
            logger.info(f"   Accuracy: {accuracy:.2%}")
            logger.info(f"   Weighted Precision: {precision:.2%}")
            logger.info(f"   Weighted Recall: {recall:.2%}")
            logger.info(f"   Weighted F1-Score: {f1:.4f}")
            
            # Per-class metrics
            report = classification_report(y_val, y_pred, 
                                         target_names=['–£–±—ã—Ç–æ—á–Ω—ã–µ', '–û–∫–æ–ª–æ –Ω—É–ª—è', '–ú–∞–ª–æ–ø—Ä–∏–±—ã–ª—å–Ω—ã–µ', 
                                                      '–ü—Ä–∏–±—ã–ª—å–Ω—ã–µ', '–í—ã—Å–æ–∫–æ–ø—Ä–∏–±—ã–ª—å–Ω—ã–µ'])
            logger.info(f"\n   –ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∞–º:\n{report}")
            
            metrics = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'classification_report': report,
                'y_pred_proba': y_pred_proba
            }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.feature_importance[model_name] = model.feature_importances_
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ —Å–ª–æ–≤–∞—Ä—å
        self.models[model_name] = model
        
        return model, metrics
    
    def plot_results(self, y_true, y_pred, model_name, metrics):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        if self.mode == 'regression':
            self._plot_regression_results(y_true, y_pred, model_name, metrics)
        elif self.mode == 'binary':
            self._plot_binary_results(y_true, metrics, model_name)
        else:
            self._plot_multiclass_results(y_true, y_pred, metrics, model_name)
    
    def _plot_regression_results(self, y_true, y_pred, model_name, metrics):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'XGBoost Regression Model: {model_name}', fontsize=16)
        
        # –ì—Ä–∞—Ñ–∏–∫ 1: Scatter plot
        axes[0, 0].scatter(y_true, y_pred, alpha=0.5, s=10)
        axes[0, 0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)
        axes[0, 0].set_xlabel('True Return (%)')
        axes[0, 0].set_ylabel('Predicted Return (%)')
        axes[0, 0].set_title(f'Predictions vs True (R¬≤ = {metrics["r2"]:.3f})')
        axes[0, 0].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 2: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫
        errors = y_pred - y_true
        axes[0, 1].hist(errors, bins=50, edgecolor='black', alpha=0.7)
        axes[0, 1].axvline(x=0, color='r', linestyle='--', lw=2)
        axes[0, 1].set_xlabel('Prediction Error (%)')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].set_title(f'Error Distribution (MAE = {metrics["mae"]:.3f}%)')
        axes[0, 1].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 3: –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Ç–æ–ø-20)
        feature_names = self.TECHNICAL_INDICATORS + [
            'rsi_oversold', 'rsi_overbought', 'macd_bullish',
            'bb_near_lower', 'bb_near_upper', 'strong_trend', 'high_volume'
        ]
        importance = self.feature_importance[model_name]
        indices = np.argsort(importance)[-20:]
        
        axes[1, 0].barh(range(20), importance[indices])
        axes[1, 0].set_yticks(range(20))
        axes[1, 0].set_yticklabels([feature_names[i] for i in indices])
        axes[1, 0].set_xlabel('Feature Importance')
        axes[1, 0].set_title('Top 20 Features')
        axes[1, 0].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 4: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        axes[1, 1].axis('off')
        stats_text = f"""
Performance Summary:
  MAE: {metrics['mae']:.3f}%
  RMSE: {metrics['rmse']:.3f}%
  R¬≤: {metrics['r2']:.3f}
  Direction Accuracy: {metrics['direction_accuracy']:.1%}
  
Model: XGBoost Regression
Trees: {len(self.models[model_name].get_booster().get_dump())}
Max Depth: 8
Learning Rate: 0.05
        """
        
        axes[1, 1].text(0.1, 0.5, stats_text, fontsize=12,
                       verticalalignment='center', family='monospace',
                       bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.5))
        
        plt.tight_layout()
        plt.savefig(f'{log_dir}/plots/{model_name}_evaluation.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def _plot_binary_results(self, y_true, metrics, model_name):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'XGBoost Binary Classification: {model_name}', fontsize=16)
        
        # –ì—Ä–∞—Ñ–∏–∫ 1: ROC –∫—Ä–∏–≤–∞—è
        if 'y_pred_proba' in metrics:
            fpr, tpr, thresholds = roc_curve(y_true, metrics['y_pred_proba'])
            axes[0, 0].plot(fpr, tpr, 'b-', lw=2, label=f'ROC (AUC = {metrics["auc"]:.3f})')
            axes[0, 0].plot([0, 1], [0, 1], 'r--', lw=2, label='Random')
            axes[0, 0].set_xlabel('False Positive Rate')
            axes[0, 0].set_ylabel('True Positive Rate')
            axes[0, 0].set_title('ROC Curve')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 2: Confusion Matrix
        cm = metrics['confusion_matrix']
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1])
        axes[0, 1].set_xlabel('Predicted')
        axes[0, 1].set_ylabel('Actual')
        axes[0, 1].set_title('Confusion Matrix')
        axes[0, 1].set_xticklabels(['–ù–µ –≤—Ö–æ–¥–∏—Ç—å', '–í—Ö–æ–¥–∏—Ç—å'])
        axes[0, 1].set_yticklabels(['–ù–µ –≤—Ö–æ–¥–∏—Ç—å', '–í—Ö–æ–¥–∏—Ç—å'])
        
        # –ì—Ä–∞—Ñ–∏–∫ 3: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        if 'y_pred_proba' in metrics:
            axes[1, 0].hist(metrics['y_pred_proba'][y_true == 0], bins=50, alpha=0.7, 
                           label='–ö–ª–∞—Å—Å 0 (–ù–µ –≤—Ö–æ–¥–∏—Ç—å)', density=True)
            axes[1, 0].hist(metrics['y_pred_proba'][y_true == 1], bins=50, alpha=0.7, 
                           label='–ö–ª–∞—Å—Å 1 (–í—Ö–æ–¥–∏—Ç—å)', density=True)
            axes[1, 0].set_xlabel('Predicted Probability')
            axes[1, 0].set_ylabel('Density')
            axes[1, 0].set_title('Probability Distribution by Class')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 4: –ú–µ—Ç—Ä–∏–∫–∏
        axes[1, 1].axis('off')
        stats_text = f"""
Performance Summary:
  Accuracy: {metrics['accuracy']:.2%}
  Precision: {metrics['precision']:.2%}
  Recall: {metrics['recall']:.2%}
  F1-Score: {metrics['f1']:.3f}
  ROC-AUC: {metrics['auc']:.3f}
  
Confusion Matrix:
  TN: {cm[0,0]:,}  FP: {cm[0,1]:,}
  FN: {cm[1,0]:,}  TP: {cm[1,1]:,}
  
Model: XGBoost Binary
Trees: {len(self.models[model_name].get_booster().get_dump())}
        """
        
        axes[1, 1].text(0.1, 0.5, stats_text, fontsize=12,
                       verticalalignment='center', family='monospace',
                       bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.5))
        
        plt.tight_layout()
        plt.savefig(f'{log_dir}/plots/{model_name}_binary_evaluation.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def _plot_multiclass_results(self, y_true, y_pred, metrics, model_name):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'XGBoost Multiclass Classification: {model_name}', fontsize=16)
        
        # –ì—Ä–∞—Ñ–∏–∫ 1: Confusion Matrix
        cm = confusion_matrix(y_true, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])
        axes[0, 0].set_xlabel('Predicted')
        axes[0, 0].set_ylabel('Actual')
        axes[0, 0].set_title('Confusion Matrix')
        class_names = ['–£–±—ã—Ç–æ—á–Ω—ã–µ', '–û–∫–æ–ª–æ –Ω—É–ª—è', '–ú–∞–ª–æ–ø—Ä–∏–±—ã–ª—å–Ω—ã–µ', '–ü—Ä–∏–±—ã–ª—å–Ω—ã–µ', '–í—ã—Å–æ–∫–æ–ø—Ä–∏–±—ã–ª—å–Ω—ã–µ']
        axes[0, 0].set_xticklabels(class_names, rotation=45)
        axes[0, 0].set_yticklabels(class_names, rotation=0)
        
        # –ì—Ä–∞—Ñ–∏–∫ 2: Per-class accuracy
        per_class_acc = []
        for i in range(len(cm)):
            if cm[i].sum() > 0:
                acc = cm[i, i] / cm[i].sum()
                per_class_acc.append(acc)
            else:
                per_class_acc.append(0)
        
        axes[0, 1].bar(range(len(per_class_acc)), per_class_acc)
        axes[0, 1].set_xticks(range(len(class_names)))
        axes[0, 1].set_xticklabels(class_names, rotation=45)
        axes[0, 1].set_ylabel('Accuracy')
        axes[0, 1].set_title('Per-Class Accuracy')
        axes[0, 1].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 3: Feature Importance
        feature_names = self.TECHNICAL_INDICATORS + [
            'rsi_oversold', 'rsi_overbought', 'macd_bullish',
            'bb_near_lower', 'bb_near_upper', 'strong_trend', 'high_volume'
        ]
        importance = self.feature_importance[model_name]
        indices = np.argsort(importance)[-15:]
        
        axes[1, 0].barh(range(15), importance[indices])
        axes[1, 0].set_yticks(range(15))
        axes[1, 0].set_yticklabels([feature_names[i] for i in indices])
        axes[1, 0].set_xlabel('Feature Importance')
        axes[1, 0].set_title('Top 15 Features')
        axes[1, 0].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 4: –ú–µ—Ç—Ä–∏–∫–∏
        axes[1, 1].axis('off')
        stats_text = f"""
Performance Summary:
  Accuracy: {metrics['accuracy']:.2%}
  Weighted Precision: {metrics['precision']:.2%}
  Weighted Recall: {metrics['recall']:.2%}
  Weighted F1-Score: {metrics['f1']:.3f}
  
Model: XGBoost Multiclass
Classes: 5
Trees: {len(self.models[model_name].get_booster().get_dump())}
        """
        
        axes[1, 1].text(0.1, 0.5, stats_text, fontsize=10,
                       verticalalignment='center', family='monospace',
                       bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.5))
        
        plt.tight_layout()
        plt.savefig(f'{log_dir}/plots/{model_name}_multiclass_evaluation.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def simulate_trading(self, X_test, y_true, y_pred_proba, threshold=0.6):
        """–°–∏–º—É–ª—è—Ü–∏—è —Ç–æ—Ä–≥–æ–≤–ª–∏ –ø–æ —Å–∏–≥–Ω–∞–ª–∞–º –º–æ–¥–µ–ª–∏ –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        trades = []
        
        # –í—Ö–æ–¥–∏–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å > threshold
        signals = y_pred_proba > threshold
        
        # –°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Ç–æ—Ä–≥–æ–≤–ª–∏
        total_signals = np.sum(signals)
        correct_signals = np.sum(signals & (y_true == 1))
        
        if total_signals > 0:
            win_rate = correct_signals / total_signals
            profit_factor = correct_signals / (total_signals - correct_signals) if (total_signals - correct_signals) > 0 else np.inf
        else:
            win_rate = 0
            profit_factor = 0
        
        return {
            'total_signals': total_signals,
            'correct_signals': correct_signals,
            'win_rate': win_rate,
            'profit_factor': profit_factor,
            'threshold': threshold
        }
    
    def train(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è"""
        logger.info("="*80)
        logger.info("üöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø XGBOOST –ú–û–î–ï–õ–ï–ô")
        if self.mode == 'regression':
            logger.info("üìä –ó–∞–¥–∞—á–∞: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ expected returns (—Ä–µ–≥—Ä–µ—Å—Å–∏—è)")
        elif self.mode == 'binary':
            logger.info("üìä –ó–∞–¥–∞—á–∞: –±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (–≤—Ö–æ–¥–∏—Ç—å/–Ω–µ –≤—Ö–æ–¥–∏—Ç—å)")
        else:
            logger.info("üìä –ó–∞–¥–∞—á–∞: –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (5 –∫–ª–∞—Å—Å–æ–≤)")
        logger.info("="*80)
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            df = self.load_data()
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            X, y_buy, y_sell, symbols, timestamps = self.prepare_features(df)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            logger.info("üîÑ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
            X_scaled = self.scaler.fit_transform(X)
            
            # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ (70/15/15)
            n = len(X_scaled)
            train_end = int(n * 0.7)
            val_end = int(n * 0.85)
            
            X_train = X_scaled[:train_end]
            X_val = X_scaled[train_end:val_end]
            X_test = X_scaled[val_end:]
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞
            if self.mode == 'binary':
                logger.info("üîÑ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏...")
                y_buy_binary = self.prepare_binary_labels(y_buy, threshold=0.3)
                y_sell_binary = self.prepare_binary_labels(y_sell, threshold=0.3)
                
                logger.info(f"   Buy - –ö–ª–∞—Å—Å 0 (–Ω–µ –≤—Ö–æ–¥–∏—Ç—å): {np.sum(y_buy_binary == 0):,} ({np.mean(y_buy_binary == 0):.1%})")
                logger.info(f"   Buy - –ö–ª–∞—Å—Å 1 (–≤—Ö–æ–¥–∏—Ç—å): {np.sum(y_buy_binary == 1):,} ({np.mean(y_buy_binary == 1):.1%})")
                logger.info(f"   Sell - –ö–ª–∞—Å—Å 0 (–Ω–µ –≤—Ö–æ–¥–∏—Ç—å): {np.sum(y_sell_binary == 0):,} ({np.mean(y_sell_binary == 0):.1%})")
                logger.info(f"   Sell - –ö–ª–∞—Å—Å 1 (–≤—Ö–æ–¥–∏—Ç—å): {np.sum(y_sell_binary == 1):,} ({np.mean(y_sell_binary == 1):.1%})")
                
                model_configs = [
                    ('buy_classifier', y_buy_binary),
                    ('sell_classifier', y_sell_binary)
                ]
                
            elif self.mode == 'multiclass':
                logger.info("üîÑ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤—ã–µ –º–µ—Ç–∫–∏...")
                y_buy_multi = self.prepare_multiclass_labels(y_buy)
                y_sell_multi = self.prepare_multiclass_labels(y_sell)
                
                logger.info("   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ Buy:")
                for i in range(5):
                    class_names = ['–£–±—ã—Ç–æ—á–Ω—ã–µ', '–û–∫–æ–ª–æ –Ω—É–ª—è', '–ú–∞–ª–æ–ø—Ä–∏–±—ã–ª—å–Ω—ã–µ', '–ü—Ä–∏–±—ã–ª—å–Ω—ã–µ', '–í—ã—Å–æ–∫–æ–ø—Ä–∏–±—ã–ª—å–Ω—ã–µ']
                    logger.info(f"     –ö–ª–∞—Å—Å {i} ({class_names[i]}): {np.sum(y_buy_multi == i):,} ({np.mean(y_buy_multi == i):.1%})")
                
                model_configs = [
                    ('buy_multiclass', y_buy_multi),
                    ('sell_multiclass', y_sell_multi)
                ]
                
            else:  # regression
                model_configs = [
                    ('buy_return_predictor', y_buy),
                    ('sell_return_predictor', y_sell)
                ]
            
            results = {}
            
            for model_name, y_values in model_configs:
                # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                y_train = y_values[:train_end]
                y_val = y_values[train_end:val_end]
                y_test = y_values[val_end:]
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                logger.info(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è {model_name}:")
                logger.info(f"   Train: {len(y_train)} –ø—Ä–∏–º–µ—Ä–æ–≤")
                logger.info(f"   Val: {len(y_val)} –ø—Ä–∏–º–µ—Ä–æ–≤")
                logger.info(f"   Test: {len(y_test)} –ø—Ä–∏–º–µ—Ä–æ–≤")
                logger.info(f"   –°—Ä–µ–¥–Ω–µ–µ: {np.mean(y_train):.3f}%")
                logger.info(f"   Std: {np.std(y_train):.3f}%")
                
                # –û–±—É—á–µ–Ω–∏–µ
                model, metrics = self.train_model(X_train, y_train, X_val, y_val, model_name)
                
                # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–µ
                if self.mode == 'regression':
                    y_pred_test = model.predict(X_test)
                    test_mae = mean_absolute_error(y_test, y_pred_test)
                    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
                    test_r2 = r2_score(y_test, y_pred_test)
                    test_direction = np.mean((y_pred_test > 0) == (y_test > 0))
                    
                    test_metrics = {
                        'mae': test_mae,
                        'rmse': test_rmse,
                        'r2': test_r2,
                        'direction_accuracy': test_direction
                    }
                    
                    logger.info(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–µ –¥–ª—è {model_name}:")
                    logger.info(f"   MAE: {test_mae:.4f}%")
                    logger.info(f"   RMSE: {test_rmse:.4f}%")
                    logger.info(f"   R¬≤: {test_r2:.4f}")
                    logger.info(f"   Direction Accuracy: {test_direction:.2%}")
                    
                elif self.mode == 'binary':
                    y_pred_test = model.predict(X_test)
                    y_pred_test_proba = model.predict_proba(X_test)[:, 1]
                    
                    test_accuracy = accuracy_score(y_test, y_pred_test)
                    test_precision = precision_score(y_test, y_pred_test, zero_division=0)
                    test_recall = recall_score(y_test, y_pred_test, zero_division=0)
                    test_f1 = f1_score(y_test, y_pred_test, zero_division=0)
                    test_auc = roc_auc_score(y_test, y_pred_test_proba)
                    test_cm = confusion_matrix(y_test, y_pred_test)
                    
                    test_metrics = {
                        'accuracy': test_accuracy,
                        'precision': test_precision,
                        'recall': test_recall,
                        'f1': test_f1,
                        'auc': test_auc,
                        'confusion_matrix': test_cm,
                        'y_pred_proba': y_pred_test_proba
                    }
                    
                    logger.info(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–µ –¥–ª—è {model_name}:")
                    logger.info(f"   Accuracy: {test_accuracy:.2%}")
                    logger.info(f"   Precision: {test_precision:.2%}")
                    logger.info(f"   Recall: {test_recall:.2%}")
                    logger.info(f"   F1-Score: {test_f1:.4f}")
                    logger.info(f"   ROC-AUC: {test_auc:.4f}")
                    
                else:  # multiclass
                    y_pred_test = model.predict(X_test)
                    y_pred_test_proba = model.predict_proba(X_test)
                    
                    test_accuracy = accuracy_score(y_test, y_pred_test)
                    test_precision = precision_score(y_test, y_pred_test, average='weighted', zero_division=0)
                    test_recall = recall_score(y_test, y_pred_test, average='weighted', zero_division=0)
                    test_f1 = f1_score(y_test, y_pred_test, average='weighted', zero_division=0)
                    
                    test_metrics = {
                        'accuracy': test_accuracy,
                        'precision': test_precision,
                        'recall': test_recall,
                        'f1': test_f1,
                        'y_pred_proba': y_pred_test_proba
                    }
                    
                    logger.info(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–µ –¥–ª—è {model_name}:")
                    logger.info(f"   Accuracy: {test_accuracy:.2%}")
                    logger.info(f"   Weighted Precision: {test_precision:.2%}")
                    logger.info(f"   Weighted Recall: {test_recall:.2%}")
                    logger.info(f"   Weighted F1-Score: {test_f1:.4f}")
                
                # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
                self.plot_results(y_test, y_pred_test, model_name, test_metrics)
                
                # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
                results[model_name] = {
                    'val_metrics': metrics,
                    'test_metrics': test_metrics
                }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
            self.save_models(results)
            
            # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç
            self.create_report(results)
            
            logger.info("\n‚úÖ –û–ë–£–ß–ï–ù–ò–ï –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!")
            logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {log_dir}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
            raise
    
    def save_models(self, results):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        logger.info("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
        for name, model in self.models.items():
            model_path = f'{MODEL_DIR}/{name}_xgboost.pkl'
            joblib.dump(model, model_path)
            logger.info(f"   ‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler
        scaler_path = f'{MODEL_DIR}/scaler_xgboost.pkl'
        joblib.dump(self.scaler, scaler_path)
        logger.info(f"   ‚úÖ Scaler —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {scaler_path}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º numpy –º–∞—Å—Å–∏–≤—ã –≤ —Å–ø–∏—Å–∫–∏ –¥–ª—è JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        json_safe_results = {}
        for model_name, result in results.items():
            json_safe_results[model_name] = {
                'val_metrics': {},
                'test_metrics': {}
            }
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            for key, value in result['val_metrics'].items():
                if isinstance(value, np.ndarray):
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º–∞—Å—Å–∏–≤—ã —Ç–∏–ø–∞ y_pred_proba
                elif isinstance(value, (np.float32, np.float64)):
                    json_safe_results[model_name]['val_metrics'][key] = float(value)
                elif isinstance(value, (np.int32, np.int64)):
                    json_safe_results[model_name]['val_metrics'][key] = int(value)
                else:
                    json_safe_results[model_name]['val_metrics'][key] = value
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Ç–µ—Å—Ç–∞
            for key, value in result['test_metrics'].items():
                if isinstance(value, np.ndarray):
                    if key == 'confusion_matrix':
                        json_safe_results[model_name]['test_metrics'][key] = value.tolist()
                    else:
                        continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—Ä—É–≥–∏–µ –º–∞—Å—Å–∏–≤—ã
                elif isinstance(value, (np.float32, np.float64)):
                    json_safe_results[model_name]['test_metrics'][key] = float(value)
                elif isinstance(value, (np.int32, np.int64)):
                    json_safe_results[model_name]['test_metrics'][key] = int(value)
                else:
                    json_safe_results[model_name]['test_metrics'][key] = value
        
        metadata = {
            'type': 'xgboost',
            'mode': self.mode,
            'features': self.TECHNICAL_INDICATORS + [
                'rsi_oversold', 'rsi_overbought', 'macd_bullish',
                'bb_near_lower', 'bb_near_upper', 'strong_trend', 'high_volume'
            ],
            'created_at': datetime.now().isoformat(),
            'results': json_safe_results,
            'feature_importance': {
                name: importance.tolist() 
                for name, importance in self.feature_importance.items()
            }
        }
        
        with open(f'{MODEL_DIR}/metadata_xgboost.json', 'w') as f:
            json.dump(metadata, f, indent=2)
        logger.info("   ‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
    
    def create_report(self, results):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        mode_name = {
            'regression': 'Regression (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ expected returns)',
            'binary': 'Binary Classification (–≤—Ö–æ–¥–∏—Ç—å/–Ω–µ –≤—Ö–æ–¥–∏—Ç—å)',
            'multiclass': 'Multiclass Classification (5 –∫–ª–∞—Å—Å–æ–≤)'
        }[self.mode]
        
        report = f"""
{'='*80}
–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ - XGBOOST –ú–û–î–ï–õ–ò
{'='*80}

–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
–¢–∏–ø: XGBoost {mode_name}

–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ù–ê –¢–ï–°–¢–û–í–û–ô –í–´–ë–û–†–ö–ï:
"""
        
        for model_name, result in results.items():
            test_metrics = result['test_metrics']
            report += f"\n{model_name.upper()}:\n"
            
            if self.mode == 'regression':
                report += f"""
- MAE: {test_metrics['mae']:.4f}%
- RMSE: {test_metrics['rmse']:.4f}%
- R¬≤: {test_metrics['r2']:.4f}
- Direction Accuracy: {test_metrics['direction_accuracy']:.2%}
"""
            elif self.mode == 'binary':
                report += f"""
- Accuracy: {test_metrics['accuracy']:.2%}
- Precision: {test_metrics['precision']:.2%}
- Recall: {test_metrics['recall']:.2%}
- F1-Score: {test_metrics['f1']:.4f}
- ROC-AUC: {test_metrics['auc']:.4f}
"""
            else:  # multiclass
                report += f"""
- Accuracy: {test_metrics['accuracy']:.2%}
- Weighted Precision: {test_metrics['precision']:.2%}
- Weighted Recall: {test_metrics['recall']:.2%}
- Weighted F1-Score: {test_metrics['f1']:.4f}
"""
        
        report += f"""
{'='*80}
–ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {MODEL_DIR}
–õ–æ–≥–∏ –∏ –≥—Ä–∞—Ñ–∏–∫–∏: {log_dir}
{'='*80}
"""
        
        with open(f'{log_dir}/final_report.txt', 'w') as f:
            f.write(report)
        
        print(report)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description='–û–±—É—á–µ–Ω–∏–µ XGBoost –º–æ–¥–µ–ª–µ–π')
    parser.add_argument('--mode', choices=['regression', 'binary', 'multiclass'], 
                       default='binary',
                       help='–†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è: regression, binary, multiclass')
    parser.add_argument('--config', default='config.yaml',
                       help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏')
    
    args = parser.parse_args()
    
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è XGBoost –º–æ–¥–µ–ª–µ–π")
    logger.info(f"üéØ –†–µ–∂–∏–º: {args.mode}")
    
    trainer = XGBoostTrainer(config_path=args.config, mode=args.mode)
    trainer.train()


if __name__ == "__main__":
    main()