"""
–ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Transformer v3
"""

import pickle
import pandas as pd
import numpy as np
import logging
from pathlib import Path
from typing import Optional, Dict, Any
import hashlib
import json

from config import Config

logger = logging.getLogger(__name__)


class CacheManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, config: Config):
        self.config = config
        self.cache_dir = Path("cache/transformer_v3")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
    def _get_cache_key(self, params: Dict[str, Any]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∫–ª—é—á–∞ –¥–ª—è –∫–µ—à–∞"""
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä–æ–∫—É –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        param_str = json.dumps(params, sort_keys=True)
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ö–µ—à
        return hashlib.md5(param_str.encode()).hexdigest()
    
    def save_data(self, data: Any, cache_name: str):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –∫–µ—à
        
        Args:
            data: –î–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            cache_name: –ò–º—è —Ñ–∞–π–ª–∞ –∫–µ—à–∞
        """
        cache_path = self.cache_dir / f"{cache_name}.pkl"
        
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
            logger.info(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –∫–µ—à: {cache_path}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –∫–µ—à: {e}")
            
    def load_data(self, cache_name: str) -> Optional[Any]:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫–µ—à–∞
        
        Args:
            cache_name: –ò–º—è —Ñ–∞–π–ª–∞ –∫–µ—à–∞
            
        Returns:
            –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ None
        """
        cache_path = self.cache_dir / f"{cache_name}.pkl"
        
        if not cache_path.exists():
            return None
            
        try:
            with open(cache_path, 'rb') as f:
                data = pickle.load(f)
            logger.info(f"üìÇ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫–µ—à–∞: {cache_path}")
            return data
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ –∫–µ—à–∞: {e}")
            return None
            
    def save_sequences(self, sequences: Dict[str, Dict[str, np.ndarray]], 
                      model_type: str, task_type: str):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        
        Args:
            sequences: –°–ª–æ–≤–∞—Ä—å —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏
            model_type: 'buy' –∏–ª–∏ 'sell'
            task_type: 'regression' –∏–ª–∏ 'classification_binary'
        """
        cache_name = f"sequences_{model_type}_{task_type}"
        self.save_data(sequences, cache_name)
        
    def load_sequences(self, model_type: str, task_type: str) -> Optional[Dict]:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        
        Args:
            model_type: 'buy' –∏–ª–∏ 'sell'
            task_type: 'regression' –∏–ª–∏ 'classification_binary'
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏ –∏–ª–∏ None
        """
        cache_name = f"sequences_{model_type}_{task_type}"
        return self.load_data(cache_name)
        
    def save_processed_data(self, df: pd.DataFrame, stage: str):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            stage: –≠—Ç–∞–ø –æ–±—Ä–∞–±–æ—Ç–∫–∏ ('raw', 'features', 'normalized')
        """
        cache_name = f"data_{stage}"
        
        # –î–ª—è –±–æ–ª—å—à–∏—Ö DataFrame –∏—Å–ø–æ–ª—å–∑—É–µ–º parquet
        if len(df) > 100000:
            cache_path = self.cache_dir / f"{cache_name}.parquet"
            df.to_parquet(cache_path, compression='snappy')
            logger.info(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ parquet: {cache_path}")
        else:
            self.save_data(df, cache_name)
            
    def load_processed_data(self, stage: str) -> Optional[pd.DataFrame]:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            stage: –≠—Ç–∞–ø –æ–±—Ä–∞–±–æ—Ç–∫–∏
            
        Returns:
            DataFrame –∏–ª–∏ None
        """
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º parquet
        parquet_path = self.cache_dir / f"data_{stage}.parquet"
        if parquet_path.exists():
            try:
                df = pd.read_parquet(parquet_path)
                logger.info(f"üìÇ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ parquet: {parquet_path}")
                return df
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ parquet: {e}")
                
        # –ó–∞—Ç–µ–º –ø—Ä–æ–±—É–µ–º pickle
        return self.load_data(f"data_{stage}")
        
    def clear_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ –≤—Å–µ–≥–æ –∫–µ—à–∞"""
        for file in self.cache_dir.glob("*"):
            file.unlink()
        logger.info("üóëÔ∏è –ö–µ—à –æ—á–∏—â–µ–Ω")
        
    def get_cache_info(self) -> Dict[str, Any]:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–µ—à–µ"""
        cache_files = list(self.cache_dir.glob("*"))
        total_size = sum(f.stat().st_size for f in cache_files)
        
        info = {
            'cache_dir': str(self.cache_dir),
            'n_files': len(cache_files),
            'total_size_mb': total_size / 1024 / 1024,
            'files': [f.name for f in cache_files]
        }
        
        return info