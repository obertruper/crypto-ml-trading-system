"""
–ï–¥–∏–Ω—ã–π —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Transformer v3
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å preprocessor –∏ feature_engineer
"""

import pandas as pd
import numpy as np
import logging
from typing import Tuple, Dict, List, Optional
from sklearn.preprocessing import RobustScaler
from sklearn.decomposition import PCA
from sklearn.feature_selection import VarianceThreshold
import pywt
from tqdm import tqdm

from config import Config

logger = logging.getLogger(__name__)


class DataProcessor:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π —à—É–º–∞"""
    
    # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ (30 –∫–ª—é—á–µ–≤—ã—Ö)
    CORE_INDICATORS = [
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Ç—Ä–µ–Ω–¥–æ–≤—ã–µ (8)
        'rsi_val', 'macd_val', 'macd_signal', 'macd_hist',
        'adx_val', 'adx_plus_di', 'adx_minus_di', 'sar',
        
        # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å (6)
        'atr_val', 'bb_upper', 'bb_lower', 'bb_basis',
        'bb_position', 'atr_norm',
        
        # –û–±—ä–µ–º–Ω—ã–µ (4)
        'obv', 'cmf', 'volume_ratio', 'mfi',
        
        # –û—Å—Ü–∏–ª–ª—è—Ç–æ—Ä—ã (6)
        'stoch_k', 'stoch_d', 'cci_val', 'williams_r',
        'roc', 'rsi_dist_from_mid',
        
        # –¶–µ–Ω–æ–≤—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è (6)
        'price_change_1', 'price_change_4', 'price_change_16',
        'volatility_4', 'volatility_16', 'log_return'
    ]
    
    # –ü—Ä–æ—Å—Ç—ã–µ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (5)
    ENGINEERED_FEATURES = [
        'rsi_oversold',    # RSI < 30
        'rsi_overbought',  # RSI > 70
        'macd_bullish',    # MACD > Signal
        'strong_trend',    # ADX > 25
        'high_volume'      # Volume ratio > 2
    ]
    
    def __init__(self, config: Config):
        self.config = config
        self.scaler = RobustScaler(quantile_range=(5, 95))  # –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –∫–ª–∏–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.pca = None
        self.feature_columns = None
        self.variance_threshold = 0.01  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è –ø—Ä–∏–∑–Ω–∞–∫–∞
        
    def process_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        """
        logger.info("üîß –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        # 1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        df = self._extract_core_features(df)
        
        # 2. –°–æ–∑–¥–∞–Ω–∏–µ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        df = self._create_simple_features(df)
        
        # 3. One-hot encoding —Å–∏–º–≤–æ–ª–æ–≤ (—Ç–æ–ª—å–∫–æ —Ç–æ–ø-10)
        df = self._encode_symbols(df)
        
        # 4. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —à—É–º–∞ —Å –ø–æ–º–æ—â—å—é wavelets
        df = self._denoise_features(df)
        
        # 5. –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω–∏–∑–∫–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π
        df = self._remove_low_variance_features(df)
        
        logger.info(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(self.feature_columns)}")
        
        return df
    
    def _extract_core_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        logger.info("üìä –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤...")
        
        processed_data = []
        
        for symbol in tqdm(df['symbol'].unique(), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∏–º–≤–æ–ª–æ–≤"):
            symbol_df = df[df['symbol'] == symbol].copy()
            symbol_df = symbol_df.sort_values('timestamp').reset_index(drop=True)
            
            features_list = []
            
            for idx, row in symbol_df.iterrows():
                feature_dict = {
                    'symbol': symbol,
                    'timestamp': row['timestamp'],
                    'buy_expected_return': row.get('buy_expected_return', 0),
                    'sell_expected_return': row.get('sell_expected_return', 0)
                }
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
                indicators = row.get('technical_indicators', {})
                
                # –ë–∞–∑–æ–≤—ã–µ OHLCV –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ log_return
                feature_dict['close'] = float(row.get('close', 0))
                feature_dict['volume'] = float(row.get('volume', 0))
                
                for indicator in self.CORE_INDICATORS:
                    if indicator == 'log_return':
                        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º log return
                        if idx > 0:
                            prev_close = symbol_df.iloc[idx-1]['close']
                            if prev_close > 0:
                                feature_dict['log_return'] = np.log(row['close'] / prev_close)
                            else:
                                feature_dict['log_return'] = 0.0
                        else:
                            feature_dict['log_return'] = 0.0
                    else:
                        value = indicators.get(indicator, 0.0)
                        if value is None or pd.isna(value):
                            value = 0.0
                        feature_dict[indicator] = float(value)
                
                features_list.append(feature_dict)
            
            symbol_features_df = pd.DataFrame(features_list)
            processed_data.append(symbol_features_df)
        
        result_df = pd.concat(processed_data, ignore_index=True)
        
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        result_df = result_df.drop(['close', 'volume'], axis=1, errors='ignore')
        
        return result_df
    
    def _create_simple_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        logger.info("üî® –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç—ã—Ö –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        # RSI –ø—Ä–∏–∑–Ω–∞–∫–∏
        if 'rsi_val' in df.columns:
            df['rsi_oversold'] = (df['rsi_val'] < 30).astype(float)
            df['rsi_overbought'] = (df['rsi_val'] > 70).astype(float)
        
        # MACD –ø—Ä–∏–∑–Ω–∞–∫
        if 'macd_val' in df.columns and 'macd_signal' in df.columns:
            df['macd_bullish'] = (df['macd_val'] > df['macd_signal']).astype(float)
        
        # ADX –ø—Ä–∏–∑–Ω–∞–∫
        if 'adx_val' in df.columns:
            df['strong_trend'] = (df['adx_val'] > 25).astype(float)
        
        # Volume –ø—Ä–∏–∑–Ω–∞–∫
        if 'volume_ratio' in df.columns:
            df['high_volume'] = (df['volume_ratio'] > 2.0).astype(float)
        
        return df
    
    def _encode_symbols(self, df: pd.DataFrame) -> pd.DataFrame:
        """One-hot encoding —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–æ–ø-10 —Å–∏–º–≤–æ–ª–æ–≤ –ø–æ –æ–±—ä–µ–º—É"""
        logger.info("üè∑Ô∏è –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏–º–≤–æ–ª–æ–≤...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–æ–ø-10 —Å–∏–º–≤–æ–ª–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∑–∞–ø–∏—Å–µ–π
        top_symbols = df['symbol'].value_counts().head(10).index.tolist()
        
        for symbol in top_symbols:
            col_name = f"is_{symbol.replace('USDT', '').lower()}"
            df[col_name] = (df['symbol'] == symbol).astype(float)
        
        # –£–¥–∞–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É symbol
        df = df.drop('symbol', axis=1, errors='ignore')
        
        return df
    
    def _denoise_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ wavelet denoising –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è —à—É–º–∞"""
        logger.info("üîá –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ wavelet denoising...")
        
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        numeric_columns = [col for col in numeric_columns 
                          if col not in ['timestamp', 'buy_expected_return', 'sell_expected_return']]
        
        for col in numeric_columns:
            if df[col].std() > 0:  # –¢–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                # –ü—Ä–∏–º–µ–Ω—è–µ–º wavelet denoising
                coeffs = pywt.wavedec(df[col].fillna(0).values, 'db4', level=3)
                
                # –ú—è–≥–∫–æ–µ –ø–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è —à—É–º–∞
                threshold = 0.1 * np.std(coeffs[-1])
                coeffs = list(coeffs)
                for i in range(1, len(coeffs)):
                    coeffs[i] = pywt.threshold(coeffs[i], threshold, mode='soft')
                
                # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–∏–≥–Ω–∞–ª
                denoised = pywt.waverec(coeffs, 'db4')
                
                # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –Ω—É–∂–Ω–æ–π –¥–ª–∏–Ω—ã
                if len(denoised) > len(df):
                    denoised = denoised[:len(df)]
                elif len(denoised) < len(df):
                    denoised = np.pad(denoised, (0, len(df) - len(denoised)), mode='edge')
                
                df[col] = denoised
        
        return df
    
    def _remove_low_variance_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω–∏–∑–∫–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π"""
        logger.info("üóëÔ∏è –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω–∏–∑–∫–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π...")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        important_cols = ['timestamp', 'buy_expected_return', 'sell_expected_return']
        feature_cols = [col for col in df.columns if col not in important_cols]
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä –¥–∏—Å–ø–µ—Ä—Å–∏–∏
        selector = VarianceThreshold(threshold=self.variance_threshold)
        features_filtered = selector.fit_transform(df[feature_cols])
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–º–µ–Ω–∞ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        selected_features = [feature_cols[i] for i in range(len(feature_cols)) 
                           if selector.variances_[i] >= self.variance_threshold]
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π DataFrame
        result_df = pd.DataFrame(features_filtered, columns=selected_features)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤–∞–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        for col in important_cols:
            if col in df.columns:
                result_df[col] = df[col].values
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.feature_columns = [col for col in result_df.columns 
                               if col not in important_cols]
        
        logger.info(f"   –£–¥–∞–ª–µ–Ω–æ {len(feature_cols) - len(selected_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω–∏–∑–∫–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π")
        
        return result_df
    
    def normalize_data(self, X_train: pd.DataFrame, X_val: pd.DataFrame, 
                      X_test: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º –∫–ª–∏–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≤—ã–±—Ä–æ—Å–æ–≤
        """
        logger.info("üìä –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –∫–ª–∏–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≤—ã–±—Ä–æ—Å–æ–≤...")
        
        # –û–±—É—á–∞–µ–º scaler —Ç–æ–ª—å–∫–æ –Ω–∞ train
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        X_test_scaled = self.scaler.transform(X_test)
        
        # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –∫–ª–∏–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤
        clip_value = 3.0
        X_train_scaled = np.clip(X_train_scaled, -clip_value, clip_value)
        X_val_scaled = np.clip(X_val_scaled, -clip_value, clip_value)
        X_test_scaled = np.clip(X_test_scaled, -clip_value, clip_value)
        
        # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: PCA –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        if len(self.feature_columns) > 40:
            logger.info(f"   –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ PCA: {len(self.feature_columns)} -> 40 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            self.pca = PCA(n_components=40, random_state=42)
            X_train_scaled = self.pca.fit_transform(X_train_scaled)
            X_val_scaled = self.pca.transform(X_val_scaled)
            X_test_scaled = self.pca.transform(X_test_scaled)
        
        return X_train_scaled, X_val_scaled, X_test_scaled
    
    def split_data(self, df: pd.DataFrame) -> Dict[str, Dict[str, np.ndarray]]:
        """
        –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç —É—Ç–µ—á–∫–∏
        """
        logger.info("‚úÇÔ∏è –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ –≤—Ä–µ–º–µ–Ω–∏...")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        df = df.sort_values('timestamp').reset_index(drop=True)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã
        n_samples = len(df)
        train_end = int(n_samples * 0.7)
        val_end = int(n_samples * 0.85)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–∑–æ—Ä –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —É—Ç–µ—á–∫–∏
        gap = 100  # 100 —Å–≤–µ—á–µ–π = 25 —á–∞—Å–æ–≤
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        train_df = df.iloc[:train_end - gap]
        val_df = df.iloc[train_end:val_end - gap]
        test_df = df.iloc[val_end:]
        
        logger.info(f"   Train: {len(train_df):,} | Val: {len(val_df):,} | Test: {len(test_df):,}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        feature_cols = self.feature_columns
        
        X_train = train_df[feature_cols]
        X_val = val_df[feature_cols]
        X_test = test_df[feature_cols]
        
        y_buy_train = train_df['buy_expected_return'].values
        y_buy_val = val_df['buy_expected_return'].values
        y_buy_test = test_df['buy_expected_return'].values
        
        y_sell_train = train_df['sell_expected_return'].values
        y_sell_val = val_df['sell_expected_return'].values
        y_sell_test = test_df['sell_expected_return'].values
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        X_train, X_val, X_test = self.normalize_data(X_train, X_val, X_test)
        
        return {
            'buy': {
                'X_train': X_train, 'y_train': y_buy_train,
                'X_val': X_val, 'y_val': y_buy_val,
                'X_test': X_test, 'y_test': y_buy_test
            },
            'sell': {
                'X_train': X_train, 'y_train': y_sell_train,
                'X_val': X_val, 'y_val': y_sell_val,
                'X_test': X_test, 'y_test': y_sell_test
            }
        }