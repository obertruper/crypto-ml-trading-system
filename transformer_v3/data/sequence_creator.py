"""
–°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è Transformer v3
"""

import numpy as np
import pandas as pd
import logging
from typing import Tuple, Dict, List, Optional
from tqdm import tqdm

from config import Config, SEQUENCE_PARAMS

logger = logging.getLogger(__name__)


class SequenceCreator:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π"""
    
    def __init__(self, config: Config):
        self.config = config
        self.sequence_length = config.model.sequence_length
        self.stride = config.training.sequence_stride
        
    def create_sequences(self, 
                        df: pd.DataFrame,
                        feature_columns: List[str],
                        target_column: str,
                        augment: bool = False) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
        
        Args:
            df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            feature_columns: –°–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            target_column: –ö–æ–ª–æ–Ω–∫–∞ —Å —Ü–µ–ª–µ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
            augment: –ü—Ä–∏–º–µ–Ω—è—Ç—å –ª–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö
            
        Returns:
            Tuple (sequences, targets, symbols)
        """
        logger.info(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª–∏–Ω–æ–π {self.sequence_length}...")
        
        sequences = []
        targets = []
        seq_symbols = []
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        for symbol in tqdm(df['symbol'].unique(), desc="–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π"):
            symbol_df = df[df['symbol'] == symbol].sort_values('timestamp').reset_index(drop=True)
            
            if len(symbol_df) < self.sequence_length + 1:
                logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {len(symbol_df)} –∑–∞–ø–∏—Å–µ–π")
                continue
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            X_symbol = symbol_df[feature_columns].values
            y_symbol = symbol_df[target_column].values
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º expected returns –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            y_mean = np.mean(y_symbol)
            y_std = np.std(y_symbol)
            if abs(y_mean) > 10 or y_std > 10:
                logger.warning(f"‚ö†Ô∏è {symbol} - {target_column}: mean={y_mean:.2f}, std={y_std:.2f} - –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º!")
                y_symbol = y_symbol / 100.0
            
            # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å –∑–∞–¥–∞–Ω–Ω—ã–º —à–∞–≥–æ–º
            for i in range(0, len(X_symbol) - self.sequence_length, self.stride):
                seq = X_symbol[i:i + self.sequence_length]
                target = y_symbol[i + self.sequence_length]
                
                sequences.append(seq)
                targets.append(target)
                seq_symbols.append(symbol)
                
                # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
                if augment and self.config.training.use_augmentation:
                    aug_sequences = self._augment_sequence(seq)
                    for aug_seq in aug_sequences:
                        sequences.append(aug_seq)
                        targets.append(target)
                        seq_symbols.append(symbol)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy arrays
        sequences = np.array(sequences, dtype=np.float32)
        targets = np.array(targets, dtype=np.float32)
        seq_symbols = np.array(seq_symbols)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∑–∞–º–µ–Ω–∞ NaN
        nan_sequences = np.isnan(sequences).sum()
        nan_targets = np.isnan(targets).sum()
        
        if nan_sequences > 0 or nan_targets > 0:
            logger.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ NaN: sequences={nan_sequences}, targets={nan_targets}")
            sequences = np.nan_to_num(sequences, nan=0.0)
            targets = np.nan_to_num(targets, nan=0.0)
        
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(sequences):,} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
        logger.info(f"   –§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {sequences.shape}")
        logger.info(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤: {len(np.unique(seq_symbols))}")
        logger.info(f"   –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è - mean: {np.mean(targets):.4f}, std: {np.std(targets):.4f}")
        logger.info(f"   –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è - min: {np.min(targets):.4f}, max: {np.max(targets):.4f}")
        
        return sequences, targets, seq_symbols
    
    def create_sequences_for_splits(self,
                                   train_df: pd.DataFrame,
                                   val_df: pd.DataFrame,
                                   test_df: pd.DataFrame,
                                   feature_columns: List[str],
                                   target_type: str = 'buy') -> Dict[str, Dict[str, np.ndarray]]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –≤—Å–µ—Ö splits
        
        Args:
            train_df: –û–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
            val_df: –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ  
            test_df: –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            feature_columns: –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            target_type: 'buy' –∏–ª–∏ 'sell'
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ split
        """
        target_column = f"{target_type}_expected_return"
        
        result = {}
        
        # Train split —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π
        if len(train_df) > 0:
            X_train, y_train, symbols_train = self.create_sequences(
                train_df, feature_columns, target_column, augment=True
            )
            result['train'] = {
                'X': X_train,
                'y': y_train,
                'symbols': symbols_train
            }
        
        # Val split –±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        if len(val_df) > 0:
            X_val, y_val, symbols_val = self.create_sequences(
                val_df, feature_columns, target_column, augment=False
            )
            result['val'] = {
                'X': X_val,
                'y': y_val,
                'symbols': symbols_val
            }
        
        # Test split –±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        if len(test_df) > 0:
            X_test, y_test, symbols_test = self.create_sequences(
                test_df, feature_columns, target_column, augment=False
            )
            result['test'] = {
                'X': X_test,
                'y': y_test,
                'symbols': symbols_test
            }
        
        return result
    
    def _augment_sequence(self, sequence: np.ndarray) -> List[np.ndarray]:
        """
        –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        
        Args:
            sequence: –ò—Å—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        """
        augmented = []
        
        if not self.config.training.use_augmentation:
            return augmented
        
        # 1. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞
        noise_level = self.config.training.noise_level
        if noise_level > 0:
            noise = np.random.normal(0, noise_level, sequence.shape)
            noisy_seq = sequence + noise
            augmented.append(noisy_seq)
        
        # 2. –í—Ä–µ–º–µ–Ω–Ω–æ–π —Å–¥–≤–∏–≥ (–Ω–µ–±–æ–ª—å—à–æ–π)
        if len(sequence) > 10:
            # –°–¥–≤–∏–≥ –Ω–∞ 1-2 —à–∞–≥–∞
            for shift in [1, 2]:
                shifted_seq = np.roll(sequence, shift, axis=0)
                # –ó–∞–ø–æ–ª–Ω—è–µ–º –Ω–∞—á–∞–ª–æ —Å—Ä–µ–¥–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                shifted_seq[:shift] = sequence[:shift].mean(axis=0)
                augmented.append(shifted_seq)
        
        return augmented
    
    def create_sliding_windows(self,
                             df: pd.DataFrame,
                             feature_columns: List[str],
                             window_size: int = None) -> np.ndarray:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —Å–∫–æ–ª—å–∑—è—â–∏—Ö –æ–∫–æ–Ω –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        
        Args:
            df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            feature_columns: –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            window_size: –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ (–µ—Å–ª–∏ None - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è sequence_length)
            
        Returns:
            –ú–∞—Å—Å–∏–≤ —Å–∫–æ–ª—å–∑—è—â–∏—Ö –æ–∫–æ–Ω
        """
        if window_size is None:
            window_size = self.sequence_length
            
        features = df[feature_columns].values
        
        if len(features) < window_size:
            # –ü–∞–¥–¥–∏–Ω–≥ –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
            pad_size = window_size - len(features)
            padding = np.repeat(features[0:1], pad_size, axis=0)
            features = np.vstack([padding, features])
        
        # –°–æ–∑–¥–∞–µ–º –æ–∫–Ω–æ
        window = features[-window_size:]
        
        return window.reshape(1, window_size, -1)
    
    def get_sequence_statistics(self, sequences: np.ndarray, targets: np.ndarray) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º"""
        stats = {
            'n_sequences': len(sequences),
            'sequence_length': sequences.shape[1] if sequences.ndim > 1 else 0,
            'n_features': sequences.shape[2] if sequences.ndim > 2 else 0,
            'target_stats': {
                'mean': np.mean(targets),
                'std': np.std(targets),
                'min': np.min(targets),
                'max': np.max(targets),
                'positive_ratio': np.mean(targets > 0)
            }
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        if self.config.training.task_type == 'classification_binary':
            unique, counts = np.unique(targets, return_counts=True)
            stats['class_distribution'] = dict(zip(unique, counts))
        
        return stats