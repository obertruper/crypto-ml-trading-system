"""
–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Transformer v3
–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ JSON –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
"""

import pandas as pd
import numpy as np
import logging
from typing import Tuple, Dict, List, Optional
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import train_test_split
from tqdm import tqdm

from config import Config, TECHNICAL_INDICATORS, ENGINEERED_FEATURES, VALIDATION_PARAMS

logger = logging.getLogger(__name__)


class DataPreprocessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, config: Config):
        self.config = config
        self.scaler = RobustScaler()
        self.feature_columns = None
        self.all_features = TECHNICAL_INDICATORS + ENGINEERED_FEATURES
        
    def extract_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ JSON –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        Args:
            df: DataFrame —Å —Å—ã—Ä—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            DataFrame —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        """
        logger.info("üîß –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ technical_indicators...")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ expected returns
        self._check_expected_returns(df)
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Å–∏–º–≤–æ–ª–∞–º –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        grouped_data = []
        
        for symbol in tqdm(df["symbol"].unique(), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∏–º–≤–æ–ª–æ–≤"):
            symbol_df = df[df["symbol"] == symbol].copy()
            symbol_df = symbol_df.sort_values("timestamp").reset_index(drop=True)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏
            features_list = []
            
            for idx, row in symbol_df.iterrows():
                feature_dict = {
                    'symbol': symbol,
                    'timestamp': row['timestamp'],
                    'datetime': row['datetime'],
                    'buy_expected_return': row['buy_expected_return'],
                    'sell_expected_return': row['sell_expected_return']
                }
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
                indicators = row["technical_indicators"]
                for indicator in TECHNICAL_INDICATORS:
                    value = indicators.get(indicator, 0.0)
                    if value is None or pd.isna(value):
                        value = 0.0
                    feature_dict[indicator] = float(value)
                
                # –°–æ–∑–¥–∞–µ–º –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                feature_dict.update(self._create_engineered_features(indicators))
                
                features_list.append(feature_dict)
            
            symbol_features_df = pd.DataFrame(features_list)
            grouped_data.append(symbol_features_df)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        result_df = pd.concat(grouped_data, ignore_index=True)
        
        logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(self.all_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {len(result_df)} –∑–∞–ø–∏—Å–µ–π")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.feature_columns = self.all_features
        
        return result_df
    
    def _create_engineered_features(self, indicators: Dict) -> Dict:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        features = {}
        
        # RSI –ø—Ä–∏–∑–Ω–∞–∫–∏
        rsi = indicators.get("rsi_val", 50.0)
        features["rsi_oversold"] = 1.0 if rsi is not None and rsi < 30 else 0.0
        features["rsi_overbought"] = 1.0 if rsi is not None and rsi > 70 else 0.0
        
        # MACD –ø—Ä–∏–∑–Ω–∞–∫–∏
        macd = indicators.get("macd_val", 0.0)
        macd_signal = indicators.get("macd_signal", 0.0)
        features["macd_bullish"] = 1.0 if macd is not None and macd_signal is not None and macd > macd_signal else 0.0
        
        # Bollinger Bands –ø—Ä–∏–∑–Ω–∞–∫–∏
        bb_position = indicators.get("bb_position", 0.5)
        features["bb_near_lower"] = 1.0 if bb_position is not None and bb_position < 0.2 else 0.0
        features["bb_near_upper"] = 1.0 if bb_position is not None and bb_position > 0.8 else 0.0
        
        # ADX —Ç—Ä–µ–Ω–¥
        adx = indicators.get("adx_val", 0.0)
        features["strong_trend"] = 1.0 if adx is not None and adx > 25 else 0.0
        
        # –û–±—ä–µ–º
        volume_ratio = indicators.get("volume_ratio", 1.0)
        features["high_volume"] = 1.0 if volume_ratio is not None and volume_ratio > 2.0 else 0.0
        
        return features
    
    def _check_expected_returns(self, df: pd.DataFrame):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ expected returns"""
        buy_returns = df['buy_expected_return'].values
        sell_returns = df['sell_expected_return'].values
        
        buy_outliers = np.sum((buy_returns < -1.1) | (buy_returns > 5.8))
        sell_outliers = np.sum((sell_returns < -1.1) | (sell_returns > 5.8))
        
        if buy_outliers > 0 or sell_outliers > 0:
            logger.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω—ã –∑–Ω–∞—á–µ–Ω–∏—è expected_return –≤–Ω–µ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ [-1.1%, +5.8%]:")
            logger.warning(f"   BUY outliers: {buy_outliers} ({buy_outliers/len(df)*100:.2f}%)")
            logger.warning(f"   SELL outliers: {sell_outliers} ({sell_outliers/len(df)*100:.2f}%)")
    
    def split_data_temporal(self, 
                          df: pd.DataFrame) -> Dict[str, Dict[str, pd.DataFrame]]:
        """
        –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å —É—á–µ—Ç–æ–º gap –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —É—Ç–µ—á–∫–∏
        
        Args:
            df: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        """
        logger.info("üìä –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        train_ratio = VALIDATION_PARAMS['train_ratio']
        val_ratio = VALIDATION_PARAMS['val_ratio']
        test_ratio = VALIDATION_PARAMS['test_ratio']
        gap = VALIDATION_PARAMS['gap_periods']
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        grouped_splits = {
            "train": [],
            "val": [],
            "test": []
        }
        
        for symbol in df["symbol"].unique():
            symbol_df = df[df["symbol"] == symbol].sort_values("timestamp").reset_index(drop=True)
            n = len(symbol_df)
            
            if n < 1000:  # –ú–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∏–º–≤–æ–ª–∞
                logger.warning(f"‚ö†Ô∏è –°–∏–º–≤–æ–ª {symbol} –∏–º–µ–µ—Ç —Ç–æ–ª—å–∫–æ {n} –∑–∞–ø–∏—Å–µ–π, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue
            
            # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
            train_end = int(n * train_ratio)
            val_end = int(n * (train_ratio + val_ratio))
            
            # –†–∞–∑–¥–µ–ª—è–µ–º —Å —É—á–µ—Ç–æ–º gap
            splits = {
                "train": symbol_df[:train_end - gap],
                "val": symbol_df[train_end + gap:val_end - gap],
                "test": symbol_df[val_end + gap:]
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—â–∏–µ —Å–ø–∏—Å–∫–∏
            for split_name, split_df in splits.items():
                if len(split_df) > 0:
                    grouped_splits[split_name].append(split_df)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ splits
        final_splits = {}
        for split_name, dfs in grouped_splits.items():
            if dfs:
                final_splits[split_name] = pd.concat(dfs, ignore_index=True)
                logger.info(f"   {split_name}: {len(final_splits[split_name]):,} –∑–∞–ø–∏—Å–µ–π")
            else:
                logger.warning(f"‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {split_name}")
                final_splits[split_name] = pd.DataFrame()
        
        return final_splits
    
    def normalize_features(self, 
                         train_df: pd.DataFrame,
                         val_df: pd.DataFrame,
                         test_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        Args:
            train_df: –û–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
            val_df: –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            test_df: –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            
        Returns:
            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        """
        logger.info("üìè –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        # Fit scaler —Ç–æ–ª—å–∫–æ –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö
        train_features = train_df[self.feature_columns]
        self.scaler.fit(train_features)
        
        # Transform –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        train_normalized = train_df.copy()
        val_normalized = val_df.copy()
        test_normalized = test_df.copy()
        
        train_normalized[self.feature_columns] = self.scaler.transform(train_features)
        
        if len(val_df) > 0:
            val_normalized[self.feature_columns] = self.scaler.transform(val_df[self.feature_columns])
            
        if len(test_df) > 0:
            test_normalized[self.feature_columns] = self.scaler.transform(test_df[self.feature_columns])
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ expected returns
        for col in ['buy_expected_return', 'sell_expected_return']:
            if col in train_normalized.columns:
                logger.info(f"üìä {col} —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: mean={train_normalized[col].mean():.4f}, "
                           f"std={train_normalized[col].std():.4f}, "
                           f"min={train_normalized[col].min():.4f}, "
                           f"max={train_normalized[col].max():.4f}")
                
                # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º expected returns –µ—Å–ª–∏ –æ–Ω–∏ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ
                if train_normalized[col].abs().max() > 10:
                    logger.warning(f"‚ö†Ô∏è {col} –∏–º–µ–µ—Ç –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è, –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –Ω–∞ 100")
                    train_normalized[col] = train_normalized[col] / 100
                    if len(val_df) > 0:
                        val_normalized[col] = val_normalized[col] / 100
                    if len(test_df) > 0:
                        test_normalized[col] = test_normalized[col] / 100
        
        logger.info("‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        
        return train_normalized, val_normalized, test_normalized
    
    def convert_to_binary_labels(self, returns: pd.Series, threshold: float = 0.3) -> pd.Series:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ expected returns –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏
        
        Args:
            returns: Series —Å expected returns
            threshold: –ü–æ—Ä–æ–≥ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (%)
            
        Returns:
            –ë–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏
        """
        return (returns > threshold).astype(np.float32)
    
    def get_feature_statistics(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º"""
        stats = []
        
        for col in self.feature_columns:
            if col in df.columns:
                col_stats = {
                    'feature': col,
                    'mean': df[col].mean(),
                    'std': df[col].std(),
                    'min': df[col].min(),
                    'max': df[col].max(),
                    'missing': df[col].isna().sum(),
                    'zeros': (df[col] == 0).sum()
                }
                stats.append(col_stats)
        
        return pd.DataFrame(stats)