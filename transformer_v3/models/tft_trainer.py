"""
Trainer –¥–ª—è Temporal Fusion Transformer
–ê–Ω–∞–ª–æ–≥ XGBoostTrainer –∏–∑ xgboost_v3
"""

import tensorflow as tf
from tensorflow import keras
import numpy as np
import pandas as pd
import logging
from typing import Dict, Tuple, Optional, List
from pathlib import Path
import json
import pickle
from datetime import datetime

from config import Config
from models.tft_model import TemporalFusionTransformer
from utils.metrics import MetricsCalculator
from utils.visualization import VisualizationCallback

logger = logging.getLogger(__name__)


class TFTTrainer:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è TFT –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self, config: Config, model_name: str = "tft_model"):
        self.config = config
        self.model_name = model_name
        self.model = None
        self.metrics_calculator = MetricsCalculator(config)
        self.training_history = None
        self.scaler = None
        self.feature_columns = None
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU
        self._setup_gpu()
        
    def _setup_gpu(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU –¥–ª—è TensorFlow"""
        gpus = tf.config.experimental.list_physical_devices('GPU')
        if gpus:
            try:
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ multi-GPU —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
                if self.config.model.use_multi_gpu and len(gpus) > 1:
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
                    num_gpus = self.config.model.num_gpus if self.config.model.num_gpus > 0 else len(gpus)
                    num_gpus = min(num_gpus, len(gpus))
                    
                    # –°–æ–∑–¥–∞–µ–º MirroredStrategy –¥–ª—è multi-GPU
                    self.strategy = tf.distribute.MirroredStrategy(
                        devices=[f"/gpu:{i}" for i in range(num_gpus)]
                    )
                    logger.info(f"üöÄ Multi-GPU —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–∞: {num_gpus} GPU")
                    logger.info(f"   –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch size: {self.config.training.batch_size * num_gpus}")
                else:
                    # Single GPU —Å—Ç—Ä–∞—Ç–µ–≥–∏—è
                    self.strategy = tf.distribute.OneDeviceStrategy(device="/gpu:0")
                    logger.info(f"üñ•Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è single GPU: {gpus[0].name}")
                
                # –í–∫–ª—é—á–∞–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –≤—ã–¥–µ–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –¥–ª—è –≤—Å–µ—Ö GPU
                for gpu in gpus:
                    tf.config.experimental.set_memory_growth(gpu, True)
                    
                # Mixed precision –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                if self.config.model.use_mixed_precision:
                    policy = tf.keras.mixed_precision.Policy('mixed_float16')
                    tf.keras.mixed_precision.set_global_policy(policy)
                    logger.info("‚ö° Mixed precision –≤–∫–ª—é—á–µ–Ω –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤ 2x")
                else:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º float32 –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                    policy = tf.keras.mixed_precision.Policy('float32')
                    tf.keras.mixed_precision.set_global_policy(policy)
                    logger.info("üìä –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è float32 (–º–æ–∂–Ω–æ –≤–∫–ª—é—á–∏—Ç—å mixed precision –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)")
                
                # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è TF
                tf.config.optimizer.set_jit(True)  # XLA compilation –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                tf.config.run_functions_eagerly(False)
                    
            except RuntimeError as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ GPU: {e}")
                self.strategy = tf.distribute.get_strategy()  # Default strategy
        else:
            logger.info("üíª GPU –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
            self.strategy = tf.distribute.get_strategy()
    
    def create_model(self, num_features: int) -> TemporalFusionTransformer:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ TFT
        
        Args:
            num_features: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            
        Returns:
            –ú–æ–¥–µ–ª—å TFT
        """
        logger.info(f"üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ {self.model_name}...")
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å –≤ —Ä–∞–º–∫–∞—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        with self.strategy.scope():
            model = TemporalFusionTransformer(
                config=self.config,
                num_features=num_features,
                name=self.model_name
            )
            
            # –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏
            self._compile_model(model)
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
            dummy_input = tf.zeros((1, self.config.model.sequence_length, num_features))
            _ = model(dummy_input)
        
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –º–æ–¥–µ–ª—å —Å {model.count_params():,} –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏")
        
        return model
    
    def _compile_model(self, model: TemporalFusionTransformer):
        """–ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        optimizer = self._create_optimizer()
        
        # Loss —Ñ—É–Ω–∫—Ü–∏—è
        if self.config.training.task_type == 'regression':
            if self.config.training.loss_function == 'huber':
                # Huber loss –±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤–∞ –∫ –≤—ã–±—Ä–æ—Å–∞–º
                loss = keras.losses.Huber(
                    delta=self.config.training.huber_delta,
                    reduction=keras.losses.Reduction.SUM_OVER_BATCH_SIZE
                )
            elif self.config.training.loss_function == 'mse':
                loss = keras.losses.MeanSquaredError(
                    reduction=keras.losses.Reduction.SUM_OVER_BATCH_SIZE
                )
            else:  # mae
                loss = keras.losses.MeanAbsoluteError(
                    reduction=keras.losses.Reduction.SUM_OVER_BATCH_SIZE
                )
                
            metrics = [
                keras.metrics.MeanAbsoluteError(name='mae'),
                keras.metrics.RootMeanSquaredError(name='rmse')
            ]
        else:  # classification_binary
            loss = keras.losses.BinaryCrossentropy(
                label_smoothing=self.config.training.label_smoothing
            )
            metrics = [
                keras.metrics.BinaryAccuracy(name='accuracy'),
                keras.metrics.Precision(name='precision'),
                keras.metrics.Recall(name='recall'),
                keras.metrics.AUC(name='auc')
            ]
        
        model.compile(
            optimizer=optimizer,
            loss=loss,
            metrics=metrics
        )
    
    def _create_optimizer(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞"""
        # –°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π learning rate –¥–ª—è multi-GPU
        lr = self.config.training.learning_rate
        if hasattr(self, 'strategy') and self.strategy.num_replicas_in_sync > 1:
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º LR –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É GPU
            lr = lr * self.strategy.num_replicas_in_sync
            logger.info(f"üìà Learning rate —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω –¥–ª—è {self.strategy.num_replicas_in_sync} GPU: {lr}")
        
        if self.config.training.optimizer == 'adam':
            optimizer = keras.optimizers.Adam(
                learning_rate=lr,
                beta_1=self.config.training.beta_1,
                beta_2=self.config.training.beta_2,
                epsilon=self.config.training.epsilon,
                clipnorm=1.0  # Gradient clipping –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            )
        elif self.config.training.optimizer == 'adamw':
            optimizer = keras.optimizers.AdamW(
                learning_rate=lr,
                beta_1=self.config.training.beta_1,
                beta_2=self.config.training.beta_2,
                epsilon=self.config.training.epsilon,
                weight_decay=0.01,
                clipnorm=1.0
            )
        else:  # sgd
            optimizer = keras.optimizers.SGD(
                learning_rate=lr,
                momentum=0.9,
                nesterov=True,
                clipnorm=1.0
            )
        
        # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ mixed precision optimizer –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if self.config.model.use_mixed_precision:
            optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)
            logger.info("‚ö° –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –æ–±–µ—Ä–Ω—É—Ç –¥–ª—è mixed precision")
            
        return optimizer
    
    def train(self,
             X_train: np.ndarray,
             y_train: np.ndarray,
             X_val: np.ndarray,
             y_val: np.ndarray,
             feature_columns: List[str] = None) -> keras.Model:
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        
        Args:
            X_train: –û–±—É—á–∞—é—â–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ [batch, seq_len, features]
            y_train: –û–±—É—á–∞—é—â–∏–µ –º–µ—Ç–∫–∏
            X_val: –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            y_val: –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
            feature_columns: –ù–∞–∑–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            
        Returns:
            –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"üöÄ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: {self.model_name}")
        logger.info(f"{'='*60}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ NaN
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ NaN...")
        train_nan_count = np.isnan(X_train).sum() + np.isnan(y_train).sum()
        val_nan_count = np.isnan(X_val).sum() + np.isnan(y_val).sum()
        
        if train_nan_count > 0:
            logger.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ {train_nan_count} NaN –∑–Ω–∞—á–µ–Ω–∏–π –≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö!")
            # –ó–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ 0
            X_train = np.nan_to_num(X_train, nan=0.0)
            y_train = np.nan_to_num(y_train, nan=0.0)
            
        if val_nan_count > 0:
            logger.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ {val_nan_count} NaN –∑–Ω–∞—á–µ–Ω–∏–π –≤ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!")
            X_val = np.nan_to_num(X_val, nan=0.0)
            y_val = np.nan_to_num(y_val, nan=0.0)
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ y_train: mean={np.mean(y_train):.4f}, std={np.std(y_train):.4f}, "
                   f"min={np.min(y_train):.4f}, max={np.max(y_train):.4f}")
        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ X_train: mean={np.mean(X_train):.4f}, std={np.std(X_train):.4f}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–ª–æ–Ω–∫–∏
        self.feature_columns = feature_columns
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        num_features = X_train.shape[2]
        self.model = self.create_model(num_features)
        
        # Callbacks
        callbacks = self._create_callbacks()
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ tf.data.Dataset –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        train_dataset = self._create_dataset(X_train, y_train, shuffle=True)
        val_dataset = self._create_dataset(X_val, y_val, shuffle=False)
        
        # –û–±—É—á–µ–Ω–∏–µ
        logger.info("üèãÔ∏è –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
        logger.info(f"   –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {X_train.shape}")
        logger.info(f"   –†–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: {X_val.shape}")
        
        self.training_history = self.model.fit(
            train_dataset,
            validation_data=val_dataset,
            epochs=self.config.training.epochs,
            callbacks=callbacks,
            verbose=1 if self.config.training.verbose else 0
        )
        
        logger.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
        
        return self.model
    
    def _create_dataset(self, X: np.ndarray, y: np.ndarray, shuffle: bool = False) -> tf.data.Dataset:
        """–°–æ–∑–¥–∞–Ω–∏–µ tf.data.Dataset —Å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞–º—è—Ç–∏"""
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float32 –∑–∞—Ä–∞–Ω–µ–µ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        X = X.astype(np.float32)
        y = y.astype(np.float32)
        
        # –ó–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ 0
        X = np.nan_to_num(X, nan=0.0)
        y = np.nan_to_num(y, nan=0.0)
        
        # –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤–º–µ—Å—Ç–æ from_tensor_slices
        if len(X) > 100000:  # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 100k –ø—Ä–∏–º–µ—Ä–æ–≤
            logger.info(f"üìä –ë–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç ({len(X)} –∑–∞–ø–∏—Å–µ–π), –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä")
            
            def generator():
                for i in range(len(X)):
                    yield X[i], y[i]
            
            dataset = tf.data.Dataset.from_generator(
                generator,
                output_signature=(
                    tf.TensorSpec(shape=(self.config.model.sequence_length, X.shape[2]), dtype=tf.float32),
                    tf.TensorSpec(shape=(), dtype=tf.float32)
                )
            )
        else:
            # –î–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π —Å–ø–æ—Å–æ–±
            dataset = tf.data.Dataset.from_tensor_slices((X, y))
        
        # –û–ø—Ü–∏–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        options = tf.data.Options()
        options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA
        options.experimental_optimization.parallel_batch = True
        options.experimental_threading.private_threadpool_size = 8
        options.experimental_threading.max_intra_op_parallelism = 1
        dataset = dataset.with_options(options)
        
        if shuffle:
            # –ë–æ–ª—å—à–æ–π buffer –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏—è
            buffer_size = min(50000, len(X))
            dataset = dataset.shuffle(buffer_size=buffer_size, reshuffle_each_iteration=True)
        
        # –ë–∞—Ç—á–∏–Ω–≥ —Å —É—á–µ—Ç–æ–º gradient accumulation –∏ multi-GPU
        actual_batch_size = self.config.training.batch_size
        
        # –î–ª—è multi-GPU: –∫–∞–∂–¥–∞—è GPU –ø–æ–ª—É—á–∞–µ—Ç batch_size –¥–∞–Ω–Ω—ã—Ö
        # –ì–ª–æ–±–∞–ª—å–Ω—ã–π batch = batch_size * num_gpus
        if hasattr(self, 'strategy') and self.strategy.num_replicas_in_sync > 1:
            # –≠—Ç–æ batch PER REPLICA (–Ω–∞ –∫–∞–∂–¥—É—é GPU)
            actual_batch_size = self.config.training.batch_size
            global_batch_size = actual_batch_size * self.strategy.num_replicas_in_sync
            logger.info(f"üöÄ Multi-GPU –±–∞—Ç—á–∏–Ω–≥: {actual_batch_size} per GPU, –≥–ª–æ–±–∞–ª—å–Ω—ã–π batch: {global_batch_size}")
        elif hasattr(self.config.training, 'gradient_accumulation_steps'):
            # –î–µ–ª–∏–º batch size –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è
            actual_batch_size = self.config.training.batch_size // self.config.training.gradient_accumulation_steps
            logger.info(f"üì¶ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è gradient accumulation: {self.config.training.gradient_accumulation_steps} —à–∞–≥–æ–≤")
            logger.info(f"   Batch per step: {actual_batch_size}, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch: {self.config.training.batch_size}")
        
        dataset = dataset.batch(actual_batch_size, drop_remainder=True)
        
        # Prefetch —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π
        dataset = dataset.prefetch(tf.data.AUTOTUNE)
        
        # Cache –≤ –ø–∞–º—è—Ç–∏ –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        if len(X) < 500000:  # –ö–µ—à–∏—Ä—É–µ–º –µ—Å–ª–∏ –º–µ–Ω—å—à–µ 500k –ø—Ä–∏–º–µ—Ä–æ–≤
            dataset = dataset.cache()
            logger.info("üíæ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–∫–µ—à–∏—Ä–æ–≤–∞–Ω –≤ –ø–∞–º—è—Ç–∏")
        
        return dataset
    
    def _create_callbacks(self) -> List[keras.callbacks.Callback]:
        """–°–æ–∑–¥–∞–Ω–∏–µ callbacks –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        callbacks = []
        
        # Early Stopping
        callbacks.append(
            keras.callbacks.EarlyStopping(
                monitor=self.config.training.early_stopping_monitor,
                patience=self.config.training.early_stopping_patience,
                mode=self.config.training.early_stopping_mode,
                restore_best_weights=self.config.training.restore_best_weights,
                verbose=1
            )
        )
        
        # Reduce LR on Plateau
        callbacks.append(
            keras.callbacks.ReduceLROnPlateau(
                monitor=self.config.training.early_stopping_monitor,
                factor=self.config.training.reduce_lr_factor,
                patience=self.config.training.reduce_lr_patience,
                min_lr=self.config.training.reduce_lr_min,
                verbose=1
            )
        )
        
        # Model Checkpoint
        if self.config.training.save_models:
            checkpoint_path = Path(self.config.training.model_dir) / f"{self.model_name}_best.h5"
            callbacks.append(
                keras.callbacks.ModelCheckpoint(
                    filepath=str(checkpoint_path),
                    monitor=self.config.training.checkpoint_monitor,
                    save_best_only=self.config.training.save_best_only,
                    save_weights_only=False,
                    verbose=1
                )
            )
        
        # TensorBoard
        if self.config.training.tensorboard:
            log_dir = self.config.get_log_dir() / "tensorboard" / self.model_name
            callbacks.append(
                keras.callbacks.TensorBoard(
                    log_dir=str(log_dir),
                    histogram_freq=1,
                    write_graph=True,
                    update_freq='epoch'
                )
            )
        
        # Visualization Callback
        if self.config.training.save_plots:
            callbacks.append(
                VisualizationCallback(
                    log_dir=self.config.get_log_dir(),
                    model_name=self.model_name,
                    update_freq=self.config.training.plot_update_freq,
                    task=self.config.training.task_type
                )
            )
        
        return callbacks
    
    def predict(self, X: np.ndarray, return_proba: bool = False) -> np.ndarray:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        
        Args:
            X: –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            return_proba: –í–µ—Ä–Ω—É—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (–¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)
            
        Returns:
            –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        """
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ train()")
        
        predictions = self.model.predict(X, verbose=0)
        predictions = predictions.flatten()
        
        if self.config.training.task_type == "classification_binary" and not return_proba:
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥
            threshold = self.config.training.classification_threshold / 100
            predictions = (predictions > threshold).astype(int)
        
        return predictions
    
    def evaluate(self, X: np.ndarray, y: np.ndarray, dataset_name: str = "Test") -> Dict[str, float]:
        """
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        
        Args:
            X: –ü—Ä–∏–∑–Ω–∞–∫–∏
            y: –ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
            dataset_name: –ù–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        predictions = self.predict(X, return_proba=True)
        
        if self.config.training.task_type == "regression":
            metrics = self.metrics_calculator.calculate_regression_metrics(y, predictions)
        else:
            metrics = self.metrics_calculator.calculate_classification_metrics(y, predictions)
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        logger.info(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ {dataset_name}:")
        for metric_name, value in metrics.items():
            if isinstance(value, (int, float)):
                logger.info(f"   {metric_name}: {value:.4f}")
        
        return metrics
    
    def save_model(self, save_dir: Path):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        save_dir = Path(save_dir)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        model_path = save_dir / f"{self.model_name}.h5"
        self.model.save(model_path)
        logger.info(f"   ‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å–∞ –æ—Ç–¥–µ–ª—å–Ω–æ (–¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏)
        weights_path = str(save_dir / f"{self.model_name}.weights.h5")
        self.model.save_weights(weights_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            'model_name': self.model_name,
            'task_type': self.config.training.task_type,
            'created_at': datetime.now().isoformat(),
            'config': {
                'model': self.config.model.__dict__,
                'training': self.config.training.__dict__
            },
            'feature_columns': self.feature_columns,
            'training_history': self.training_history.history if self.training_history else None
        }
        
        metadata_path = save_dir / f"{self.model_name}_metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler –µ—Å–ª–∏ –µ—Å—Ç—å
        if self.scaler:
            scaler_path = save_dir / f"{self.model_name}_scaler.pkl"
            with open(scaler_path, 'wb') as f:
                pickle.dump(self.scaler, f)
    
    def load_model(self, model_path: Path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        self.model = keras.models.load_model(model_path, compile=False)
        self._compile_model(self.model)
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {model_path}")
    
    def get_feature_importance(self) -> pd.DataFrame:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ VSN"""
        if self.model is None:
            return pd.DataFrame()
        
        importance = self.model.get_feature_importance()
        if importance is None or self.feature_columns is None:
            return pd.DataFrame()
        
        # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –±–∞—Ç—á–∞–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if importance.ndim > 1:
            importance = importance.mean(axis=0)
        
        df = pd.DataFrame({
            'feature': self.feature_columns[:len(importance)],
            'importance': importance
        })
        
        df = df.sort_values('importance', ascending=False).reset_index(drop=True)
        
        return df