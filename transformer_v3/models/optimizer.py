"""
Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è TFT
–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–æ –∏–∑ XGBoost v3.0
"""

import optuna
import numpy as np
import tensorflow as tf
from tensorflow import keras
import logging
from typing import Dict, Tuple, Callable, Optional
import tempfile
import shutil
from pathlib import Path

from config import Config
from .tft_model import create_tft_model

logger = logging.getLogger(__name__)


class OptunaOptimizer:
    """
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ TFT —Å –ø–æ–º–æ—â—å—é Optuna
    """
    
    def __init__(self, config: Config):
        self.config = config
        self.study = None
        self.best_params = None
        self.trial_results = []
        
    def optimize(self,
                X_train: np.ndarray,
                y_train: np.ndarray,
                X_val: np.ndarray, 
                y_val: np.ndarray,
                n_trials: int = 50,
                model_type: str = "buy") -> Dict:
        """
        –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        
        Args:
            X_train: –û–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
            y_train: –û–±—É—á–∞—é—â–∏–µ –º–µ—Ç–∫–∏
            X_val: –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            y_val: –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
            n_trials: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            model_type: –¢–∏–ø –º–æ–¥–µ–ª–∏ (buy/sell)
            
        Returns:
            –õ—É—á—à–∏–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        """
        logger.info(f"üîç –ó–∞–ø—É—Å–∫ Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è {model_type} –º–æ–¥–µ–ª–∏...")
        logger.info(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫: {n_trials}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –∞—Ç—Ä–∏–±—É—Ç—ã –¥–ª—è objective —Ñ—É–Ω–∫—Ü–∏–∏
        self.X_train = X_train
        self.y_train = y_train
        self.X_val = X_val
        self.y_val = y_val
        self.input_shape = (X_train.shape[1], X_train.shape[2])
        
        # –°–æ–∑–¥–∞–µ–º study
        study_name = f"tft_optimization_{model_type}"
        self.study = optuna.create_study(
            direction='minimize',
            study_name=study_name,
            sampler=optuna.samplers.TPESampler(seed=42)
        )
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é
        self.study.optimize(
            self._objective,
            n_trials=n_trials,
            timeout=None,
            show_progress_bar=True
        )
        
        self.best_params = self.study.best_params
        
        logger.info("‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        logger.info(f"   –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {self.study.best_value:.4f}")
        logger.info(f"   –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {self.best_params}")
        
        return self.best_params
    
    def _objective(self, trial: optuna.Trial) -> float:
        """
        Objective —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è Optuna
        
        Args:
            trial: Optuna trial –æ–±—ä–µ–∫—Ç
            
        Returns:
            –ó–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏
        """
        # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        params = self._suggest_parameters(trial)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config = self._update_config_with_params(params)
        
        try:
            # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            model = create_tft_model(config, self.input_shape)
            
            # Callbacks –¥–ª—è —Ä–∞–Ω–Ω–µ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            callbacks = [
                keras.callbacks.EarlyStopping(
                    monitor='val_loss',
                    patience=max(5, params.get('early_stopping_patience', 10) // 2),
                    restore_best_weights=True,
                    verbose=0
                )
            ]
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            history = model.fit(
                self.X_train, self.y_train,
                validation_data=(self.X_val, self.y_val),
                epochs=min(params.get('epochs', 50), 30),  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —ç–ø–æ—Ö–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
                batch_size=params.get('batch_size', 32),
                callbacks=callbacks,
                verbose=0
            )
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à—É—é val_loss
            best_val_loss = min(history.history['val_loss'])
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            self.trial_results.append({
                'trial_number': trial.number,
                'params': params,
                'val_loss': best_val_loss,
                'epochs_trained': len(history.history['val_loss'])
            })
            
            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
            del model
            tf.keras.backend.clear_session()
            
            return best_val_loss
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Trial {trial.number} failed: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–æ–ª—å—à–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –Ω–µ—É–¥–∞—á–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
            return float('inf')
    
    def _suggest_parameters(self, trial: optuna.Trial) -> Dict:
        """
        –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        """
        params = {}
        
        # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏
        params['hidden_size'] = trial.suggest_categorical('hidden_size', [64, 96, 128, 160, 192, 224, 256])
        params['lstm_layers'] = trial.suggest_int('lstm_layers', 1, 3)
        params['num_heads'] = trial.suggest_categorical('num_heads', [2, 4, 8])
        params['dropout_rate'] = trial.suggest_float('dropout_rate', 0.05, 0.3, step=0.05)
        params['state_size'] = trial.suggest_categorical('state_size', [32, 48, 64, 80, 96])
        
        # –û–±—É—á–µ–Ω–∏–µ
        params['learning_rate'] = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)
        params['batch_size'] = trial.suggest_categorical('batch_size', [16, 24, 32, 48, 64])
        params['epochs'] = trial.suggest_int('epochs', 30, 100)
        
        # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
        params['l2_regularization'] = trial.suggest_float('l2_regularization', 0.001, 0.1, log=True)
        params['gradient_clip_val'] = trial.suggest_float('gradient_clip_val', 0.5, 2.0, step=0.1)
        
        # Early stopping
        params['early_stopping_patience'] = trial.suggest_int('early_stopping_patience', 8, 20)
        params['reduce_lr_patience'] = trial.suggest_int('reduce_lr_patience', 3, 8)
        params['reduce_lr_factor'] = trial.suggest_float('reduce_lr_factor', 0.3, 0.7, step=0.1)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if params['hidden_size'] % params['num_heads'] != 0:
            # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º hidden_size —á—Ç–æ–±—ã –±—ã–ª –∫—Ä–∞—Ç–µ–Ω num_heads
            params['hidden_size'] = (params['hidden_size'] // params['num_heads']) * params['num_heads']
        
        return params
    
    def _update_config_with_params(self, params: Dict) -> Config:
        """
        –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        """
        config = Config()
        
        # –ö–æ–ø–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        config.training = self.config.training
        config.database = self.config.database
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        for key, value in params.items():
            if hasattr(config.model, key):
                setattr(config.model, key, value)
        
        return config
    
    def get_optimization_history(self) -> Dict:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        """
        if self.study is None:
            return {}
        
        trials_df = self.study.trials_dataframe()
        
        return {
            'best_value': self.study.best_value,
            'best_params': self.study.best_params,
            'n_trials': len(self.study.trials),
            'trials_df': trials_df,
            'trial_results': self.trial_results
        }
    
    def plot_optimization_history(self, save_path: Optional[str] = None):
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –∏—Å—Ç–æ—Ä–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        """
        if self.study is None:
            logger.warning("‚ö†Ô∏è –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–µ –∑–∞–ø—É—â–µ–Ω–∞, –Ω–µ—á–µ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å")
            return
        
        try:
            import matplotlib.pyplot as plt
            
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('Optuna Optimization History', fontsize=16)
            
            # 1. Optimization history
            trials = self.study.trials
            values = [t.value for t in trials if t.value is not None]
            trial_numbers = [t.number for t in trials if t.value is not None]
            
            axes[0, 0].plot(trial_numbers, values, 'b-', alpha=0.7)
            axes[0, 0].axhline(y=self.study.best_value, color='r', linestyle='--', 
                              label=f'Best: {self.study.best_value:.4f}')
            axes[0, 0].set_title('Optimization History')
            axes[0, 0].set_xlabel('Trial')
            axes[0, 0].set_ylabel('Objective Value')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
            
            # 2. Parameter importance (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
            try:
                importance = optuna.importance.get_param_importances(self.study)
                if importance:
                    params = list(importance.keys())[:10]  # –¢–æ–ø-10
                    importances = [importance[p] for p in params]
                    
                    axes[0, 1].barh(params, importances)
                    axes[0, 1].set_title('Parameter Importance')
                    axes[0, 1].set_xlabel('Importance')
            except:
                axes[0, 1].text(0.5, 0.5, 'Parameter importance\nnot available', 
                               ha='center', va='center', transform=axes[0, 1].transAxes)
            
            # 3. Best trial parameters
            best_params = self.study.best_params
            if best_params:
                param_names = list(best_params.keys())[:8]  # –¢–æ–ø-8 –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                param_values = [best_params[p] for p in param_names]
                
                axes[1, 0].barh(param_names, param_values)
                axes[1, 0].set_title('Best Parameters')
                axes[1, 0].set_xlabel('Value')
            
            # 4. Convergence
            if len(values) > 1:
                best_so_far = []
                current_best = float('inf')
                for val in values:
                    if val < current_best:
                        current_best = val
                    best_so_far.append(current_best)
                
                axes[1, 1].plot(trial_numbers, best_so_far, 'g-', linewidth=2)
                axes[1, 1].set_title('Convergence')
                axes[1, 1].set_xlabel('Trial')
                axes[1, 1].set_ylabel('Best Value So Far')
                axes[1, 1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                logger.info(f"   ‚úÖ –ì—Ä–∞—Ñ–∏–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")
            
            plt.close()
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {e}")
    
    def suggest_best_config(self) -> Config:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å –ª—É—á—à–∏–º–∏ –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        """
        if self.best_params is None:
            logger.warning("‚ö†Ô∏è –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–µ –ø—Ä–æ–≤–æ–¥–∏–ª–∞—Å—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –∏—Å—Ö–æ–¥–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è")
            return self.config
        
        return self._update_config_with_params(self.best_params)