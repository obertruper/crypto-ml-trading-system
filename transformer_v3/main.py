#!/usr/bin/env python3
"""
–ì–ª–∞–≤–Ω—ã–π pipeline –¥–ª—è Transformer v3
–ü–æ–ª–Ω–æ—Å—Ç—å—é –º–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–æ –ø—Ä–∏–º–µ—Ä—É xgboost_v3
"""

import argparse
import logging
import time
import warnings
import numpy as np
import tensorflow as tf
from pathlib import Path

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
warnings.filterwarnings('ignore')
np.random.seed(42)
tf.random.set_seed(42)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω–æ {len(gpus)} GPU —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –≤—ã–¥–µ–ª–µ–Ω–∏–µ–º –ø–∞–º—è—Ç–∏")
    except RuntimeError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ GPU: {e}")

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ –º–æ–¥—É–ª–µ–π
from config import Config
from data import DataLoader, DataPreprocessor, SequenceCreator, CacheManager
from models import TFTTrainer, TFTEnsemble
from utils import LoggingManager, ReportGenerator

logger = logging.getLogger(__name__)


def parse_args():
    """–ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
    parser = argparse.ArgumentParser(description="Transformer v3.0 Training")
    
    parser.add_argument('--config', type=str, default=None,
                       help='–ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É')
    
    parser.add_argument('--task', type=str, default='regression',
                       choices=['regression', 'classification_binary'],
                       help='–¢–∏–ø –∑–∞–¥–∞—á–∏')
    
    parser.add_argument('--test-mode', action='store_true',
                       help='–†–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–±—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ)')
    
    parser.add_argument('--test-symbols', nargs='+', default=['BTCUSDT', 'ETHUSDT'],
                       help='–°–∏–º–≤–æ–ª—ã –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞')
    
    parser.add_argument('--no-cache', action='store_true',
                       help='–ù–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–µ—à')
    
    parser.add_argument('--ensemble-size', type=int, default=3,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –≤ –∞–Ω—Å–∞–º–±–ª–µ')
    
    parser.add_argument('--epochs', type=int, default=None,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è')
    
    parser.add_argument('--batch-size', type=int, default=None,
                       help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞')
    
    return parser.parse_args()


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    # –ü–∞—Ä—Å–∏–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã
    args = parse_args()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    if args.config:
        config = Config.from_yaml(args.config)
    else:
        config = Config()
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    config.training.task_type = args.task
    
    if args.test_mode:
        logger.info("üß™ –ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º")
        config.training.epochs = 20           # –£–≤–µ–ª–∏—á–∏–ª–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        config.training.batch_size = 64       # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π batch_size –∏–∑ config
        config.model.sequence_length = 100    # –ü–æ–ª–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        config.training.test_mode = True      # –§–ª–∞–≥ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞
        
    if args.epochs:
        config.training.epochs = args.epochs
        
    if args.batch_size:
        config.training.batch_size = args.batch_size
    
    # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config.validate()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    log_dir = config.get_log_dir()
    logging_manager = LoggingManager(log_dir)
    logging_manager.setup_logging()
    
    logger.info("""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë      Transformer v3.0 - ML Trading       ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    logger.info(config)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config.save(log_dir / "config.yaml")
    
    try:
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        logger.info("\n" + "="*60)
        logger.info("üì• –®–ê–ì 1: –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•")
        logger.info("="*60)
        
        cacher = CacheManager(config)
        
        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –∫–µ—à–∞
        df = None
        if not args.no_cache:
            df = cacher.load_processed_data('raw')
            if df is not None:
                logger.info("üìÇ –ò—Å–ø–æ–ª—å–∑—É—é –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ")
        
        if df is None:
            with DataLoader(config) as data_loader:
                if args.test_mode:
                    df = data_loader.load_data(symbols=args.test_symbols, limit=10000)
                else:
                    df = data_loader.load_data()
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
                quality_report = data_loader.check_data_quality(df)
                logger.info(f"üìä –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö: {quality_report['total_records']:,} –∑–∞–ø–∏—Å–µ–π")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
            if not args.test_mode:
                cacher.save_processed_data(df, 'raw')
        
        # 2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        logger.info("\n" + "="*60)
        logger.info("üîß –®–ê–ì 2: –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•")
        logger.info("="*60)
        
        preprocessor = DataPreprocessor(config)
        
        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –∫–µ—à–∞
        features_df = None
        if not args.no_cache:
            features_df = cacher.load_processed_data('features')
            if features_df is not None:
                logger.info("üìÇ –ò—Å–ø–æ–ª—å–∑—É—é –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")
                # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º feature_columns
                preprocessor.feature_columns = features_df.columns.drop(['symbol', 'timestamp', 'datetime', 'buy_expected_return', 'sell_expected_return']).tolist()
        
        if features_df is None:
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            features_df = preprocessor.extract_features(df)
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
            if not args.test_mode:
                cacher.save_processed_data(features_df, 'features')
        
        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–µ—à–∞
        normalized_data = None
        if not args.no_cache:
            normalized_data = cacher.load_data('normalized_splits')
            if normalized_data is not None:
                logger.info("üìÇ –ò—Å–ø–æ–ª—å–∑—É—é –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
                train_norm = normalized_data['train']
                val_norm = normalized_data['val'] 
                test_norm = normalized_data['test']
                preprocessor.scaler = normalized_data['scaler']
                preprocessor.feature_columns = normalized_data['feature_columns']
        
        if normalized_data is None:
            # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
            data_splits = preprocessor.split_data_temporal(features_df)
            
            # 3. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            logger.info("\n" + "="*60)
            logger.info("üìè –®–ê–ì 3: –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –î–ê–ù–ù–´–•")
            logger.info("="*60)
            
            train_norm, val_norm, test_norm = preprocessor.normalize_features(
                data_splits['train'],
                data_splits['val'],
                data_splits['test']
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
            if not args.test_mode:
                normalized_cache = {
                    'train': train_norm,
                    'val': val_norm,
                    'test': test_norm,
                    'scaler': preprocessor.scaler,
                    'feature_columns': preprocessor.feature_columns
                }
                cacher.save_data(normalized_cache, 'normalized_splits')
        
        # 4. –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        logger.info("\n" + "="*60)
        logger.info("üîÑ –®–ê–ì 4: –°–û–ó–î–ê–ù–ò–ï –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û–°–¢–ï–ô")
        logger.info("="*60)
        
        sequence_creator = SequenceCreator(config)
        
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        models = {}
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏ –¥–ª—è buy –∏ sell
        for direction in ['buy', 'sell']:
            logger.info(f"\n{'='*60}")
            logger.info(f"üéØ –û–ë–†–ê–ë–û–¢–ö–ê –ù–ê–ü–†–ê–í–õ–ï–ù–ò–Ø: {direction.upper()}")
            logger.info(f"{'='*60}")
            
            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ –∫–µ—à–∞
            sequences = None
            if not args.no_cache:
                sequences = cacher.load_sequences(direction, config.training.task_type)
                if sequences is not None:
                    logger.info("üìÇ –ò—Å–ø–æ–ª—å–∑—É—é –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ NaN
                    for split_name, split_data in sequences.items():
                        if 'X' in split_data and 'y' in split_data:
                            X_nan = np.isnan(split_data['X']).sum()
                            y_nan = np.isnan(split_data['y']).sum()
                            if X_nan > 0 or y_nan > 0:
                                logger.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ NaN –≤ {split_name}: X={X_nan}, y={y_nan}")
                                sequences = None  # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –µ—Å–ª–∏ –µ—Å—Ç—å NaN
                                break
                            
                            # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                            logger.info(f"üìä {split_name} - X: {split_data['X'].shape}, "
                                       f"y: mean={np.mean(split_data['y']):.4f}, "
                                       f"std={np.std(split_data['y']):.4f}")
            
            if sequences is None:
                # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                logger.info("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π...")
                sequences = sequence_creator.create_sequences_for_splits(
                    train_norm, val_norm, test_norm,
                    feature_columns=preprocessor.feature_columns,
                    target_type=direction
                )
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
                if not args.test_mode:
                    cacher.save_sequences(sequences, direction, config.training.task_type)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –º–µ—Ç–∫–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if config.training.task_type == 'classification_binary':
                for split in ['train', 'val', 'test']:
                    if split in sequences:
                        sequences[split]['y'] = preprocessor.convert_to_binary_labels(
                            sequences[split]['y']
                        )
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
            stats = sequence_creator.get_sequence_statistics(
                sequences['train']['X'],
                sequences['train']['y']
            )
            logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {stats}")
            
            # 5. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
            logger.info("\n" + "="*60)
            logger.info(f"üöÄ –®–ê–ì 5: –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô ({direction})")
            logger.info("="*60)
            
            # –°–æ–∑–¥–∞–µ–º –∞–Ω—Å–∞–º–±–ª—å
            ensemble = TFTEnsemble(config, base_name=f"tft_{direction}")
            
            # –û–±—É—á–∞–µ–º –∞–Ω—Å–∞–º–±–ª—å
            ensemble_models = ensemble.train_ensemble(
                sequences['train']['X'],
                sequences['train']['y'],
                sequences['val']['X'],
                sequences['val']['y'],
                n_models=args.ensemble_size,
                feature_columns=preprocessor.feature_columns
            )
            
            # 6. –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            logger.info("\n" + "="*60)
            logger.info(f"üìä –®–ê–ì 6: –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–• ({direction})")
            logger.info("="*60)
            
            test_metrics = ensemble.evaluate(
                sequences['test']['X'],
                sequences['test']['y'],
                f"Test ({direction})"
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            models[direction] = {
                'ensemble': ensemble,
                'test_metrics': test_metrics,
                'sequences_stats': stats
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
            if config.training.save_models:
                model_dir = log_dir / 'models' / direction
                ensemble.save_ensemble(model_dir)
        
        # 7. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
        logger.info("\n" + "="*60)
        logger.info("üìù –®–ê–ì 7: –ì–ï–ù–ï–†–ê–¶–ò–Ø –û–¢–ß–ï–¢–ê")
        logger.info("="*60)
        
        report_generator = ReportGenerator(config, log_dir)
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç—á–µ—Ç–∞
        results = {
            'models': models,
            'data_info': {
                'total_records': len(df),
                'n_symbols': df['symbol'].nunique(),
                'n_features': len(preprocessor.feature_columns),
                'train_size': len(sequences['train']['X']) if 'train' in sequences else 0,
                'val_size': len(sequences['val']['X']) if 'val' in sequences else 0,
                'test_size': len(sequences['test']['X']) if 'test' in sequences else 0
            }
        }
        
        report_generator.generate_report(results)
        
        logger.info("\n" + "="*60)
        logger.info("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
        logger.info(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {log_dir}")
        logger.info("="*60)
        
    except Exception as e:
        logger.error(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}", exc_info=True)
        raise


if __name__ == "__main__":
    main()