#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ
–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –ª–æ–∫–∞–ª—å–Ω—ã–π –∫–µ—à –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π —Ä–∞–±–æ—Ç—ã
"""

import argparse
import logging
import time
import warnings
import pandas as pd
import numpy as np
from pathlib import Path

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
warnings.filterwarnings('ignore')

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ –º–æ–¥—É–ª–µ–π
from config import Config, EXCLUDE_SYMBOLS
from data import DataLoader, DataPreprocessor, SequenceCreator, CacheManager
from utils import LoggingManager

logger = logging.getLogger(__name__)


def parse_args():
    """–ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
    parser = argparse.ArgumentParser(description="Cache All Data for Transformer v3")
    
    parser.add_argument('--all-symbols', action='store_true',
                       help='–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ –≤—Å–µ–º —Å–∏–º–≤–æ–ª–∞–º')
    
    parser.add_argument('--test-symbols', nargs='+', default=['BTCUSDT', 'ETHUSDT'],
                       help='–°–∏–º–≤–æ–ª—ã –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞')
    
    parser.add_argument('--force-refresh', action='store_true',
                       help='–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å –∫–µ—à')
    
    parser.add_argument('--skip-sequences', action='store_true',
                       help='–ù–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (—Ç–æ–ª—å–∫–æ —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ)')
    
    return parser.parse_args()


def cache_raw_data_by_symbols(data_loader: DataLoader, cacher: CacheManager, 
                             all_symbols: bool = False, test_symbols: list = None,
                             incremental: bool = True) -> pd.DataFrame:
    """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å–∏–º–≤–æ–ª–∞–º —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º –∏ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º"""
    logger.info("üì• –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL...")
    
    start_time = time.time()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫—ç—à –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
    existing_df = None
    existing_symbols = set()
    existing_timestamps = {}
    
    if incremental:
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫—ç—à
        existing_df = cacher.load_processed_data('raw')
        if existing_df is not None:
            logger.info(f"üìÇ –ù–∞–π–¥–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫—ç—à —Å {len(existing_df):,} –∑–∞–ø–∏—Å—è–º–∏")
        else:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π –∫—ç—à
            existing_df = cacher.load_processed_data('raw_temp')
            if existing_df is not None:
                logger.info(f"üìÇ –ù–∞–π–¥–µ–Ω –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π –∫—ç—à —Å {len(existing_df):,} –∑–∞–ø–∏—Å—è–º–∏")
                logger.info("üîÑ –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å –º–µ—Å—Ç–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏...")
                
        if existing_df is not None:
            existing_symbols = set(existing_df['symbol'].unique())
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ timestamp –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
            for symbol in existing_symbols:
                symbol_data = existing_df[existing_df['symbol'] == symbol]
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º numpy.int64 –≤ –æ–±—ã—á–Ω—ã–π Python int
                existing_timestamps[symbol] = int(symbol_data['timestamp'].max())
            logger.info(f"üìä –í –∫—ç—à–µ {len(existing_symbols)} —Å–∏–º–≤–æ–ª–æ–≤: {', '.join(sorted(existing_symbols))}")
    
    if all_symbols:
        logger.info("üåç –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ –í–°–ï–ú –¥–æ—Å—Ç—É–ø–Ω—ã–º —Å–∏–º–≤–æ–ª–∞–º")
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤
        try:
            symbols_to_load = data_loader.load_symbols_list()
            logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(symbols_to_load)} –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤")
            
            if incremental and existing_symbols:
                new_symbols = set(symbols_to_load) - existing_symbols
                if new_symbols:
                    logger.info(f"üÜï –ù–∞–π–¥–µ–Ω–æ {len(new_symbols)} –Ω–æ–≤—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤: {', '.join(list(new_symbols)[:5])}...")
                else:
                    logger.info("‚úÖ –í—Å–µ —Å–∏–º–≤–æ–ª—ã —É–∂–µ –µ—Å—Ç—å –≤ –∫—ç—à–µ")
                    
            logger.info(f"üìã –°–∏–º–≤–æ–ª—ã: {', '.join(symbols_to_load[:10])}{'...' if len(symbols_to_load) > 10 else ''}")
        except Exception as e:
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤: {e}")
            raise
    else:
        logger.info(f"üß™ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ —Ç–µ—Å—Ç–æ–≤—ã–º —Å–∏–º–≤–æ–ª–∞–º: {test_symbols}")
        symbols_to_load = test_symbols
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Å–∏–º–≤–æ–ª–∞–º —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º
    all_dataframes = []
    if existing_df is not None and incremental:
        all_dataframes.append(existing_df)  # –î–æ–±–∞–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        
    total_records = 0
    updated_symbols = 0
    new_records = 0
    
    for i, symbol in enumerate(symbols_to_load):
        symbol_start = time.time()
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        if incremental and symbol in existing_symbols and symbol not in existing_timestamps:
            logger.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫ {i+1}/{len(symbols_to_load)}: {symbol} (—É–∂–µ –≤ –∫—ç—à–µ)")
            continue
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–ª—è —ç—Ç–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
        if incremental and symbol in existing_timestamps:
            last_timestamp = existing_timestamps[symbol]
            logger.info(f"‚è≥ –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π {i+1}/{len(symbols_to_load)}: {symbol} (–ø–æ—Å–ª–µ {pd.to_datetime(last_timestamp, unit='ms')})")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            symbol_df = data_loader.load_symbol_updates(symbol, after_timestamp=last_timestamp)
            
            if len(symbol_df) > 0:
                # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ —ç—Ç–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –∏–∑ existing_df
                if existing_df is not None:
                    mask = existing_df['symbol'] != symbol
                    all_dataframes[0] = existing_df[mask]
                    
                # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª–∞ (—Å—Ç–∞—Ä—ã–µ + –Ω–æ–≤—ã–µ)
                full_symbol_df = data_loader.load_symbol_data(symbol)
                all_dataframes.append(full_symbol_df)
                
                new_records += len(symbol_df)
                updated_symbols += 1
                logger.info(f"üîÑ {symbol}: +{len(symbol_df):,} –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π (–≤—Å–µ–≥–æ {len(full_symbol_df):,})")
            else:
                logger.info(f"‚úÖ {symbol}: –¥–∞–Ω–Ω—ã–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã")
        else:
            logger.info(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ {i+1}/{len(symbols_to_load)}: {symbol}")
            
            try:
                symbol_df = data_loader.load_symbol_data(symbol)
                symbol_time = time.time() - symbol_start
                
                if len(symbol_df) > 0:
                    all_dataframes.append(symbol_df)
                    total_records += len(symbol_df)
                    logger.info(f"‚úÖ {symbol}: {len(symbol_df):,} –∑–∞–ø–∏—Å–µ–π –∑–∞ {symbol_time:.2f} —Å–µ–∫")
                else:
                    logger.warning(f"‚ö†Ô∏è {symbol}: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
                    
            except Exception as e:
                logger.error(f"‚ùå {symbol}: –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ - {e}")
                continue
            
        # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞–∂–¥—ã–µ 10 —Å–∏–º–≤–æ–ª–æ–≤
        if (i + 1) % 10 == 0:
            elapsed = time.time() - start_time
            avg_time = elapsed / (i + 1)
            remaining = (len(symbols_to_load) - i - 1) * avg_time
            logger.info(f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {i+1}/{len(symbols_to_load)} ({(i+1)/len(symbols_to_load)*100:.1f}%), "
                       f"–∑–∞–ø–∏—Å–µ–π: {total_records:,}, –æ—Å—Ç–∞–ª–æ—Å—å: ~{remaining/60:.1f} –º–∏–Ω")
            
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∂–¥—ã–µ 5 —Å–∏–º–≤–æ–ª–æ–≤
        if (i + 1) % 5 == 0 and len(all_dataframes) > 0:
            logger.info("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
            temp_df = pd.concat(all_dataframes, ignore_index=True)
            temp_df = temp_df.sort_values(['symbol', 'timestamp']).reset_index(drop=True)
            cacher.save_processed_data(temp_df, 'raw_temp')
            logger.info(f"‚úÖ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π –∫—ç—à —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {len(temp_df):,} –∑–∞–ø–∏—Å–µ–π")
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
    if not all_dataframes:
        raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è!")
        
    logger.info("üìã –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ—Ç –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤...")
    df = pd.concat(all_dataframes, ignore_index=True)
    df = df.sort_values(['symbol', 'timestamp']).reset_index(drop=True)
    
    query_time = time.time() - start_time
    logger.info(f"üìä –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {query_time:.2f} —Å–µ–∫")
    logger.info(f"üìä –í—Å–µ–≥–æ –≤ –∫—ç—à–µ: {len(df):,} –∑–∞–ø–∏—Å–µ–π –ø–æ {df['symbol'].nunique()} —Å–∏–º–≤–æ–ª–∞–º")
    
    if incremental and (updated_symbols > 0 or new_records > 0):
        logger.info(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–æ —Å–∏–º–≤–æ–ª–æ–≤: {updated_symbols}")
        logger.info(f"üÜï –ù–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π: {new_records:,}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞–Ω–Ω—ã–º
    date_range = df['datetime'].max() - df['datetime'].min()
    logger.info(f"üìÖ –ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö: {df['datetime'].min():%Y-%m-%d} - {df['datetime'].max():%Y-%m-%d} ({date_range.days} –¥–Ω–µ–π)")
    
    # –¢–æ–ø —Å–∏–º–≤–æ–ª–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∑–∞–ø–∏—Å–µ–π
    top_symbols = df['symbol'].value_counts().head(5)
    logger.info("üîù –¢–æ–ø-5 —Å–∏–º–≤–æ–ª–æ–≤ –ø–æ –∑–∞–ø–∏—Å—è–º:")
    for symbol, count in top_symbols.items():
        logger.info(f"   {symbol}: {count:,} –∑–∞–ø–∏—Å–µ–π")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
    logger.info("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫–µ—à...")
    cache_start = time.time()
    
    cacher.save_processed_data(df, 'raw')
    
    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∫—ç—à –µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    temp_cache_path = cacher.cache_dir / "data_raw_temp.parquet"
    if temp_cache_path.exists():
        temp_cache_path.unlink()
        logger.info("üóëÔ∏è –í—Ä–µ–º–µ–Ω–Ω—ã–π –∫—ç—à —É–¥–∞–ª–µ–Ω")
    
    cache_time = time.time() - cache_start
    total_time = time.time() - start_time
    
    logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫–µ—à: {cache_time:.2f} —Å–µ–∫")
    logger.info(f"‚úÖ –û–±—â–µ–µ –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏: {total_time:.2f} —Å–µ–∫")
    logger.info(f"üìà –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(df)/total_time:,.0f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫")
    
    return df


def cache_raw_data(data_loader: DataLoader, cacher: CacheManager, 
                   all_symbols: bool = False, test_symbols: list = None,
                   force_refresh: bool = False) -> pd.DataFrame:
    """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ë–î (—Å –≤—ã–±–æ—Ä–æ–º –º–µ—Ç–æ–¥–∞)"""
    
    if all_symbols:
        # –î–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤ –í–°–ï–ì–î–ê –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É –ø–æ —á–∞—Å—Ç—è–º
        return cache_raw_data_by_symbols(data_loader, cacher, all_symbols, test_symbols, 
                                       incremental=not force_refresh)
    else:
        # –î–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ - –æ–±—ã—á–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
        logger.info("üì• –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL...")
        start_time = time.time()
        
        logger.info(f"üß™ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ —Ç–µ—Å—Ç–æ–≤—ã–º —Å–∏–º–≤–æ–ª–∞–º: {test_symbols}")
        
        logger.info("‚è≥ –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∑–∞–ø—Ä–æ—Å –∫ PostgreSQL...")
        df = data_loader.load_data(symbols=test_symbols)
        
        logger.info("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫–µ—à...")
        cacher.save_processed_data(df, 'raw')
        
        total_time = time.time() - start_time
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df):,} –∑–∞–ø–∏—Å–µ–π –∑–∞ {total_time:.2f} —Å–µ–∫")
        
        return df


def cache_processed_features(df: pd.DataFrame, preprocessor: DataPreprocessor, 
                           cacher: CacheManager) -> pd.DataFrame:
    """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    logger.info("üîß –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
    
    start_time = time.time()
    logger.info(f"üìä –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(df):,} –∑–∞–ø–∏—Å–µ–π, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
    logger.info("‚è≥ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤...")
    extraction_start = time.time()
    
    features_df = preprocessor.extract_features(df)
    
    extraction_time = time.time() - extraction_start
    logger.info(f"üìä –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {extraction_time:.2f} —Å–µ–∫")
    logger.info(f"üìä –ü–æ–ª—É—á–µ–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(preprocessor.feature_columns)} –∫–æ–ª–æ–Ω–æ–∫")
    logger.info(f"üìä –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {len(features_df):,} –∑–∞–ø–∏—Å–µ–π")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
    logger.info("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –∫–µ—à...")
    cache_start = time.time()
    
    cacher.save_processed_data(features_df, 'features')
    
    cache_time = time.time() - cache_start
    total_time = time.time() - start_time
    
    logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫–µ—à: {cache_time:.2f} —Å–µ–∫")
    logger.info(f"‚úÖ –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_time:.2f} —Å–µ–∫")
    logger.info(f"üìà –°–∫–æ—Ä–æ—Å—Ç—å: {len(features_df)/total_time:,.0f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫")
    
    return features_df


def cache_normalized_data(features_df: pd.DataFrame, preprocessor: DataPreprocessor,
                         cacher: CacheManager) -> tuple:
    """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    logger.info("üìè –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    start_time = time.time()
    logger.info(f"üìä –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(features_df):,} –∑–∞–ø–∏—Å–µ–π")
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
    logger.info("‚è≥ –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
    split_start = time.time()
    
    data_splits = preprocessor.split_data_temporal(features_df)
    train_df = data_splits['train']
    val_df = data_splits['val']
    test_df = data_splits['test']
    
    split_time = time.time() - split_start
    logger.info(f"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö: {split_time:.2f} —Å–µ–∫")
    logger.info(f"üìä –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(train_df):,} –∑–∞–ø–∏—Å–µ–π ({len(train_df)/len(features_df)*100:.1f}%)")
    logger.info(f"üìä –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(val_df):,} –∑–∞–ø–∏—Å–µ–π ({len(val_df)/len(features_df)*100:.1f}%)")
    logger.info(f"üìä –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(test_df):,} –∑–∞–ø–∏—Å–µ–π ({len(test_df)/len(features_df)*100:.1f}%)")
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
    logger.info("‚è≥ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
    norm_start = time.time()
    
    train_df_norm, val_df_norm, test_df_norm = preprocessor.normalize_features(
        train_df, val_df, test_df
    )
    
    norm_time = time.time() - norm_start
    logger.info(f"üìä –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: {norm_time:.2f} —Å–µ–∫")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    logger.info("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –∫–µ—à...")
    cache_start = time.time()
    
    normalized_data = {
        'train': train_df_norm,
        'val': val_df_norm, 
        'test': test_df_norm,
        'scaler': preprocessor.scaler,
        'feature_columns': preprocessor.feature_columns
    }
    
    cacher.save_data(normalized_data, 'normalized_splits')
    
    cache_time = time.time() - cache_start
    total_time = time.time() - start_time
    
    logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫–µ—à: {cache_time:.2f} —Å–µ–∫")
    logger.info(f"‚úÖ –û–±—â–µ–µ –≤—Ä–µ–º—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {total_time:.2f} —Å–µ–∫")
    
    return train_df_norm, val_df_norm, test_df_norm, preprocessor.scaler


def cache_sequences(train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame,
                   feature_columns: list, sequence_creator: SequenceCreator, 
                   cacher: CacheManager):
    """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –æ–±–µ–∏—Ö –∑–∞–¥–∞—á"""
    logger.info("üîÑ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π...")
    
    start_time = time.time()
    logger.info(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π:")
    logger.info(f"   –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {sequence_creator.config.model.sequence_length}")
    logger.info(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_columns)}")
    
    # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è BUY
    logger.info("üìà –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è BUY...")
    buy_start = time.time()
    
    buy_sequences = sequence_creator.create_sequences_for_splits(
        train_df, val_df, test_df, feature_columns, target_type='buy'
    )
    
    buy_time = time.time() - buy_start
    logger.info(f"üìä BUY –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ–∑–¥–∞–Ω—ã –∑–∞ {buy_time:.2f} —Å–µ–∫")
    logger.info(f"   Train: {len(buy_sequences['train']['X'])} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
    logger.info(f"   Val: {len(buy_sequences['val']['X'])} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π") 
    logger.info(f"   Test: {len(buy_sequences['test']['X'])} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
    
    cacher.save_sequences(buy_sequences, 'buy', 'regression')
    
    # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è SELL
    logger.info("üìâ –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è SELL...")
    sell_start = time.time()
    
    sell_sequences = sequence_creator.create_sequences_for_splits(
        train_df, val_df, test_df, feature_columns, target_type='sell'
    )
    
    sell_time = time.time() - sell_start
    logger.info(f"üìä SELL –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ–∑–¥–∞–Ω—ã –∑–∞ {sell_time:.2f} —Å–µ–∫")
    logger.info(f"   Train: {len(sell_sequences['train']['X'])} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
    logger.info(f"   Val: {len(sell_sequences['val']['X'])} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
    logger.info(f"   Test: {len(sell_sequences['test']['X'])} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
    
    cacher.save_sequences(sell_sequences, 'sell', 'regression')
    
    total_time = time.time() - start_time
    total_sequences = (len(buy_sequences['train']['X']) + len(buy_sequences['val']['X']) + 
                      len(buy_sequences['test']['X']) + len(sell_sequences['train']['X']) + 
                      len(sell_sequences['val']['X']) + len(sell_sequences['test']['X']))
    
    logger.info(f"‚úÖ –í—Å–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω—ã –∑–∞ {total_time:.2f} —Å–µ–∫")
    logger.info(f"üìà –°–æ–∑–¥–∞–Ω–æ {total_sequences:,} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
    logger.info(f"üìà –°–∫–æ—Ä–æ—Å—Ç—å: {total_sequences/total_time:,.0f} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π/—Å–µ–∫")


def cache_database_metadata(data_loader: DataLoader, cacher: CacheManager):
    """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –ë–î"""
    logger.info("üìä –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –ë–î...")
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤
        symbols_list = data_loader.load_symbols_list()
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            'available_symbols': symbols_list,
            'excluded_symbols': EXCLUDE_SYMBOLS,
            'total_symbols': len(symbols_list),
            'cache_timestamp': pd.Timestamp.now().isoformat()
        }
        
        cacher.save_data(metadata, 'database_metadata')
        
        logger.info(f"‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω—ã: {len(symbols_list)} —Å–∏–º–≤–æ–ª–æ–≤")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
    args = parse_args()
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    config = Config()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    log_dir = Path("logs/cache_data")
    log_dir.mkdir(parents=True, exist_ok=True)
    logging_manager = LoggingManager(log_dir)
    logging_manager.setup_logging()
    
    logger.info("""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë      Data Caching for Transformer v3     ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    cacher = CacheManager(config)
    preprocessor = DataPreprocessor(config)
    sequence_creator = SequenceCreator(config)
    
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∫–µ—à–∞
        if not args.force_refresh:
            existing_raw = cacher.load_processed_data('raw')
            if existing_raw is not None:
                logger.info("üìÇ –ù–∞–π–¥–µ–Ω—ã –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ")
                if not args.all_symbols:
                    logger.info("‚úÖ –ö–µ—à –∞–∫—Ç—É–∞–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ")
                    df = existing_raw
                else:
                    logger.info("üîÑ –¢—Ä–µ–±—É–µ—Ç—Å—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤")
                    df = None
            else:
                df = None
        else:
            logger.info("üîÑ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–µ—à–∞")
            df = None
        
        # 1. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if df is None:
            with DataLoader(config) as data_loader:
                # –°–Ω–∞—á–∞–ª–∞ –∫—ç—à–∏—Ä—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                cache_database_metadata(data_loader, cacher)
                
                # –ó–∞—Ç–µ–º –¥–∞–Ω–Ω—ã–µ
                df = cache_raw_data(
                    data_loader, cacher, 
                    all_symbols=args.all_symbols,
                    test_symbols=args.test_symbols,
                    force_refresh=args.force_refresh
                )
        
        # 2. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        logger.info("\n" + "="*60)
        logger.info("üîß –≠–¢–ê–ü 2: –ö–≠–®–ò–†–û–í–ê–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í")
        logger.info("="*60)
        
        features_df = cache_processed_features(df, preprocessor, cacher)
        
        # 3. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        logger.info("\n" + "="*60)
        logger.info("üìè –≠–¢–ê–ü 3: –ö–≠–®–ò–†–û–í–ê–ù–ò–ï –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–ò")
        logger.info("="*60)
        
        train_df, val_df, test_df, scaler = cache_normalized_data(
            features_df, preprocessor, cacher
        )
        
        # 4. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        if not args.skip_sequences:
            logger.info("\n" + "="*60)
            logger.info("üîÑ –≠–¢–ê–ü 4: –ö–≠–®–ò–†–û–í–ê–ù–ò–ï –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û–°–¢–ï–ô")
            logger.info("="*60)
            
            cache_sequences(
                train_df, val_df, test_df, 
                preprocessor.feature_columns,
                sequence_creator, cacher
            )
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–µ—à–µ
        logger.info("\n" + "="*60)
        logger.info("üìä –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ö–ï–®–ï")
        logger.info("="*60)
        
        cache_info = cacher.get_cache_info()
        logger.info(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫–µ—à–∞: {cache_info['cache_dir']}")
        logger.info(f"üìÑ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤: {cache_info['n_files']}")
        logger.info(f"üíæ –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {cache_info['total_size_mb']:.1f} MB")
        logger.info("üìã –§–∞–π–ª—ã –∫–µ—à–∞:")
        for file_name in sorted(cache_info['files']):
            logger.info(f"   - {file_name}")
        
        logger.info("\nüéâ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        logger.info("üöÄ –¢–µ–ø–µ—Ä—å –æ–±—É—á–µ–Ω–∏–µ –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –∞–≤—Ç–æ–Ω–æ–º–Ω–æ (–±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞)")
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª-—Ñ–ª–∞–≥ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫–µ—à–∞
        cache_ready_file = cacher.cache_dir / "CACHE_READY"
        with open(cache_ready_file, 'w') as f:
            f.write(f"Cache created: {pd.Timestamp.now().isoformat()}\n")
            f.write(f"Symbols: {'ALL' if args.all_symbols else str(args.test_symbols)}\n")
            f.write(f"Total size: {cache_info['total_size_mb']:.1f} MB\n")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        raise


if __name__ == "__main__":
    main()