# üöÄ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è GPU –ø–∞–º—è—Ç–∏ –¥–ª—è Enhanced TFT v2.1

## ‚úÖ –í–Ω–µ—Å–µ–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è

### 1. **–£–º–µ–Ω—å—à–µ–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏** (—Å—Ç—Ä–æ–∫–∏ 761-770):
- `batch_size`: 16 ‚Üí 8
- `gradient_accumulation_steps`: 8 ‚Üí 4 (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch = 32 –≤–º–µ—Å—Ç–æ 128)
- `d_model`: 256 ‚Üí 128
- `num_heads`: 8 (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
- `num_transformer_blocks`: 6 ‚Üí 4
- `ff_dim`: 512 ‚Üí 256
- `dropout_rate`: 0.3 (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)

### 2. **–í–∫–ª—é—á–µ–Ω Mixed Precision Training** (—Å—Ç—Ä–æ–∫–∏ 82-86):
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ float16 –≤–º–µ—Å—Ç–æ float32 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ loss –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
- –§–∏–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–∏ –æ—Å—Ç–∞—é—Ç—Å—è –≤ float32 –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏

### 3. **–î–æ–±–∞–≤–ª–µ–Ω MemoryCleanupCallback** (—Å—Ç—Ä–æ–∫–∏ 91-108):
- –û—á–∏—Å—Ç–∫–∞ TensorFlow —Å–µ—Å—Å–∏–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏
- –°–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞ Python (gc.collect())
- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU –ø–∞–º—è—Ç–∏

### 4. **–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä** (—Å—Ç—Ä–æ–∫–∏ 896-898):
- –î–æ–±–∞–≤–ª–µ–Ω LossScaleOptimizer –¥–ª—è mixed precision
- –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã Adam

## üìä –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏:
- **–î–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏**: ~22 GB (OOM –Ω–∞ 24GB GPU)
- **–ü–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏**: ~8-10 GB (–∫–æ–º—Ñ–æ—Ä—Ç–Ω–æ –¥–ª—è RTX 4090)

### –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:
- –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –º–æ–∂–µ—Ç —É–≤–µ–ª–∏—á–∏—Ç—å—Å—è –Ω–∞ 20-30% –±–ª–∞–≥–æ–¥–∞—Ä—è float16
- –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –¥–æ–ª–∂–Ω–æ –æ—Å—Ç–∞—Ç—å—Å—è –Ω–∞ —Ç–æ–º –∂–µ —É—Ä–æ–≤–Ω–µ

## üîß –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

### 1. **–ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç OOM**:
```python
# –£–º–µ–Ω—å—à–∏—Ç—å sequence_length –≤ config.yaml
sequence_length: 30  # –≤–º–µ—Å—Ç–æ 60
```

### 2. **–î–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏**:
```python
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å tf.data.Dataset
dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
dataset = dataset.batch(batch_size)
dataset = dataset.prefetch(tf.data.AUTOTUNE)
dataset = dataset.cache()
```

### 3. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU**:
```bash
# –í –æ—Ç–¥–µ–ª—å–Ω–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ
watch -n 1 nvidia-smi
```

### 4. **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ –ø—Ä–∏ OOM**:
```python
# –û–±–µ—Ä–Ω—É—Ç—å –æ–±—É—á–µ–Ω–∏–µ –≤ try-except
try:
    history = model.fit(...)
except tf.errors.ResourceExhaustedError:
    # –û—á–∏—Å—Ç–∏—Ç—å –ø–∞–º—è—Ç—å
    tf.keras.backend.clear_session()
    gc.collect()
    # –£–º–µ–Ω—å—à–∏—Ç—å batch_size –∏ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å–Ω–æ–≤–∞
```

## üìà –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

1. **–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –≤ —Ç–µ—Å—Ç–æ–≤–æ–º —Ä–µ–∂–∏–º–µ**:
```bash
python train_universal_transformer_v2.py --test_mode --task classification_binary
```

2. **–ï—Å–ª–∏ —Ç–µ—Å—Ç –ø—Ä–æ—à–µ–ª —É—Å–ø–µ—à–Ω–æ, –∑–∞–ø—É—Å–∫–∞–π—Ç–µ –ø–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ**:
```bash
python train_universal_transformer_v2.py --task classification_binary --ensemble_size 1
```

3. **–ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ –ª–æ–≥–∏**:
```bash
tail -f logs/training_*/training.log
```

## üéØ –¶–µ–ª–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏

- GPU Memory Usage: < 20 GB
- Training Speed: > 100 samples/sec
- No OOM errors
- Model quality: —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç–µ–∫—É—â–∏–π —É—Ä–æ–≤–µ–Ω—å accuracy/MAE