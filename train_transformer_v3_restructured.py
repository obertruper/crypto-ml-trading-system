#!/usr/bin/env python3
"""
–†–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è Temporal Fusion Transformer v3.0
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ª—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –∏–∑ XGBoost v3:
- –£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ —Ç–∞—Ä–≥–µ—Ç—ã
- –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤  
- –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
- –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import logging
from datetime import datetime
import json
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ XGBoost –º–æ–¥—É–ª—è–º
sys.path.append('xgboost_v3')
from data.target_calculator import TargetCalculator
from utils.feature_selector import FeatureSelector

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ImprovedGatedLinearUnit(nn.Module):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π GLU —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π –∏ dropout"""
    def __init__(self, input_dim, output_dim, dropout_rate=0.1):
        super().__init__()
        self.fc = nn.Linear(input_dim, output_dim * 2)
        self.layer_norm = nn.LayerNorm(output_dim)
        self.dropout = nn.Dropout(dropout_rate)
        
    def forward(self, x):
        x = self.fc(x)
        x = F.glu(x, dim=-1)
        x = self.layer_norm(x)
        x = self.dropout(x)
        return x


class ImprovedGatedResidualNetwork(nn.Module):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π GRN —Å skip connections"""
    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.1, use_time_distributed=True):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.use_time_distributed = use_time_distributed
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Å–ª–æ–∏
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.elu = nn.ELU()
        
        # GLU –¥–ª—è –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.glu = ImprovedGatedLinearUnit(hidden_dim, output_dim, dropout_rate)
        
        # Skip connection –µ—Å–ª–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç
        self.skip_connection = nn.Linear(input_dim, output_dim) if input_dim != output_dim else None
        
        # Layer normalization
        self.layer_norm = nn.LayerNorm(output_dim)
        
    def forward(self, x):
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è skip connection
        residual = x
        
        # –û—Å–Ω–æ–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        x = self.fc1(x)
        x = self.elu(x)
        x = self.glu(x)
        
        # Skip connection
        if self.skip_connection is not None:
            residual = self.skip_connection(residual)
            
        x = x + residual
        x = self.layer_norm(x)
        
        return x


class ImprovedVariableSelectionNetwork(nn.Module):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–µ—Ç—å –≤—ã–±–æ—Ä–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –≤–Ω–∏–º–∞–Ω–∏—è"""
    def __init__(self, input_dim, num_inputs, hidden_dim, dropout_rate=0.1):
        super().__init__()
        self.num_inputs = num_inputs
        self.hidden_dim = hidden_dim
        
        # Flattening –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö –≤—Ö–æ–¥–æ–≤ –≤–º–µ—Å—Ç–µ
        self.flattened_grn = ImprovedGatedResidualNetwork(
            input_dim * num_inputs,
            hidden_dim,
            num_inputs,
            dropout_rate
        )
        
        # –û—Ç–¥–µ–ª—å–Ω—ã–µ GRN –¥–ª—è –∫–∞–∂–¥–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        self.variable_grns = nn.ModuleList([
            ImprovedGatedResidualNetwork(input_dim, hidden_dim, hidden_dim, dropout_rate)
            for _ in range(num_inputs)
        ])
        
        # Softmax –¥–ª—è –≤–µ—Å–æ–≤
        self.softmax = nn.Softmax(dim=-1)
        
    def forward(self, inputs):
        # inputs shape: (batch_size, num_inputs, input_dim)
        batch_size = inputs.size(0)
        
        # Flatten –∏ –ø–æ–ª—É—á–∞–µ–º –≤–µ—Å–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        flattened = inputs.view(batch_size, -1)
        variable_weights = self.softmax(self.flattened_grn(flattened))
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ—Ç–¥–µ–ª—å–Ω–æ
        processed_inputs = []
        for i in range(self.num_inputs):
            processed = self.variable_grns[i](inputs[:, i, :])
            processed_inputs.append(processed)
            
        processed_inputs = torch.stack(processed_inputs, dim=1)
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è
        weighted_inputs = processed_inputs * variable_weights.unsqueeze(-1)
        combined = weighted_inputs.sum(dim=1)
        
        return combined, variable_weights


class TemporalFusionTransformerV3(nn.Module):
    """
    –†–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Temporal Fusion Transformer
    —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏ –∏–∑ XGBoost v3
    """
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        self.input_dim = config['input_dim']
        self.hidden_dim = config['hidden_dim']
        self.num_heads = config['num_heads']
        self.dropout_rate = config['dropout_rate']
        self.output_dim = config['output_dim']
        
        # Variable Selection –¥–ª—è —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.static_vsn = ImprovedVariableSelectionNetwork(
            self.input_dim,
            1,  # –í—Å–µ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫–∞–∫ –æ–¥–∏–Ω –≤—Ö–æ–¥
            self.hidden_dim,
            self.dropout_rate
        )
        
        # LSTM Encoder –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.lstm_encoder = nn.LSTM(
            self.hidden_dim,
            self.hidden_dim,
            num_layers=2,
            dropout=self.dropout_rate,
            batch_first=True,
            bidirectional=True
        )
        
        # Multi-Head Attention –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        self.self_attention = nn.MultiheadAttention(
            self.hidden_dim * 2,  # bidirectional LSTM
            self.num_heads,
            dropout=self.dropout_rate,
            batch_first=True
        )
        
        # Decoder GRN
        self.decoder_grn = ImprovedGatedResidualNetwork(
            self.hidden_dim * 2,
            self.hidden_dim,
            self.hidden_dim,
            self.dropout_rate
        )
        
        # Output layers –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–¥–∞—á–∏
        if config['task'] == 'regression':
            self.output_layer = nn.Sequential(
                nn.Linear(self.hidden_dim, self.hidden_dim // 2),
                nn.ReLU(),
                nn.Dropout(self.dropout_rate),
                nn.Linear(self.hidden_dim // 2, self.output_dim)
            )
        else:  # classification
            self.output_layer = nn.Sequential(
                nn.Linear(self.hidden_dim, self.hidden_dim // 2),
                nn.ReLU(),
                nn.Dropout(self.dropout_rate),
                nn.Linear(self.hidden_dim // 2, self.output_dim),
                nn.Sigmoid() if self.output_dim == 1 else nn.Softmax(dim=-1)
            )
            
    def forward(self, x_static):
        batch_size = x_static.size(0)
        
        # 1. Variable Selection –¥–ª—è —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        selected_features, feature_weights = self.static_vsn(x_static.unsqueeze(1))
        
        # 2. –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è LSTM (–ø—Ä–æ—Å—Ç–æ–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ)
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        sequence_length = 10  # —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        lstm_input = selected_features.unsqueeze(1).repeat(1, sequence_length, 1)
        
        # 3. LSTM Encoder
        lstm_output, (hidden, cell) = self.lstm_encoder(lstm_input)
        
        # 4. Self-Attention
        attn_output, attn_weights = self.self_attention(
            lstm_output, lstm_output, lstm_output
        )
        
        # 5. Decoder GRN
        decoded = self.decoder_grn(attn_output)
        
        # 6. –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (—Å—Ä–µ–¥–Ω–µ–µ)
        aggregated = decoded.mean(dim=1)
        
        # 7. Output
        output = self.output_layer(aggregated)
        
        return output, {
            'feature_weights': feature_weights,
            'attention_weights': attn_weights
        }


class TransformerTrainer:
    """–¢—Ä–µ–Ω–µ—Ä –¥–ª—è Temporal Fusion Transformer —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π"""
    
    def __init__(self, model, config, log_dir):
        self.model = model
        self.config = config
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        self.optimizer = optim.AdamW(
            model.parameters(),
            lr=config['learning_rate'],
            weight_decay=config['weight_decay']
        )
        
        # Scheduler
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=5,
            verbose=True
        )
        
        # Loss functions
        if config['task'] == 'regression':
            self.criterion = nn.MSELoss()
        elif config['output_dim'] == 1:
            self.criterion = nn.BCELoss()
        else:
            self.criterion = nn.CrossEntropyLoss()
            
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
        # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'train_metrics': [],
            'val_metrics': [],
            'learning_rates': []
        }
        
    def train_epoch(self, train_loader):
        """–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏"""
        self.model.train()
        total_loss = 0
        predictions = []
        targets = []
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            # Forward pass
            self.optimizer.zero_grad()
            output, _ = self.model(data)
            
            # Loss
            if self.config['task'] == 'classification' and self.config['output_dim'] > 1:
                loss = self.criterion(output, target.long())
            else:
                loss = self.criterion(output.squeeze(), target)
                
            # Backward pass
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()
            
            total_loss += loss.item()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            predictions.extend(output.detach().cpu().numpy())
            targets.extend(target.detach().cpu().numpy())
            
        avg_loss = total_loss / len(train_loader)
        metrics = self._calculate_metrics(predictions, targets)
        
        return avg_loss, metrics
        
    def validate(self, val_loader):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
        self.model.eval()
        total_loss = 0
        predictions = []
        targets = []
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device), target.to(self.device)
                
                output, _ = self.model(data)
                
                if self.config['task'] == 'classification' and self.config['output_dim'] > 1:
                    loss = self.criterion(output, target.long())
                else:
                    loss = self.criterion(output.squeeze(), target)
                    
                total_loss += loss.item()
                
                predictions.extend(output.detach().cpu().numpy())
                targets.extend(target.detach().cpu().numpy())
                
        avg_loss = total_loss / len(val_loader)
        metrics = self._calculate_metrics(predictions, targets)
        
        return avg_loss, metrics
        
    def _calculate_metrics(self, predictions, targets):
        """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–¥–∞—á–∏"""
        predictions = np.array(predictions)
        targets = np.array(targets)
        
        if self.config['task'] == 'regression':
            return {
                'mae': mean_absolute_error(targets, predictions),
                'rmse': np.sqrt(mean_squared_error(targets, predictions)),
                'r2': r2_score(targets, predictions)
            }
        else:
            if self.config['output_dim'] == 1:
                # –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
                preds_binary = (predictions > 0.5).astype(int)
                return {
                    'accuracy': accuracy_score(targets, preds_binary),
                    'precision': precision_score(targets, preds_binary, zero_division=0),
                    'recall': recall_score(targets, preds_binary, zero_division=0),
                    'f1': f1_score(targets, preds_binary, zero_division=0),
                    'roc_auc': roc_auc_score(targets, predictions) if len(np.unique(targets)) > 1 else 0
                }
            else:
                # –ú—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
                preds_class = np.argmax(predictions, axis=1)
                return {
                    'accuracy': accuracy_score(targets, preds_class),
                    'macro_f1': f1_score(targets, preds_class, average='macro', zero_division=0)
                }
                
    def train(self, train_loader, val_loader, epochs):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è"""
        logger.info(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ {epochs} —ç–ø–æ—Ö")
        logger.info(f"üìä –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        best_val_loss = float('inf')
        patience_counter = 0
        
        for epoch in range(epochs):
            # –û–±—É—á–µ–Ω–∏–µ
            train_loss, train_metrics = self.train_epoch(train_loader)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            val_loss, val_metrics = self.validate(val_loader)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ scheduler
            self.scheduler.step(val_loss)
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
            self.history['train_loss'].append(train_loss)
            self.history['val_loss'].append(val_loss)
            self.history['train_metrics'].append(train_metrics)
            self.history['val_metrics'].append(val_metrics)
            self.history['learning_rates'].append(current_lr)
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            logger.info(f"\n–≠–ø–æ—Ö–∞ {epoch+1}/{epochs}")
            logger.info(f"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")
            logger.info(f"Learning Rate: {current_lr:.6f}")
            
            if self.config['task'] == 'regression':
                logger.info(f"Val MAE: {val_metrics['mae']:.4f} | "
                          f"Val RMSE: {val_metrics['rmse']:.4f} | "
                          f"Val R¬≤: {val_metrics['r2']:.4f}")
            else:
                logger.info(f"Val Accuracy: {val_metrics['accuracy']:.4f}")
                
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                self._save_checkpoint(epoch, val_loss, val_metrics)
            else:
                patience_counter += 1
                
            # Early stopping
            if patience_counter >= 10:
                logger.info("‚èπÔ∏è Early stopping triggered")
                break
                
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞–∂–¥—ã–µ 5 —ç–ø–æ—Ö
            if (epoch + 1) % 5 == 0:
                self._plot_training_progress(epoch + 1)
                
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        self._plot_final_results()
        self._save_training_summary()
        
        logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        
    def _save_checkpoint(self, epoch, val_loss, val_metrics):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –º–æ–¥–µ–ª–∏"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'val_loss': val_loss,
            'val_metrics': val_metrics,
            'config': self.config
        }
        
        checkpoint_path = self.log_dir / 'best_model.pth'
        torch.save(checkpoint, checkpoint_path)
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç: {checkpoint_path}")
        
    def _plot_training_progress(self, epoch):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Loss
        axes[0, 0].plot(self.history['train_loss'], label='Train Loss')
        axes[0, 0].plot(self.history['val_loss'], label='Val Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].set_title('Training and Validation Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # Learning Rate
        axes[0, 1].plot(self.history['learning_rates'])
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Learning Rate')
        axes[0, 1].set_title('Learning Rate Schedule')
        axes[0, 1].grid(True)
        
        # Primary Metric
        if self.config['task'] == 'regression':
            train_metric = [m['mae'] for m in self.history['train_metrics']]
            val_metric = [m['mae'] for m in self.history['val_metrics']]
            metric_name = 'MAE'
        else:
            train_metric = [m['accuracy'] for m in self.history['train_metrics']]
            val_metric = [m['accuracy'] for m in self.history['val_metrics']]
            metric_name = 'Accuracy'
            
        axes[1, 0].plot(train_metric, label=f'Train {metric_name}')
        axes[1, 0].plot(val_metric, label=f'Val {metric_name}')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel(metric_name)
        axes[1, 0].set_title(f'{metric_name} Progress')
        axes[1, 0].legend()
        axes[1, 0].grid(True)
        
        # Additional Metrics
        if self.config['task'] == 'regression':
            val_r2 = [m['r2'] for m in self.history['val_metrics']]
            axes[1, 1].plot(val_r2)
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('R¬≤')
            axes[1, 1].set_title('Validation R¬≤ Score')
            axes[1, 1].grid(True)
        else:
            if 'f1' in self.history['val_metrics'][0]:
                val_f1 = [m['f1'] for m in self.history['val_metrics']]
                axes[1, 1].plot(val_f1)
                axes[1, 1].set_xlabel('Epoch')
                axes[1, 1].set_ylabel('F1 Score')
                axes[1, 1].set_title('Validation F1 Score')
                axes[1, 1].grid(True)
                
        plt.tight_layout()
        plt.savefig(self.log_dir / f'training_progress_epoch_{epoch}.png')
        plt.close()
        
    def _plot_final_results(self):
        """–§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        checkpoint = torch.load(self.log_dir / 'best_model.pth')
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        logger.info("üìä –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")
        
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        # –Ω–∞–ø—Ä–∏–º–µ—Ä, confusion matrix –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        # –∏–ª–∏ scatter plot –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
        
    def _save_training_summary(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        summary = {
            'config': self.config,
            'final_train_loss': self.history['train_loss'][-1],
            'final_val_loss': self.history['val_loss'][-1],
            'final_train_metrics': self.history['train_metrics'][-1],
            'final_val_metrics': self.history['val_metrics'][-1],
            'total_epochs': len(self.history['train_loss']),
            'best_val_loss': min(self.history['val_loss']),
            'best_epoch': self.history['val_loss'].index(min(self.history['val_loss'])) + 1
        }
        
        with open(self.log_dir / 'training_summary.json', 'w') as f:
            json.dump(summary, f, indent=2)
            
        # –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç
        with open(self.log_dir / 'final_report.txt', 'w') as f:
            f.write("=" * 60 + "\n")
            f.write("TEMPORAL FUSION TRANSFORMER V3 - –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢\n")
            f.write("=" * 60 + "\n\n")
            
            f.write(f"–î–∞—Ç–∞ –æ–±—É—á–µ–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"–ó–∞–¥–∞—á–∞: {self.config['task']}\n")
            f.write(f"–í—Å–µ–≥–æ —ç–ø–æ—Ö: {summary['total_epochs']}\n")
            f.write(f"–õ—É—á—à–∞—è —ç–ø–æ—Ö–∞: {summary['best_epoch']}\n")
            f.write(f"–õ—É—á—à–∏–π Val Loss: {summary['best_val_loss']:.4f}\n\n")
            
            f.write("–§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:\n")
            for metric, value in summary['final_val_metrics'].items():
                f.write(f"  {metric}: {value:.4f}\n")


def prepare_data_for_transformer(df, target_type='threshold_binary', test_size=0.2):
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —É–ø—Ä–æ—â–µ–Ω–Ω—ã–µ —Ç–∞—Ä–≥–µ—Ç—ã –∏–∑ XGBoost v3
    """
    logger.info("üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞")
    
    # –†–∞—Å—á–µ—Ç —Ç–∞—Ä–≥–µ—Ç–æ–≤
    target_calculator = TargetCalculator(lookahead_bars=4, price_threshold=0.5)
    df_with_targets = target_calculator.calculate_all_targets(df)
    
    # –í—ã–±–æ—Ä –∫–æ–ª–æ–Ω–∫–∏ —Ç–∞—Ä–≥–µ—Ç–∞
    target_column = f"target_{target_type}"
    if target_column not in df_with_targets.columns:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —Ç–∞—Ä–≥–µ—Ç–∞: {target_type}")
        
    # –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–µ–º FeatureSelector –∏–∑ XGBoost)
    feature_columns = [col for col in df.columns if col not in ['symbol', 'timestamp', 'close'] 
                      and not col.startswith('target_')]
    
    # –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_selector = FeatureSelector(method='hierarchical', top_k=100)
    X = df_with_targets[feature_columns]
    y = df_with_targets[target_column]
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN
    mask = ~(X.isna().any(axis=1) | y.isna())
    X = X[mask]
    y = y[mask]
    
    X_selected, selected_features = feature_selector.select_features(X, y, feature_columns)
    
    logger.info(f"‚úÖ –û—Ç–æ–±—Ä–∞–Ω–æ {len(selected_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ {len(feature_columns)}")
    
    # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ (–∫–∞–∫ –≤ XGBoost)
    n_samples = len(X_selected)
    train_size = int(n_samples * (1 - test_size))
    
    X_train = X_selected.iloc[:train_size]
    X_test = X_selected.iloc[train_size:]
    y_train = y.iloc[:train_size]
    y_test = y.iloc[train_size:]
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    logger.info(f"üìà Train: {len(X_train)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    logger.info(f"üìà Test: {len(X_test)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    if target_type != 'simple_regression':
        unique, counts = np.unique(y_train, return_counts=True)
        logger.info("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ train:")
        for val, count in zip(unique, counts):
            logger.info(f"   –ö–ª–∞—Å—Å {val}: {count} ({count/len(y_train)*100:.1f}%)")
            
    return (X_train_scaled, X_test_scaled, y_train.values, y_test.values, 
            selected_features, scaler)


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Temporal Fusion Transformer v3')
    parser.add_argument('--task', type=str, default='classification',
                       choices=['classification', 'regression'],
                       help='–¢–∏–ø –∑–∞–¥–∞—á–∏')
    parser.add_argument('--target-type', type=str, default='threshold_binary',
                       choices=['simple_binary', 'threshold_binary', 
                               'direction_multiclass', 'simple_regression'],
                       help='–¢–∏–ø —Ç–∞—Ä–≥–µ—Ç–∞')
    parser.add_argument('--epochs', type=int, default=100,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö')
    parser.add_argument('--batch-size', type=int, default=256,
                       help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞')
    parser.add_argument('--learning-rate', type=float, default=0.001,
                       help='Learning rate')
    parser.add_argument('--hidden-dim', type=int, default=128,
                       help='–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è')
    
    args = parser.parse_args()
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    config = {
        'task': args.task,
        'input_dim': 100,  # –±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–æ –ø–æ—Å–ª–µ –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        'hidden_dim': args.hidden_dim,
        'num_heads': 8,
        'dropout_rate': 0.1,
        'output_dim': 1,  # –±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–æ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–¥–∞—á–∏
        'learning_rate': args.learning_rate,
        'weight_decay': 0.0001
    }
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ output_dim
    if args.target_type == 'direction_multiclass':
        config['output_dim'] = 5
        config['task'] = 'classification'
    elif args.target_type == 'simple_regression':
        config['output_dim'] = 1
        config['task'] = 'regression'
    else:
        config['output_dim'] = 1
        config['task'] = 'classification'
        
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –ª–æ–≥–æ–≤
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_dir = f"logs/transformer_v3_{timestamp}"
    
    logger.info("=" * 60)
    logger.info("üöÄ TEMPORAL FUSION TRANSFORMER V3")
    logger.info(f"üìä –ó–∞–¥–∞—á–∞: {config['task']}")
    logger.info(f"üéØ –¢–∏–ø —Ç–∞—Ä–≥–µ—Ç–∞: {args.target_type}")
    logger.info("=" * 60)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ processed_market_data)
    import psycopg2
    conn = psycopg2.connect(
        host="localhost",
        port=5555,
        database="crypto_trading",
        user="ruslan"
    )
    
    query = """
    SELECT * FROM processed_market_data 
    WHERE symbol IN ('BTCUSDT', 'ETHUSDT')
    ORDER BY timestamp
    LIMIT 100000
    """
    
    df = pd.read_sql(query, conn)
    conn.close()
    
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    (X_train, X_test, y_train, y_test, 
     selected_features, scaler) = prepare_data_for_transformer(df, args.target_type)
    
    # –û–±–Ω–æ–≤–ª—è–µ–º input_dim
    config['input_dim'] = X_train.shape[1]
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataLoaders
    train_dataset = TensorDataset(
        torch.FloatTensor(X_train),
        torch.FloatTensor(y_train)
    )
    val_dataset = TensorDataset(
        torch.FloatTensor(X_test),
        torch.FloatTensor(y_test)
    )
    
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = TemporalFusionTransformerV3(config)
    logger.info(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏: {sum(p.numel() for p in model.parameters()):,}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
    trainer = TransformerTrainer(model, config, log_dir)
    
    # –û–±—É—á–µ–Ω–∏–µ
    trainer.train(train_loader, val_loader, args.epochs)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    with open(f"{log_dir}/config.json", 'w') as f:
        json.dump(config, f, indent=2)
        
    with open(f"{log_dir}/selected_features.json", 'w') as f:
        json.dump(selected_features, f, indent=2)
        
    logger.info(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {log_dir}")
    

if __name__ == "__main__":
    import torch.nn.functional as F
    main()