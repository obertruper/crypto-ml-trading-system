#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Transformer –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ–∂–∏–¥–∞–µ–º–æ–π –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –≤ –∫—Ä–∏–ø—Ç–æ—Ç—Ä–µ–π–¥–∏–Ω–≥–µ
- Temporal Fusion Transformer (TFT) –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏ –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
- –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
- –û–±—É—á–µ–Ω–∏–µ –¥–≤—É—Ö –º–æ–¥–µ–ª–µ–π: buy –∏ sell –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä—ã
"""

import numpy as np
import pandas as pd
import tensorflow as tf
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞–º—è—Ç–∏ GPUgpus = tf.config.experimental.list_physical_devices("GPU")if gpus:    try:        tf.config.experimental.set_memory_growth(gpus[0], True)    except RuntimeError as e:        print(e)
from tensorflow import keras
from tensorflow.keras import layers
import os
import json
import pickle
import time
from datetime import datetime
import psycopg2
from psycopg2.extras import execute_values, Json
import logging
import yaml
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import (
    mean_absolute_error, mean_squared_error, r2_score,
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score
)
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import matplotlib
matplotlib.use('Agg')

warnings.filterwarnings('ignore')
np.random.seed(42)
tf.random.set_seed(42)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
log_dir = f'logs/training_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
os.makedirs(log_dir, exist_ok=True)
os.makedirs(f'{log_dir}/plots', exist_ok=True)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –§–∞–π–ª–æ–≤—ã–π –ª–æ–≥–≥–µ—Ä
file_handler = logging.FileHandler(f'{log_dir}/training.log', encoding='utf-8')
file_handler.setLevel(logging.INFO)
file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
file_handler.setFormatter(file_formatter)
logger.addHandler(file_handler)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU
physical_devices = tf.config.list_physical_devices('GPU')
if len(physical_devices) > 0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
    logger.info(f"üñ•Ô∏è GPU –¥–æ—Å—Ç—É–ø–µ–Ω: {physical_devices[0].name}")
else:
    logger.info("‚ö†Ô∏è GPU –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")


class PostgreSQLManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å PostgreSQL"""
    
    def __init__(self, db_config: dict):
        self.db_config = db_config.copy()
        if not self.db_config.get('password'):
            self.db_config.pop('password', None)
        self.connection = None

    def connect(self):
        """–°–æ–∑–¥–∞–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î"""
        try:
            self.connection = psycopg2.connect(**self.db_config)
            self.connection.autocommit = True
            logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ PostgreSQL: {e}")
            raise

    def disconnect(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î"""
        if self.connection:
            self.connection.close()
            logger.info("üì§ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL –∑–∞–∫—Ä—ã—Ç–æ")

    def fetch_dataframe(self, query: str, params=None) -> pd.DataFrame:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–∞–∫ DataFrame"""
        try:
            return pd.read_sql_query(query, self.connection, params=params)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞ DataFrame: {e}")
            raise


class TransformerBlock(layers.Layer):
    """–ë–ª–æ–∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ —Å multi-head attention"""
    
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(TransformerBlock, self).__init__()
        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = keras.Sequential([
            layers.Dense(ff_dim, activation="gelu"),
            layers.Dense(embed_dim),
        ])
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(rate)
        self.dropout2 = layers.Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)


class PositionalEncoding(layers.Layer):
    """–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""
    
    def __init__(self, position, d_model):
        super(PositionalEncoding, self).__init__()
        self.pos_encoding = self.positional_encoding(position, d_model)
    
    def get_angles(self, position, i, d_model):
        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))
        return position * angles
    
    def positional_encoding(self, position, d_model):
        angle_rads = self.get_angles(
            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],
            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],
            d_model=d_model
        )
        
        sines = tf.math.sin(angle_rads[:, 0::2])
        cosines = tf.math.cos(angle_rads[:, 1::2])
        
        pos_encoding = tf.concat([sines, cosines], axis=-1)
        pos_encoding = pos_encoding[tf.newaxis, ...]
        
        return tf.cast(pos_encoding, tf.float32)
    
    def call(self, inputs):
        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]


class GatedResidualNetwork(layers.Layer):
    """Gated Residual Network –¥–ª—è TFT"""
    
    def __init__(self, units, dropout_rate=0.1):
        super().__init__()
        self.units = units
        self.elu_dense = layers.Dense(units, activation='elu')
        self.linear_dense = layers.Dense(units)
        self.dropout = layers.Dropout(dropout_rate)
        self.gate_dense = layers.Dense(units, activation='sigmoid')
        self.layernorm = layers.LayerNormalization()
        
    def call(self, inputs):
        x = self.elu_dense(inputs)
        x = self.linear_dense(x)
        x = self.dropout(x)
        gate = self.gate_dense(inputs)
        x = gate * x
        x = x + inputs
        return self.layernorm(x)


class TemporalFusionTransformer(keras.Model):
    """Temporal Fusion Transformer –¥–ª—è –∑–∞–¥–∞—á–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    
    def __init__(self, num_features, sequence_length, 
                 d_model=128, num_heads=8, num_transformer_blocks=4,
                 mlp_units=[256, 128], dropout=0.2, task='regression'):
        super(TemporalFusionTransformer, self).__init__()
        
        self.sequence_length = sequence_length
        self.d_model = d_model
        
        # Variable Selection Network
        self.vsn_dense = layers.Dense(d_model, activation='gelu')
        self.vsn_grn = GatedResidualNetwork(d_model, dropout)
        
        # Static covariate encoder
        self.static_encoder = keras.Sequential([
            layers.Dense(d_model, activation='gelu'),
            layers.Dropout(dropout),
            GatedResidualNetwork(d_model, dropout)
        ])
        
        # LSTM encoder –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.lstm_encoder = layers.LSTM(d_model, return_sequences=True)
        
        # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.pos_encoding = PositionalEncoding(sequence_length, d_model)
        
        # Transformer –±–ª–æ–∫–∏
        self.transformer_blocks = [
            TransformerBlock(d_model, num_heads, mlp_units[0], dropout)
            for _ in range(num_transformer_blocks)
        ]
        
        # Interpretable Multi-Head Attention
        self.interpretable_attention = layers.MultiHeadAttention(
            num_heads=num_heads, 
            key_dim=d_model,
            dropout=dropout
        )
        
        # Output layer –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–¥–∞—á–∏
        self.task = task
        if task == 'regression':
            self.output_dense = keras.Sequential([
                layers.Dense(mlp_units[0], activation='gelu'),
                layers.Dropout(dropout),
                layers.Dense(mlp_units[1], activation='gelu'),
                layers.Dropout(dropout),
                layers.Dense(1)  # –õ–∏–Ω–µ–π–Ω—ã–π –≤—ã—Ö–æ–¥ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
            ])
        else:  # classification_binary
            self.output_dense = keras.Sequential([
                layers.Dense(mlp_units[0], activation='gelu'),
                layers.Dropout(dropout),
                layers.Dense(mlp_units[1], activation='gelu'),
                layers.Dropout(dropout),
                layers.Dense(1, activation='sigmoid')  # Sigmoid –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            ])
    
    def call(self, inputs, training=False):
        # Variable selection
        selected = self.vsn_dense(inputs)
        selected = self.vsn_grn(selected)
        
        # LSTM encoding
        lstm_out = self.lstm_encoder(selected)
        
        # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        x = self.pos_encoding(lstm_out)
        
        # Transformer blocks
        for transformer in self.transformer_blocks:
            x = transformer(x, training=training)
        
        # Interpretable attention
        attn_output = self.interpretable_attention(x, x, training=training)
        
        # Global average pooling
        x = layers.GlobalAveragePooling1D()(attn_output)
        
        # Output
        return self.output_dense(x)


class VisualizationCallback(keras.callbacks.Callback):
    """Callback –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, log_dir, model_name, update_freq=5, task='regression'):
        super().__init__()
        self.log_dir = log_dir
        self.model_name = model_name
        self.update_freq = update_freq
        self.epoch_count = 0
        self.task = task
        
        self.history = {
            'loss': [],
            'val_loss': [],
            'lr': []
        }
        
        if task == 'regression':
            self.history.update({
                'mae': [],
                'val_mae': [],
                'rmse': [],
                'val_rmse': []
            })
        else:  # classification
            self.history.update({
                'accuracy': [],
                'val_accuracy': [],
                'precision': [],
                'val_precision': [],
                'recall': [],
                'val_recall': []
            })
        
        self.fig, self.axes = plt.subplots(2, 2, figsize=(15, 10))
        self.fig.suptitle(f'Training Progress: {model_name}', fontsize=16)
        
    def on_epoch_end(self, epoch, logs=None):
        self.epoch_count += 1
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        self.history['loss'].append(logs.get('loss', 0))
        self.history['val_loss'].append(logs.get('val_loss', 0))
        self.history['lr'].append(self.model.optimizer.learning_rate.numpy())
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        if self.task == 'regression':
            self.history['mae'].append(logs.get('mae', 0))
            self.history['val_mae'].append(logs.get('val_mae', 0))
            self.history['rmse'].append(logs.get('rmse', 0))
            self.history['val_rmse'].append(logs.get('val_rmse', 0))
        else:  # classification
            self.history['accuracy'].append(logs.get('accuracy', 0))
            self.history['val_accuracy'].append(logs.get('val_accuracy', 0))
            self.history['precision'].append(logs.get('precision', 0))
            self.history['val_precision'].append(logs.get('val_precision', 0))
            self.history['recall'].append(logs.get('recall', 0))
            self.history['val_recall'].append(logs.get('val_recall', 0))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ CSV
        metrics_df = pd.DataFrame(self.history)
        metrics_df.to_csv(f'{self.log_dir}/{self.model_name}_metrics.csv', index=False)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –≥—Ä–∞—Ñ–∏–∫–∏
        if self.epoch_count % self.update_freq == 0:
            self.update_plots()
            
    def update_plots(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤"""
        epochs = range(1, len(self.history['loss']) + 1)
        
        # –ì—Ä–∞—Ñ–∏–∫ 1: Loss
        self.axes[0, 0].clear()
        self.axes[0, 0].plot(epochs, self.history['loss'], 'b-', label='Train Loss', linewidth=2)
        self.axes[0, 0].plot(epochs, self.history['val_loss'], 'r-', label='Val Loss', linewidth=2)
        self.axes[0, 0].set_title('Model Loss')
        self.axes[0, 0].set_xlabel('Epoch')
        self.axes[0, 0].set_ylabel('Loss')
        self.axes[0, 0].legend()
        self.axes[0, 0].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 2: –û—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞
        self.axes[0, 1].clear()
        if self.task == 'regression':
            self.axes[0, 1].plot(epochs, self.history['mae'], 'b-', label='Train MAE', linewidth=2)
            self.axes[0, 1].plot(epochs, self.history['val_mae'], 'r-', label='Val MAE', linewidth=2)
            self.axes[0, 1].set_title('Mean Absolute Error (%)')
            self.axes[0, 1].set_ylabel('MAE (%)')
        else:  # classification
            self.axes[0, 1].plot(epochs, self.history['accuracy'], 'b-', label='Train Accuracy', linewidth=2)
            self.axes[0, 1].plot(epochs, self.history['val_accuracy'], 'r-', label='Val Accuracy', linewidth=2)
            self.axes[0, 1].set_title('Model Accuracy')
            self.axes[0, 1].set_ylabel('Accuracy')
            self.axes[0, 1].set_ylim(0, 1)
        
        self.axes[0, 1].set_xlabel('Epoch')
        self.axes[0, 1].legend()
        self.axes[0, 1].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 3: Learning Rate
        self.axes[1, 0].clear()
        self.axes[1, 0].plot(epochs, self.history['lr'], 'g-', linewidth=2)
        self.axes[1, 0].set_title('Learning Rate Schedule')
        self.axes[1, 0].set_xlabel('Epoch')
        self.axes[1, 0].set_ylabel('Learning Rate')
        self.axes[1, 0].set_yscale('log')
        self.axes[1, 0].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 4: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.axes[1, 1].clear()
        self.axes[1, 1].axis('off')
        
        if self.task == 'regression':
            current_stats = f"""
Current Epoch: {len(self.history['loss'])}

Loss:
  Train:  {self.history['loss'][-1]:.4f}
  Val:    {self.history['val_loss'][-1]:.4f}
  
MAE (%):
  Train:  {self.history['mae'][-1]:.3f}%
  Val:    {self.history['val_mae'][-1]:.3f}%

RMSE (%):
  Train:  {self.history['rmse'][-1]:.3f}%
  Val:    {self.history['val_rmse'][-1]:.3f}%

Best Results:
  Val Loss: {min(self.history['val_loss']):.4f}
  Val MAE:  {min(self.history['val_mae']):.3f}%
  Epoch:    {self.history['val_loss'].index(min(self.history['val_loss'])) + 1}

Learning Rate: {self.history['lr'][-1]:.2e}
"""
        else:  # classification
            current_stats = f"""
Current Epoch: {len(self.history['loss'])}

Loss:
  Train:  {self.history['loss'][-1]:.4f}
  Val:    {self.history['val_loss'][-1]:.4f}
  
Accuracy:
  Train:  {self.history['accuracy'][-1]:.3f}
  Val:    {self.history['val_accuracy'][-1]:.3f}

Precision:
  Train:  {self.history['precision'][-1]:.3f}
  Val:    {self.history['val_precision'][-1]:.3f}
  
Recall:
  Train:  {self.history['recall'][-1]:.3f}
  Val:    {self.history['val_recall'][-1]:.3f}

Best Results:
  Val Loss: {min(self.history['val_loss']):.4f}
  Val Acc:  {max(self.history['val_accuracy']) if self.history['val_accuracy'] else 0:.3f}
  Epoch:    {self.history['val_accuracy'].index(max(self.history['val_accuracy'])) + 1 if self.history['val_accuracy'] else 0}

Learning Rate: {self.history['lr'][-1]:.2e}
"""
        
        self.axes[1, 1].text(0.1, 0.5, current_stats, fontsize=12, 
                            verticalalignment='center', family='monospace',
                            bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.5))
        
        plt.tight_layout()
        plt.savefig(f'{self.log_dir}/plots/training_progress.png', dpi=150, bbox_inches='tight')
        plt.savefig(f'{self.log_dir}/plots/epoch_{self.epoch_count:03d}.png', dpi=100)
        
        logger.info(f"üìä –ì—Ä–∞—Ñ–∏–∫ –æ–±–Ω–æ–≤–ª–µ–Ω: —ç–ø–æ—Ö–∞ {self.epoch_count}")


class UniversalTransformerTrainer:
    """–¢—Ä–µ–Ω–µ—Ä –¥–ª—è Transformer –º–æ–¥–µ–ª–µ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    
    def __init__(self, db_manager: PostgreSQLManager, config_path='config.yaml', task='regression'):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
            
        self.db = db_manager
        self.task = task  # 'regression' –∏–ª–∏ 'classification_binary'
        self.sequence_length = self.config['model']['sequence_length']
        self.batch_size = 32
        self.epochs = 100
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
        self.d_model = 128
        self.num_heads = 8
        self.num_transformer_blocks = 4
        self.ff_dim = 256
        self.dropout_rate = 0.2
        
        self.scaler = RobustScaler()
        self.models = {}
        self.feature_columns = None
        self.log_dir = log_dir
        
        # –ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ (49 —à—Ç—É–∫)
        self.TECHNICAL_INDICATORS = [
            # –¢—Ä–µ–Ω–¥–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            'ema_15', 'adx_val', 'adx_plus_di', 'adx_minus_di',
            'macd_val', 'macd_signal', 'macd_hist', 'sar',
            'ichimoku_conv', 'ichimoku_base', 'aroon_up', 'aroon_down',
            'dpo',
            
            # –û—Å—Ü–∏–ª–ª—è—Ç–æ—Ä—ã
            'rsi_val', 'stoch_k', 'stoch_d', 'cci_val', 'williams_r',
            'roc', 'ult_osc', 'mfi',
            
            # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
            'atr_val', 'bb_upper', 'bb_lower', 'bb_basis',
            'donchian_upper', 'donchian_lower',
            
            # –û–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            'obv', 'cmf', 'volume_sma', 'volume_ratio',
            
            # Vortex –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            'vortex_vip', 'vortex_vin',
            
            # –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            'macd_signal_ratio', 'adx_diff', 'bb_position',
            'rsi_dist_from_mid', 'stoch_diff', 'vortex_ratio',
            'ichimoku_diff', 'atr_norm',
            
            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            'hour', 'day_of_week', 'is_weekend',
            
            # –¶–µ–Ω–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            'price_change_1', 'price_change_4', 'price_change_16',
            'volatility_4', 'volatility_16'
        ]
        
        self.feature_importance = {}
        
    def convert_to_binary_labels(self, returns, threshold=0.3):
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ expected returns –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏ (–ø–æ—Ä–æ–≥ > 0.3%)"""
        return (returns > threshold).astype(np.float32)
    
    def create_model(self, input_shape, name):
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        
        model = TemporalFusionTransformer(
            num_features=input_shape[1],
            sequence_length=input_shape[0],
            d_model=self.d_model,
            num_heads=self.num_heads,
            num_transformer_blocks=self.num_transformer_blocks,
            mlp_units=[self.ff_dim, self.ff_dim//2],
            dropout=self.dropout_rate,
            task=self.task
        )
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        optimizer = keras.optimizers.Adam(
            learning_rate=0.0002,  # –£–≤–µ–ª–∏—á–µ–Ω –¥–ª—è –ª—É—á—à–µ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å –ø–æ—Ä–æ–≥–æ–º 0.3%
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-7
        )
        
        # –ö–æ–º–ø–∏–ª—è—Ü–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–¥–∞—á–∏
        if self.task == 'regression':
            model.compile(
                optimizer=optimizer,
                loss=keras.losses.Huber(delta=1.0),
                metrics=[
                    keras.metrics.MeanAbsoluteError(name='mae'),
                    keras.metrics.RootMeanSquaredError(name='rmse')
                ]
            )
        else:  # classification_binary
            model.compile(
                optimizer=optimizer,
                loss=keras.losses.BinaryCrossentropy(label_smoothing=0.05),
                metrics=[
                    keras.metrics.BinaryAccuracy(name='accuracy'),
                    keras.metrics.Precision(name='precision'),
                    keras.metrics.Recall(name='recall')
                ]
            )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        dummy_input = tf.zeros((1, input_shape[0], input_shape[1]))
        _ = model(dummy_input)
        
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –º–æ–¥–µ–ª—å {name} —Å {model.count_params():,} –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏")
        
        return model
    
    def load_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL"""
        logger.info("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL...")
        start_time = time.time()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å expected returns –∏–∑ –∫–æ–ª–æ–Ω–æ–∫ –ë–î
        query = """
        SELECT 
            p.symbol, p.timestamp, p.datetime,
            p.technical_indicators,
            p.buy_expected_return,
            p.sell_expected_return,
            p.is_long_entry,
            p.is_short_entry,
            p.open, p.high, p.low, p.close, p.volume
        FROM processed_market_data p
        JOIN raw_market_data r ON p.raw_data_id = r.id
        WHERE p.technical_indicators IS NOT NULL
          AND r.market_type = 'futures'
        ORDER BY p.symbol, p.timestamp
        """
        
        df = self.db.fetch_dataframe(query)
        load_time = time.time() - start_time
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –∑–∞ {load_time:.2f} —Å–µ–∫—É–Ω–¥ ({load_time/60:.1f} –º–∏–Ω—É—Ç)")
        
        if len(df) == 0:
            raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        symbol_counts = df['symbol'].value_counts()
        logger.info("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–∏–º–≤–æ–ª–∞–º:")
        for symbol, count in symbol_counts.items():
            logger.info(f"   {symbol}: {count:,} –∑–∞–ø–∏—Å–µ–π")
        
        return df
    
    def prepare_features_and_targets(self, df):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –≤—Ä–µ–º–µ–Ω–Ω—ã–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º"""
        logger.info("üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...")
        logger.info(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {len(self.TECHNICAL_INDICATORS)} –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤")
        start_time = time.time()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω expected returns
        buy_returns = df['buy_expected_return'].values
        sell_returns = df['sell_expected_return'].values
        
        buy_outliers = np.sum((buy_returns < -1.1) | (buy_returns > 5.8))
        sell_outliers = np.sum((sell_returns < -1.1) | (sell_returns > 5.8))
        
        if buy_outliers > 0 or sell_outliers > 0:
            logger.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω—ã –∑–Ω–∞—á–µ–Ω–∏—è expected_return –≤–Ω–µ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ [-1.1%, +5.8%]:")
            logger.warning(f"   BUY outliers: {buy_outliers} ({buy_outliers/len(df)*100:.2f}%)")
            logger.warning(f"   SELL outliers: {sell_outliers} ({sell_outliers/len(df)*100:.2f}%)")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        df = df.sort_values(["symbol", "timestamp"])
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Å–∏–º–≤–æ–ª–∞–º –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        grouped_data = {
            "train": {"X": [], "y_buy": [], "y_sell": [], "symbols": []},
            "val": {"X": [], "y_buy": [], "y_sell": [], "symbols": []},
            "test": {"X": [], "y_buy": [], "y_sell": [], "symbols": []},
        }
        
        for symbol in tqdm(df["symbol"].unique(), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∏–º–≤–æ–ª–æ–≤"):
            symbol_df = df[df["symbol"] == symbol].reset_index(drop=True)
            n = len(symbol_df)
            
            # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –°–´–†–´–• –¥–∞–Ω–Ω—ã—Ö
            train_end = int(n * 0.7)
            val_end = int(n * 0.85)
            gap = self.sequence_length  # –ó–∞–∑–æ—Ä –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —É—Ç–µ—á–∫–∏
            
            # –†–∞–∑–¥–µ–ª—è–µ–º —Å —É—á–µ—Ç–æ–º gap
            splits = {
                "train": symbol_df[:train_end - gap],
                "val": symbol_df[train_end + gap:val_end - gap],
                "test": symbol_df[val_end + gap:]
            }
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π split –æ—Ç–¥–µ–ª—å–Ω–æ
            for split_name, split_df in splits.items():
                if len(split_df) < self.sequence_length + 1:
                    continue
                    
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
                X_split = []
                for _, row in split_df.iterrows():
                    feature_values = []
                    
                    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
                    indicators = row["technical_indicators"]
                    for indicator in self.TECHNICAL_INDICATORS:
                        value = indicators.get(indicator, 0.0)
                        if value is None or pd.isna(value):
                            value = 0.0
                        feature_values.append(float(value))
                    
                    # –ò–Ω–∂–µ–Ω–µ—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                    rsi = indicators.get("rsi_val", 50.0)
                    feature_values.append(1.0 if rsi is not None and rsi < 30 else 0.0)
                    feature_values.append(1.0 if rsi is not None and rsi > 70 else 0.0)
                    
                    macd = indicators.get("macd_val", 0.0)
                    macd_signal = indicators.get("macd_signal_val", 0.0)
                    feature_values.append(1.0 if macd is not None and macd_signal is not None and macd > macd_signal else 0.0)
                    
                    bb_position = indicators.get("bb_position", 0.5)
                    feature_values.append(1.0 if bb_position is not None and bb_position < 0.2 else 0.0)
                    feature_values.append(1.0 if bb_position is not None and bb_position > 0.8 else 0.0)
                    
                    adx = indicators.get("adx_val", 0.0)
                    feature_values.append(1.0 if adx is not None and adx > 25 else 0.0)
                    
                    volume_ratio = indicators.get("volume_ratio", 1.0)
                    feature_values.append(1.0 if volume_ratio is not None and volume_ratio > 2.0 else 0.0)
                    
                    X_split.append(feature_values)
                
                # –¶–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                y_buy = split_df["buy_expected_return"].values.astype(float)
                y_sell = split_df["sell_expected_return"].values.astype(float)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π split
                grouped_data[split_name]["X"].extend(X_split)
                grouped_data[split_name]["y_buy"].extend(y_buy)
                grouped_data[split_name]["y_sell"].extend(y_sell)
                grouped_data[split_name]["symbols"].extend([symbol] * len(X_split))
        
        prep_time = time.time() - start_time
        logger.info(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {prep_time:.2f} —Å–µ–∫—É–Ω–¥ ({prep_time/60:.1f} –º–∏–Ω—É—Ç)")
        return grouped_data
    def create_sequences(self, X, y, symbols, stride=5):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""
        logger.info(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª–∏–Ω–æ–π {self.sequence_length}...")
        start_time = time.time()
        
        sequences = []
        targets = []
        seq_symbols = []
        
        unique_symbols = np.unique(symbols)
        
        for symbol in unique_symbols:
            symbol_mask = symbols == symbol
            symbol_indices = np.where(symbol_mask)[0]
            
            if len(symbol_indices) < self.sequence_length + 1:
                logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {len(symbol_indices)} –∑–∞–ø–∏—Å–µ–π")
                continue
            
            X_symbol = X[symbol_indices]
            y_symbol = y[symbol_indices]
            
            # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            for i in range(0, len(X_symbol) - self.sequence_length, stride):
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º stride –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
                sequences.append(X_symbol[i:i + self.sequence_length])
                targets.append(y_symbol[i + self.sequence_length])
                seq_symbols.append(symbol)
        
        sequences = np.array(sequences)
        targets = np.array(targets)
        
        seq_time = time.time() - start_time
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(sequences)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∑–∞ {seq_time:.2f} —Å–µ–∫—É–Ω–¥")
        logger.info(f"   –§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {sequences.shape}")
        
        return sequences, targets, np.array(seq_symbols)
    
    
    def train_model(self, model, X_train, y_train, X_val, y_val, model_name):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π"""
        logger.info(f"\n{'='*70}")
        logger.info(f"üöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø: {model_name}")
        logger.info(f"{'='*70}")
        
        # Callbacks
        callbacks = [
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
            VisualizationCallback(self.log_dir, model_name, update_freq=5, task=self.task),
            
            # Early stopping —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º patience
            keras.callbacks.EarlyStopping(
                monitor='val_loss',
                patience=20,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
                restore_best_weights=True,
                verbose=1
            ),
            
            # Reduce LR
            keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=7,
                min_lr=1e-7,
                verbose=1
            ),
            
            # Model checkpoint
            keras.callbacks.ModelCheckpoint(
                filepath=f'trained_model/{model_name}_best.h5',
                monitor='val_loss',
                save_best_only=True,
                verbose=1
            ),
            
            # TensorBoard
            keras.callbacks.TensorBoard(
                log_dir=f'{self.log_dir}/tensorboard/{model_name}',
                histogram_freq=1,
                write_graph=True,
                update_freq='epoch'
            )
        ]
        
        # –û–±—É—á–µ–Ω–∏–µ
        if self.task == 'classification_binary':
            # –ü–æ–¥—Å—á–µ—Ç –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
            unique, counts = np.unique(y_train, return_counts=True)
            class_weight = {0: counts[1] / counts[0], 1: 1.0} if len(unique) == 2 else None
            
            if class_weight:
                logger.info(f"üìä –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ - 0: {counts[0]:,}, 1: {counts[1]:,}")
                logger.info(f"‚öñÔ∏è –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ - 0: {class_weight[0]:.2f}, 1: {class_weight[1]:.2f}")
            
            history = model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=self.epochs,
                batch_size=self.batch_size,
                callbacks=callbacks,
                class_weight=class_weight,
                verbose=1
            )
        else:  # regression
            history = model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=self.epochs,
                batch_size=self.batch_size,
                callbacks=callbacks,
                verbose=1
            )
        
        return history
    
    def evaluate_model(self, model, X_test, y_test, model_name):
        """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏"""
        logger.info(f"\nüìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred = model.predict(X_test, verbose=0)
        y_pred = y_pred.flatten()
        
        if self.task == 'regression':
            # –ú–µ—Ç—Ä–∏–∫–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
            mae = mean_absolute_error(y_test, y_pred)
            mse = mean_squared_error(y_test, y_pred)
            rmse = np.sqrt(mse)
            r2 = r2_score(y_test, y_pred)
            direction_accuracy = np.mean((y_pred > 0) == (y_test > 0))
            
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
            self._plot_regression_results(y_test, y_pred, model_name, mae, rmse, r2, direction_accuracy)
            
            metrics = {
                'mae': mae,
                'rmse': rmse,
                'r2': r2,
                'direction_accuracy': direction_accuracy,
                'mean_error': np.mean(y_pred - y_test),
                'std_error': np.std(y_pred - y_test)
            }
        else:  # classification_binary
            # –ë–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            y_pred_binary = (y_pred > 0.5).astype(int)
            
            # –ú–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            accuracy = accuracy_score(y_test, y_pred_binary)
            precision = precision_score(y_test, y_pred_binary, zero_division=0)
            recall = recall_score(y_test, y_pred_binary, zero_division=0)
            f1 = f1_score(y_test, y_pred_binary, zero_division=0)
            auc = roc_auc_score(y_test, y_pred)
            cm = confusion_matrix(y_test, y_pred_binary)
            
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
            self._plot_classification_results(y_test, y_pred, y_pred_binary, model_name, 
                                            accuracy, precision, recall, f1, auc, cm)
            
            metrics = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'auc': auc,
                'confusion_matrix': cm
            }
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        logger.info(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è {model_name}:")
        for key, value in metrics.items():
            if isinstance(value, (int, float)):
                logger.info(f"   {key}: {value:.4f}")
            elif key == 'confusion_matrix':
                logger.info(f"   Confusion Matrix:")
                logger.info(f"      TN: {value[0,0]:,}  FP: {value[0,1]:,}")
                logger.info(f"      FN: {value[1,0]:,}  TP: {value[1,1]:,}")
        
        return metrics
    
    
    def _plot_regression_results(self, y_test, y_pred, model_name, mae, rmse, r2, direction_accuracy):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'Model Evaluation: {model_name}', fontsize=16)
        
        # –ì—Ä–∞—Ñ–∏–∫ 1: Scatter plot
        axes[0, 0].scatter(y_test, y_pred, alpha=0.5, s=10)
        axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
        axes[0, 0].set_xlabel('True Return (%)')
        axes[0, 0].set_ylabel('Predicted Return (%)')
        axes[0, 0].set_title(f'Predictions vs True Values (R¬≤ = {r2:.3f})')
        axes[0, 0].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 2: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫
        errors = y_pred - y_test
        axes[0, 1].hist(errors, bins=50, edgecolor='black', alpha=0.7)
        axes[0, 1].axvline(x=0, color='r', linestyle='--', lw=2)
        axes[0, 1].set_xlabel('Prediction Error (%)')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].set_title(f'Error Distribution (MAE = {mae:.3f}%)')
        axes[0, 1].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 3: –í—Ä–µ–º–µ–Ω–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        sample_size = min(500, len(y_test))
        axes[1, 0].plot(y_test[:sample_size], 'b-', label='True', alpha=0.7, linewidth=1)
        axes[1, 0].plot(y_pred[:sample_size], 'r-', label='Predicted', alpha=0.7, linewidth=1)
        axes[1, 0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)
        axes[1, 0].set_xlabel('Sample Index')
        axes[1, 0].set_ylabel('Return (%)')
        axes[1, 0].set_title('Sample Predictions Timeline')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 4: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º
        axes[1, 1].axis('off')
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º
        ranges = [(-np.inf, -2), (-2, -0.5), (-0.5, 0.5), (0.5, 2), (2, np.inf)]
        range_names = ['< -2%', '-2% to -0.5%', '-0.5% to 0.5%', '0.5% to 2%', '> 2%']
        range_stats = []
        
        for i, (low, high) in enumerate(ranges):
            mask = (y_test >= low) & (y_test < high)
            if np.any(mask):
                range_mae = mean_absolute_error(y_test[mask], y_pred[mask])
                range_stats.append(f"{range_names[i]}: {np.sum(mask)} samples, MAE = {range_mae:.3f}%")
        
        stats_text = f"""
Performance Summary:
  MAE: {mae:.3f}%
  RMSE: {rmse:.3f}%
  R¬≤: {r2:.3f}
  Direction Accuracy: {direction_accuracy:.1%}

Range Analysis:
  """ + '\n  '.join(range_stats)
        
        axes[1, 1].text(0.1, 0.5, stats_text, fontsize=11, 
                       verticalalignment='center', family='monospace',
                       bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.5))
        
        plt.tight_layout()
        plt.savefig(f'{self.log_dir}/plots/{model_name}_evaluation.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    
    def _plot_classification_results(self, y_test, y_pred_proba, y_pred_binary, model_name, 
                                   accuracy, precision, recall, f1, auc, cm):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        from sklearn.metrics import roc_curve
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'Model Evaluation: {model_name} (Binary Classification)', fontsize=16)
        
        # –ì—Ä–∞—Ñ–∏–∫ 1: ROC –∫—Ä–∏–≤–∞—è
        fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
        axes[0, 0].plot(fpr, tpr, 'b-', lw=2, label=f'ROC (AUC = {auc:.3f})')
        axes[0, 0].plot([0, 1], [0, 1], 'r--', lw=2, label='Random')
        axes[0, 0].set_xlabel('False Positive Rate')
        axes[0, 0].set_ylabel('True Positive Rate')
        axes[0, 0].set_title('ROC Curve')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 2: Confusion Matrix
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1])
        axes[0, 1].set_xlabel('Predicted')
        axes[0, 1].set_ylabel('Actual')
        axes[0, 1].set_title('Confusion Matrix')
        axes[0, 1].set_xticklabels(['–ù–µ –≤—Ö–æ–¥–∏—Ç—å', '–í—Ö–æ–¥–∏—Ç—å'])
        axes[0, 1].set_yticklabels(['–ù–µ –≤—Ö–æ–¥–∏—Ç—å', '–í—Ö–æ–¥–∏—Ç—å'])
        
        # –ì—Ä–∞—Ñ–∏–∫ 3: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        axes[1, 0].hist(y_pred_proba[y_test == 0], bins=50, alpha=0.7, 
                       label='–ö–ª–∞—Å—Å 0 (–ù–µ –≤—Ö–æ–¥–∏—Ç—å)', density=True)
        axes[1, 0].hist(y_pred_proba[y_test == 1], bins=50, alpha=0.7, 
                       label='–ö–ª–∞—Å—Å 1 (–í—Ö–æ–¥–∏—Ç—å)', density=True)
        axes[1, 0].set_xlabel('Predicted Probability')
        axes[1, 0].set_ylabel('Density')
        axes[1, 0].set_title('Probability Distribution by Class')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 4: –ú–µ—Ç—Ä–∏–∫–∏
        axes[1, 1].axis('off')
        stats_text = f"""
Performance Summary:
  Accuracy: {accuracy:.3f}
  Precision: {precision:.3f}
  Recall: {recall:.3f}
  F1-Score: {f1:.3f}
  ROC-AUC: {auc:.3f}
  
Confusion Matrix:
  TN: {cm[0,0]:,}  FP: {cm[0,1]:,}
  FN: {cm[1,0]:,}  TP: {cm[1,1]:,}
  
Positive Rate: {np.sum(y_test == 1) / len(y_test):.1%}
"""
        
        axes[1, 1].text(0.1, 0.5, stats_text, fontsize=12,
                       verticalalignment='center', family='monospace',
                       bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.5))
        
        plt.tight_layout()
        plt.savefig(f'{self.log_dir}/plots/{model_name}_classification_evaluation.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def train(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è"""
        logger.info("\n" + "="*80)
        if self.task == 'regression':
            logger.info("üöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø TRANSFORMER –ú–û–î–ï–õ–ò –†–ï–ì–†–ï–°–°–ò–ò")
            logger.info("üìä –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ expected returns –¥–ª—è buy –∏ sell –ø–æ–∑–∏—Ü–∏–π")
        else:
            logger.info("üöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø TRANSFORMER –ú–û–î–ï–õ–ò –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò")
            logger.info("üìä –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –≤—Ö–æ–¥–∏—Ç—å/–Ω–µ –≤—Ö–æ–¥–∏—Ç—å (–ø–æ—Ä–æ–≥ > 0.3%)")
        logger.info("="*80)
        
        try:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            os.makedirs('trained_model', exist_ok=True)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            df = self.load_data()
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            grouped_data = self.prepare_features_and_targets(df)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
            logger.info("üîÑ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è fit scaler
            all_X = []
            for split in ["train", "val", "test"]:
                all_X.extend(grouped_data[split]["X"])
            
            self.scaler.fit(all_X)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–∞–∂–¥—ã–π split
            for split in ["train", "val", "test"]:
                grouped_data[split]["X"] = self.scaler.transform(grouped_data[split]["X"]).tolist()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            all_buy = []
            all_sell = []
            for split in ["train", "val", "test"]:
                all_buy.extend(grouped_data[split]["y_buy"])
                all_sell.extend(grouped_data[split]["y_sell"])
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            if self.task == 'regression':
                logger.info("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ buy_return:")
                logger.info(f"   –°—Ä–µ–¥–Ω–µ–µ: {np.mean(all_buy):.3f}%")
                logger.info(f"   Std: {np.std(all_buy):.3f}%")
                logger.info(f"   Min/Max: {np.min(all_buy):.3f}% / {np.max(all_buy):.3f}%")
            else:  # classification
                buy_binary = self.convert_to_binary_labels(np.array(all_buy), threshold=0.3)
                sell_binary = self.convert_to_binary_labels(np.array(all_sell), threshold=0.3)
                
                logger.info("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∏–Ω–∞—Ä–Ω—ã—Ö –º–µ—Ç–æ–∫ (–ø–æ—Ä–æ–≥ > 0.3%):")
                logger.info(f"   Buy - –ö–ª–∞—Å—Å 0 (–Ω–µ –≤—Ö–æ–¥–∏—Ç—å): {np.sum(buy_binary == 0):,} ({np.mean(buy_binary == 0):.1%})")
                logger.info(f"   Buy - –ö–ª–∞—Å—Å 1 (–≤—Ö–æ–¥–∏—Ç—å): {np.sum(buy_binary == 1):,} ({np.mean(buy_binary == 1):.1%})")
                logger.info(f"   Sell - –ö–ª–∞—Å—Å 0 (–Ω–µ –≤—Ö–æ–¥–∏—Ç—å): {np.sum(sell_binary == 0):,} ({np.mean(sell_binary == 0):.1%})")
                logger.info(f"   Sell - –ö–ª–∞—Å—Å 1 (–≤—Ö–æ–¥–∏—Ç—å): {np.sum(sell_binary == 1):,} ({np.mean(sell_binary == 1):.1%})")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
            unique_buy = len(np.unique(all_buy))
            unique_sell = len(np.unique(all_sell))
            buy_uniqueness = unique_buy / len(all_buy) * 100
            sell_uniqueness = unique_sell / len(all_sell) * 100
            
            logger.info(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {unique_buy:,} ({buy_uniqueness:.1f}%)")
            
            logger.info("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ sell_return:")
            logger.info(f"   –°—Ä–µ–¥–Ω–µ–µ: {np.mean(all_sell):.3f}%")
            logger.info(f"   Std: {np.std(all_sell):.3f}%")
            logger.info(f"   Min/Max: {np.min(all_sell):.3f}% / {np.max(all_sell):.3f}%")
            logger.info(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {unique_sell:,} ({sell_uniqueness:.1f}%)")
            
            # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –Ω–∏–∑–∫–æ–π —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
            if buy_uniqueness < 10 or sell_uniqueness < 10:
                logger.warning("\n‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ù–∏–∑–∫–∞—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π!")
                logger.warning("   –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö —Å —á–∞—Å—Ç–∏—á–Ω—ã–º–∏ –∑–∞–∫—Ä—ã—Ç–∏—è–º–∏.")
                logger.warning(f"   –û—Å–Ω–æ–≤–Ω—ã–µ —É—Ä–æ–≤–Ω–∏: -1.1% (SL), 0.48%, 1.56%, 2.49%, 3.17%, 5.8% (TP)")
                
                # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
                buy_values, buy_counts = np.unique(all_buy, return_counts=True)
                sell_values, sell_counts = np.unique(all_sell, return_counts=True)
                
                logger.info("\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ expected returns:")
                logger.info("   Buy —Ç–æ–ø-5 –∑–Ω–∞—á–µ–Ω–∏–π:")
                for val, cnt in sorted(zip(buy_values, buy_counts), key=lambda x: -x[1])[:5]:
                    logger.info(f"     {val:.2f}%: {cnt} ({cnt/len(all_buy)*100:.1f}%)")
                
                # –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                if buy_uniqueness < 1 or sell_uniqueness < 1:
                    logger.error("‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –Ω–∏–∑–∫–∞—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å! –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ.")
                    logger.error("   –í—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ.")
                    raise ValueError("–î–∞–Ω–Ω—ã–µ –Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è - –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            os.makedirs("trained_model", exist_ok=True)
            stats = {
                "mean": self.scaler.center_.tolist() if hasattr(self.scaler, "center_") else None,
                "scale": self.scaler.scale_.tolist() if hasattr(self.scaler, "scale_") else None,
                "feature_names": self.TECHNICAL_INDICATORS + [
                    "rsi_oversold", "rsi_overbought", "macd_bullish",
                    "bb_near_lower", "bb_near_upper", "strong_trend", "high_volume"
                ]
            }
            
            with open("trained_model/scaler_stats.json", "w") as f:
                json.dump(stats, f, indent=2)
            
            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π
            if self.task == 'regression':
                model_configs = [
                    ('buy_return_predictor', 'buy'),
                    ('sell_return_predictor', 'sell')
                ]
            else:  # classification
                model_configs = [
                    ('buy_classifier', 'buy'),
                    ('sell_classifier', 'sell')
                ]
            
            results = {}
            
            for model_name, target_type in model_configs:
                logger.info(f"\n{'='*60}")
                logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}")
                logger.info(f"{'='*60}")
                
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
                all_X = []
                all_y = []
                all_symbols = []
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö splits –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
                for split in ["train", "val", "test"]:
                    X_split = np.array(grouped_data[split]["X"])
                    symbols_split = np.array(grouped_data[split]["symbols"])
                    
                    if target_type == 'buy':
                        y_split = np.array(grouped_data[split]["y_buy"])
                    else:  # 'sell'
                        y_split = np.array(grouped_data[split]["y_sell"])
                    
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                    if self.task == 'classification_binary':
                        y_split = self.convert_to_binary_labels(y_split, threshold=0.3)
                    
                    # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ split –æ—Ç–¥–µ–ª—å–Ω–æ
                    X_seq, y_seq, seq_symbols = self.create_sequences(
                        X_split, y_split, symbols_split, stride=5
                    )
                    
                    if split == "train":
                        X_train = X_seq
                        y_train = y_seq
                        symbols_train = seq_symbols
                    elif split == "val":
                        X_val = X_seq
                        y_val = y_seq
                        symbols_val = seq_symbols
                    else:  # test
                        X_test = X_seq
                        y_test = y_seq
                        symbols_test = seq_symbols
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö
                if len(X_train) == 0 or len(X_val) == 0 or len(X_test) == 0:
                    logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {model_name}")
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                unique_train = len(np.unique(y_train)) / len(y_train) * 100
                logger.info(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {unique_train:.1f}%")
                
                # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
                model = self.create_model(
                    input_shape=(self.sequence_length, X_train.shape[2]),
                    name=model_name
                )
                
                # –û–±—É—á–∞–µ–º
                history = self.train_model(
                    model, X_train, y_train, X_val, y_val, 
                    model_name
                )
                
                # –û—Ü–µ–Ω–∏–≤–∞–µ–º
                test_metrics = self.evaluate_model(
                    model, X_test, y_test, model_name
                )
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                results[model_name] = {
                    'history': history.history,
                    'metrics': test_metrics,
                    'model': model
                }
                
                self.models[model_name] = model
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            self.save_models(results)
            
            # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
            self.create_final_report(results)
            
            logger.info("\n‚úÖ –û–ë–£–ß–ï–ù–ò–ï –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!")
            logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {self.log_dir}")
            
        except Exception as e:
            logger.error(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
            logger.error("–¢—Ä–µ–π—Å–±–µ–∫:", exc_info=True)
            raise
    
    def save_models(self, results):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        logger.info("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å–∞ –º–æ–¥–µ–ª–µ–π
        for name, model in self.models.items():
            model_path = f'trained_model/{name}.h5'
            model.save(model_path)
            logger.info(f"   ‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler
        with open('trained_model/scaler.pkl', 'wb') as f:
            pickle.dump(self.scaler, f)
        logger.info("   ‚úÖ Scaler —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É scaler –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
        scaler_stats = {
            'center': self.scaler.center_.tolist() if hasattr(self.scaler, 'center_') else None,
            'scale': self.scaler.scale_.tolist() if hasattr(self.scaler, 'scale_') else None,
            'feature_names': self.TECHNICAL_INDICATORS + [
                "rsi_oversold", "rsi_overbought", "macd_bullish",
                "bb_near_lower", "bb_near_upper", "strong_trend", "high_volume"
            ]
        }
        with open('trained_model/scaler_stats.json', 'w') as f:
            json.dump(scaler_stats, f, indent=2)
        logger.info("   ‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ scaler —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        with open('trained_model/feature_importance.json', 'w') as f:
            json.dump(self.feature_importance, f, indent=2)
        logger.info("   ‚úÖ –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            'type': 'temporal_fusion_transformer',
            'task_type': 'regression',
            'sequence_length': self.sequence_length,
            'd_model': self.d_model,
            'num_heads': self.num_heads,
            'num_transformer_blocks': self.num_transformer_blocks,
            'features': self.TECHNICAL_INDICATORS + [
                "rsi_oversold", "rsi_overbought", "macd_bullish",
                "bb_near_lower", "bb_near_upper", "strong_trend", "high_volume"
            ],
            'technical_indicators': self.TECHNICAL_INDICATORS,
            'created_at': datetime.now().isoformat(),
            'results': {
                name: result['metrics']
                for name, result in results.items()
            }
        }
        
        with open('trained_model/metadata.json', 'w') as f:
            json.dump(metadata, f, indent=2)
        logger.info("   ‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
    
    def predict_live(self, model, current_indicators, symbol):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –º–æ–º–µ–Ω—Ç–∞ —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º"""
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_values = []
        for col in self.TECHNICAL_INDICATORS:
            val = current_indicators.get(col, 0.0)
            if val is None or np.isnan(val) or np.isinf(val):
                val = 0.0
            feature_values.append(float(val))
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        rsi = current_indicators.get('rsi_val', 50)
        feature_values.append(1.0 if rsi is not None and rsi < 30 else 0.0)
        feature_values.append(1.0 if rsi is not None and rsi > 70 else 0.0)
        
        macd = current_indicators.get('macd_val', 0)
        macd_signal = current_indicators.get('macd_signal', 0)
        feature_values.append(1.0 if macd is not None and macd_signal is not None and macd > macd_signal else 0.0)
        
        bb_position = current_indicators.get('bb_position', 0.5)
        feature_values.append(1.0 if bb_position is not None and bb_position < 0.2 else 0.0)
        feature_values.append(1.0 if bb_position is not None and bb_position > 0.8 else 0.0)
        
        adx = current_indicators.get('adx_val', 0)
        feature_values.append(1.0 if adx is not None and adx > 25 else 0.0)
        
        volume_ratio = current_indicators.get('volume_ratio', 1.0)
        feature_values.append(1.0 if volume_ratio is not None and volume_ratio > 2.0 else 0.0)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        X = np.array(feature_values).reshape(1, -1)
        X_scaled = self.scaler.transform(X)
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–ø–æ–≤—Ç–æ—Ä—è–µ–º —Ç–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ)
        X_seq = np.repeat(X_scaled[np.newaxis, :, :], self.sequence_length, axis=1)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        prediction = model.predict(X_seq, verbose=0)[0, 0]
        
        # –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        key_indicators = self._get_key_indicators_for_prediction(current_indicators, prediction)
        
        # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏
        confidence = self._calculate_confidence(prediction)
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
        if abs(prediction) < 0.5:
            action = 'HOLD'
            reason = '–°–ª–∞–±—ã–π —Å–∏–≥–Ω–∞–ª'
        elif prediction > 1.5:
            action = 'STRONG_BUY' if 'buy' in model.name else 'STRONG_SELL'
            reason = '–°–∏–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª –Ω–∞ –≤—Ö–æ–¥'
        elif prediction > 0.5:
            action = 'BUY' if 'buy' in model.name else 'SELL'
            reason = '–£–º–µ—Ä–µ–Ω–Ω—ã–π —Å–∏–≥–Ω–∞–ª'
        else:
            action = 'AVOID'
            reason = '–í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–∞'
        
        return {
            'symbol': symbol,
            'prediction': float(prediction),
            'action': action,
            'confidence': confidence,
            'reason': reason,
            'key_indicators': key_indicators,
            'timestamp': datetime.now().isoformat()
        }
    
    def _get_key_indicators_for_prediction(self, indicators, prediction):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã, –ø–æ–≤–ª–∏—è–≤—à–∏–µ –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""
        key_factors = []
        
        # RSI –∞–Ω–∞–ª–∏–∑
        rsi = indicators.get('rsi_val', 50)
        if rsi is not None:
            if rsi < 30:
                key_factors.append(f'RSI –ø–µ—Ä–µ–ø—Ä–æ–¥–∞–Ω ({rsi:.1f})')
            elif rsi > 70:
                key_factors.append(f'RSI –ø–µ—Ä–µ–∫—É–ø–ª–µ–Ω ({rsi:.1f})')
        
        # MACD –∞–Ω–∞–ª–∏–∑
        macd = indicators.get('macd_val', 0)
        macd_signal = indicators.get('macd_signal', 0)
        macd_hist = indicators.get('macd_hist', 0)
        if macd is not None and macd_signal is not None and macd_hist is not None:
            if macd > macd_signal and macd_hist > 0:
                key_factors.append('MACD –±—ã—á—å–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ')
            elif macd < macd_signal and macd_hist < 0:
                key_factors.append('MACD –º–µ–¥–≤–µ–∂—å–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ')
        
        # ADX —Ç—Ä–µ–Ω–¥
        adx = indicators.get('adx_val', 0)
        if adx is not None and adx > 25:
            key_factors.append(f'–°–∏–ª—å–Ω—ã–π —Ç—Ä–µ–Ω–¥ (ADX={adx:.1f})')
        
        # –û–±—ä–µ–º
        volume_ratio = indicators.get('volume_ratio', 1.0)
        if volume_ratio is not None:
            if volume_ratio > 2.0:
                key_factors.append(f'–í—ã—Å–æ–∫–∏–π –æ–±—ä–µ–º (x{volume_ratio:.1f})')
            elif volume_ratio < 0.5:
                key_factors.append(f'–ù–∏–∑–∫–∏–π –æ–±—ä–µ–º (x{volume_ratio:.1f})')
        
        # Bollinger Bands
        bb_position = indicators.get('bb_position', 0.5)
        if bb_position is not None:
            if bb_position < 0.2:
                key_factors.append('–¶–µ–Ω–∞ —É –Ω–∏–∂–Ω–µ–π –≥—Ä–∞–Ω–∏—Ü—ã Bollinger')
            elif bb_position > 0.8:
                key_factors.append('–¶–µ–Ω–∞ —É –≤–µ—Ä—Ö–Ω–µ–π –≥—Ä–∞–Ω–∏—Ü—ã Bollinger')
        
        # Vortex
        vortex_ratio = indicators.get('vortex_ratio', 1.0)
        if vortex_ratio is not None:
            if vortex_ratio > 1.2:
                key_factors.append('Vortex –±—ã—á–∏–π —Å–∏–≥–Ω–∞–ª')
            elif vortex_ratio < 0.8:
                key_factors.append('Vortex –º–µ–¥–≤–µ–∂–∏–π —Å–∏–≥–Ω–∞–ª')
        
        return key_factors[:5]  # –¢–æ–ø-5 —Ñ–∞–∫—Ç–æ—Ä–æ–≤
    
    def _calculate_confidence(self, prediction):
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏"""
        # –ß–µ–º –¥–∞–ª—å—à–µ –æ—Ç 0, —Ç–µ–º —É–≤–µ—Ä–µ–Ω–Ω–µ–µ
        abs_pred = abs(prediction)
        
        if abs_pred < 0.5:
            return '–ù–∏–∑–∫–∞—è'
        elif abs_pred < 1.0:
            return '–°—Ä–µ–¥–Ω—è—è'
        elif abs_pred < 2.0:
            return '–í—ã—Å–æ–∫–∞—è'
        else:
            return '–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è'
    
    def explain_prediction(self, model, indicators, prediction_result):
        """–ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
        explanation = f"""
üéØ –ê–ù–ê–õ–ò–ó –°–ò–ì–ù–ê–õ–ê
================

–°–∏–º–≤–æ–ª: {prediction_result['symbol']}
–í—Ä–µ–º—è: {prediction_result['timestamp']}
–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {prediction_result['prediction']:.2f}%
–î–µ–π—Å—Ç–≤–∏–µ: {prediction_result['action']}
–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {prediction_result['confidence']}

üìä –ö–õ–Æ–ß–ï–í–´–ï –§–ê–ö–¢–û–†–´:
"""
        for factor in prediction_result['key_indicators']:
            explanation += f"‚Ä¢ {factor}\n"
        
        explanation += f"""

üìà –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø:
{prediction_result['reason']}

üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø:
"""
        
        if prediction_result['action'] in ['BUY', 'STRONG_BUY']:
            explanation += f"""–û—Ç–∫—Ä—ã—Ç—å LONG –ø–æ–∑–∏—Ü–∏—é
Stop Loss: -1.1% –æ—Ç –≤—Ö–æ–¥–∞
Take Profit: +1.2%, +2.4%, +3.5% (—á–∞—Å—Ç–∏—á–Ω—ã–µ –∑–∞–∫—Ä—ã—Ç–∏—è)"""
        elif prediction_result['action'] in ['SELL', 'STRONG_SELL']:
            explanation += f"""–û—Ç–∫—Ä—ã—Ç—å SHORT –ø–æ–∑–∏—Ü–∏—é
Stop Loss: +1.1% –æ—Ç –≤—Ö–æ–¥–∞
Take Profit: -1.2%, -2.4%, -3.5% (—á–∞—Å—Ç–∏—á–Ω—ã–µ –∑–∞–∫—Ä—ã—Ç–∏—è)"""
        else:
            explanation += "–í–æ–∑–¥–µ—Ä–∂–∞—Ç—å—Å—è –æ—Ç –≤—Ö–æ–¥–∞ –≤ –ø–æ–∑–∏—Ü–∏—é"
        
        return explanation
    
    def create_final_report(self, results):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        report = f"""
{'='*80}
–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –ü–û –û–ë–£–ß–ï–ù–ò–Æ
{'='*80}

–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
–õ–æ–≥ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {self.log_dir}
–¢–∏–ø –∑–∞–¥–∞—á–∏: {'–†–ï–ì–†–ï–°–°–ò–Ø' if self.task == 'regression' else '–ë–ò–ù–ê–†–ù–ê–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø'}

–ê–†–•–ò–¢–ï–ö–¢–£–†–ê:
- –¢–∏–ø: Temporal Fusion Transformer (TFT)
- Sequence Length: {self.sequence_length}
- Model Dimension: {self.d_model}
- Number of Heads: {self.num_heads}
- Transformer Blocks: {self.num_transformer_blocks}
- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤: {len(self.TECHNICAL_INDICATORS)}
- –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(self.TECHNICAL_INDICATORS) + 7}

–†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–£–ß–ï–ù–ò–Ø:
"""
        
        for model_name, result in results.items():
            metrics = result['metrics']
            report += f"\n{model_name.upper()}:\n"
            
            if self.task == 'regression':
                report += f"""- MAE: {metrics['mae']:.4f}%
- RMSE: {metrics['rmse']:.4f}%
- R¬≤: {metrics['r2']:.4f}
- Direction Accuracy: {metrics['direction_accuracy']:.2%}
- Mean Error: {metrics['mean_error']:.4f}%
- Std Error: {metrics['std_error']:.4f}%
"""
            else:  # classification
                report += f"""- Accuracy: {metrics['accuracy']:.4f}
- Precision: {metrics['precision']:.4f}
- Recall: {metrics['recall']:.4f}
- F1-Score: {metrics['f1']:.4f}
- ROC-AUC: {metrics['auc']:.4f}
- Confusion Matrix:
  - TN: {metrics['confusion_matrix'][0,0]:,}  FP: {metrics['confusion_matrix'][0,1]:,}
  - FN: {metrics['confusion_matrix'][1,0]:,}  TP: {metrics['confusion_matrix'][1,1]:,}
"""
        
        report += f"""
{'='*80}
–í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø:
- –ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è: {self.log_dir}/plots/training_progress.png
- –ì—Ä–∞—Ñ–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏: {self.log_dir}/plots/*_evaluation.png
- –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {self.log_dir}/plots/feature_importance.png

–ú–û–î–ï–õ–ò –°–û–•–†–ê–ù–ï–ù–´:
- trained_model/*.h5
- trained_model/scaler.pkl
- trained_model/metadata.json

TENSORBOARD:
tensorboard --logdir {self.log_dir}/tensorboard/
{'='*80}
"""
        
        with open(f'{self.log_dir}/final_report.txt', 'w') as f:
            f.write(report)
        
        logger.info(f"\nüìù –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {self.log_dir}/final_report.txt")
        print(report)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Transformer —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è –∫—Ä–∏–ø—Ç–æ—Ç—Ä–µ–π–¥–∏–Ω–≥–∞')
    parser.add_argument('--config', type=str, default='config.yaml',
                      help='–ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É')
    parser.add_argument('--task', type=str, choices=['regression', 'classification_binary'],
                      default='regression', help='–¢–∏–ø –∑–∞–¥–∞—á–∏: regression –∏–ª–∏ classification_binary')
    args = parser.parse_args()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ë–î
    db_manager = PostgreSQLManager(config['database'])
    
    try:
        # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –ë–î
        db_manager.connect()
        
        # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        trainer = UniversalTransformerTrainer(
            db_manager, 
            config_path=args.config,
            task=args.task
        )
        trainer.train()
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        raise
    finally:
        db_manager.disconnect()


if __name__ == "__main__":
    main()